{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge AI Workshop: Face Detection and Recognition Pipeline\n",
    "\n",
    "This notebook demonstrates the complete face detection and recognition pipeline that students will implement in C on the STM32N6 board.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Load Photos** - Load test images from PC\n",
    "2. **CenterFace Input Preparation** - Resize, normalize, convert to CHW format\n",
    "3. **CenterFace Inference** - Run face detection model\n",
    "4. **Post-processing** - Parse detections, apply NMS\n",
    "5. **Face Crop & Align** - Extract face regions for recognition\n",
    "6. **MobileFaceNet Inference** - Generate face embeddings\n",
    "7. **Similarity Calculation** - Compare embeddings using cosine similarity\n",
    "8. **Advanced: Quantized Models** - Explore INT8 quantization for STM32\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand neural network input/output formats\n",
    "- Learn preprocessing and postprocessing techniques\n",
    "- Practice with CHW vs HWC data layouts\n",
    "- Implement similarity metrics for face recognition\n",
    "- See immediate results at each step\n",
    "- Explore model quantization for edge deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "print(\"üì¶ All packages imported successfully!\")\n",
    "print(\"üöÄ Workshop environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample photos and model paths\n",
    "sample_photos = [\n",
    "    'SamplePics/trump1.jpg',  # Same person\n",
    "    'SamplePics/trump2.jpg',  # Same person\n",
    "    'SamplePics/obama.jpg'   # Different person\n",
    "]\n",
    "\n",
    "# Model paths\n",
    "centerface_model_path = 'onnx_tflite_src/centerface.tflite'\n",
    "mobilefacenet_model_path = 'onnx_tflite_src/mobilefacenet_fp32.onnx'\n",
    "\n",
    "print(\"üìÅ Paths configured:\")\n",
    "print(f\"   CenterFace: {centerface_model_path}\")\n",
    "print(f\"   MobileFaceNet: {mobilefacenet_model_path}\")\n",
    "print(f\"   Sample photos: {len(sample_photos)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AI models\n",
    "print(\"üîÑ Loading AI models...\")\n",
    "\n",
    "# Load CenterFace TFLite model\n",
    "if os.path.exists(centerface_model_path):\n",
    "    interpreter = tflite.Interpreter(model_path=centerface_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(\"‚úÖ CenterFace TFLite model loaded successfully!\")\n",
    "    print(f\"   Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"   Input type: {input_details[0]['dtype']}\")\n",
    "    print(f\"   Output shapes: {[output['shape'] for output in output_details]}\")\n",
    "else:\n",
    "    print(f\"‚ùå CenterFace model file not found: {centerface_model_path}\")\n",
    "    interpreter = None\n",
    "\n",
    "# Load MobileFaceNet ONNX model\n",
    "if os.path.exists(mobilefacenet_model_path):\n",
    "    try:\n",
    "        mobilefacenet_session = ort.InferenceSession(mobilefacenet_model_path)\n",
    "        mobilefacenet_input_name = mobilefacenet_session.get_inputs()[0].name\n",
    "        mobilefacenet_output_name = mobilefacenet_session.get_outputs()[0].name\n",
    "        mobilefacenet_input_shape = mobilefacenet_session.get_inputs()[0].shape\n",
    "        \n",
    "        print(\"‚úÖ MobileFaceNet ONNX model loaded successfully!\")\n",
    "        print(f\"   Input name: {mobilefacenet_input_name}\")\n",
    "        print(f\"   Input shape: {mobilefacenet_input_shape}\")\n",
    "        print(f\"   Output name: {mobilefacenet_output_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load MobileFaceNet model: {e}\")\n",
    "        mobilefacenet_session = None\n",
    "else:\n",
    "    print(f\"‚ùå MobileFaceNet model file not found: {mobilefacenet_model_path}\")\n",
    "    mobilefacenet_session = None\n",
    "\n",
    "print(\"\\nüìö Model loading complete!\")\n",
    "print(\"Ready to run face detection and recognition pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Display Photos\n",
    "\n",
    "First, let's load our test photos and see what we're working with. This step shows how to:\n",
    "- Read images from files\n",
    "- Convert BGR to RGB format (OpenCV uses BGR by default)\n",
    "- Display images in a grid layout\n",
    "- Handle missing files gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load image from file path\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        \n",
    "    Returns:\n",
    "        RGB image as numpy array (HWC format)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        # Create a dummy image if file doesn't exist\n",
    "        print(f\"‚ö†Ô∏è  {image_path} not found, creating dummy image\")\n",
    "        dummy_img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        return dummy_img\n",
    "    \n",
    "    # Load image using OpenCV (returns BGR format)\n",
    "    img = cv2.imread(image_path)\n",
    "    # Convert BGR to RGB for proper display\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img_rgb\n",
    "\n",
    "# Load all test photos\n",
    "print(\"üì∏ Loading test photos...\")\n",
    "images = []\n",
    "for photo_path in sample_photos:\n",
    "    img = load_image(photo_path)\n",
    "    images.append(img)\n",
    "    print(f\"   ‚úÖ Loaded {photo_path}: {img.shape} (H√óW√óC)\")\n",
    "\n",
    "# Display the photos in a grid\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (img, name) in enumerate(zip(images, sample_photos)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{name}\\n{img.shape}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Successfully loaded {len(images)} photos!\")\n",
    "print(\"These images will be processed through our face recognition pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: CenterFace Input Preparation\n",
    "\n",
    "CenterFace expects input in a specific format. This step demonstrates:\n",
    "- **Model requirements**: Understanding input shape and data type\n",
    "- **Image preprocessing**: Resizing, format conversion, normalization\n",
    "- **CHW vs HWC**: Converting between different tensor layouts\n",
    "- **Batch dimension**: Adding batch dimension for model inference\n",
    "\n",
    "**Key Concepts:**\n",
    "- **HWC**: Height √ó Width √ó Channels (typical image format)\n",
    "- **CHW**: Channels √ó Height √ó Width (neural network format)\n",
    "- **Batch**: Multiple samples processed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_centerface_input(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare image for CenterFace TFLite input\n",
    "    \n",
    "    This function shows students exactly what preprocessing is needed:\n",
    "    1. Resize to model input size (128√ó128)\n",
    "    2. Convert HWC to CHW format\n",
    "    3. Add batch dimension\n",
    "    4. Ensure correct data type\n",
    "    \n",
    "    Args:\n",
    "        image: Input image in HWC format (uint8)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image for TFLite model (1,3,128,128) CHW format\n",
    "    \"\"\"\n",
    "    if interpreter is None:\n",
    "        # Fallback preprocessing when model not available\n",
    "        target_size = (128, 128)\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        converted = resized.astype(np.float32)\n",
    "        chw_image = np.transpose(converted, (2, 0, 1))\n",
    "        batch_input = np.expand_dims(chw_image, axis=0)\n",
    "        return batch_input\n",
    "    \n",
    "    # Get model input requirements\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    \n",
    "    print(f\"üéØ Model requirements:\")\n",
    "    print(f\"   Expected shape: {input_shape}\")\n",
    "    print(f\"   Expected type: {input_dtype}\")\n",
    "    \n",
    "    # CenterFace expects 128√ó128 input\n",
    "    model_input_size = (128, 128)\n",
    "    \n",
    "    # Use OpenCV's blobFromImage for proper preprocessing\n",
    "    # This is the same approach used in production CenterFace implementations\n",
    "    input_blob = cv2.dnn.blobFromImage(\n",
    "        image, \n",
    "        scalefactor=1.0,           # No pixel value scaling\n",
    "        size=model_input_size,     # Resize to 128√ó128\n",
    "        mean=(0, 0, 0),           # No mean subtraction\n",
    "        swapRB=True,              # Convert BGR to RGB\n",
    "        crop=False                # Just resize, don't crop\n",
    "    )\n",
    "    \n",
    "    print(f\"üîÑ Preprocessing: {image.shape} ‚Üí {input_blob.shape}\")\n",
    "    print(f\"   Value range: [{input_blob.min():.1f}, {input_blob.max():.1f}]\")\n",
    "    print(f\"   Data type: {input_blob.dtype}\")\n",
    "    \n",
    "    # Convert to model's expected data type if needed\n",
    "    if input_dtype != input_blob.dtype:\n",
    "        if input_dtype == np.uint8:\n",
    "            input_blob = input_blob.astype(np.uint8)\n",
    "        elif input_dtype == np.int8:\n",
    "            input_blob = input_blob.astype(np.int8)\n",
    "        print(f\"üîÑ Type conversion: ‚Üí {input_dtype}\")\n",
    "    \n",
    "    return input_blob\n",
    "\n",
    "# Prepare inputs for all images\n",
    "print(\"üöÄ Preparing CenterFace inputs for all images...\\n\")\n",
    "centerface_inputs = []\n",
    "for i, img in enumerate(images):\n",
    "    print(f\"üì∑ Processing image {i+1}:\")\n",
    "    prepared = prepare_centerface_input(img)\n",
    "    centerface_inputs.append(prepared)\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(centerface_inputs)} inputs for CenterFace inference\")\n",
    "print(\"Each input is ready for the face detection model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: CenterFace Inference\n",
    "\n",
    "Now we run the actual CenterFace TensorFlow Lite model for face detection. This step demonstrates:\n",
    "\n",
    "**CenterFace Output Format:**\n",
    "- **Heatmap**: Confidence scores for face centers (32√ó32√ó1)\n",
    "- **Scale**: Bounding box size regression (32√ó32√ó2)\n",
    "- **Offset**: Bounding box position regression (32√ó32√ó2)  \n",
    "- **Landmarks**: 5 facial keypoints (32√ó32√ó10)\n",
    "\n",
    "**Key Algorithms Students Will Implement:**\n",
    "- **Peak detection**: Finding face centers in heatmap\n",
    "- **Coordinate decoding**: Converting network outputs to pixel coordinates\n",
    "- **Non-Maximum Suppression**: Removing duplicate detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, nms_thresh):\n",
    "    \"\"\"\n",
    "    Non-Maximum Suppression - removes overlapping face detections\n",
    "    \n",
    "    This is a critical algorithm students will implement in C!\n",
    "    It prevents the same face from being detected multiple times.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Array of bounding boxes [x1, y1, x2, y2]\n",
    "        scores: Confidence scores for each box\n",
    "        nms_thresh: IoU threshold for suppression\n",
    "    \n",
    "    Returns:\n",
    "        Indices of boxes to keep\n",
    "    \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1] \n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = np.argsort(scores)[::-1]  # Sort by confidence (highest first)\n",
    "    num_detections = boxes.shape[0]\n",
    "    suppressed = np.zeros((num_detections,), dtype=bool)\n",
    "\n",
    "    keep = []\n",
    "    for _i in range(num_detections):\n",
    "        i = order[_i]\n",
    "        if suppressed[i]:\n",
    "            continue\n",
    "        keep.append(i)\n",
    "\n",
    "        # Calculate IoU with remaining boxes\n",
    "        ix1, iy1, ix2, iy2 = x1[i], y1[i], x2[i], y2[i]\n",
    "        iarea = areas[i]\n",
    "\n",
    "        for _j in range(_i + 1, num_detections):\n",
    "            j = order[_j]\n",
    "            if suppressed[j]:\n",
    "                continue\n",
    "            \n",
    "            # Calculate intersection area\n",
    "            xx1 = max(ix1, x1[j])\n",
    "            yy1 = max(iy1, y1[j])\n",
    "            xx2 = min(ix2, x2[j])\n",
    "            yy2 = min(iy2, y2[j])\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "            inter = w * h\n",
    "            ovr = inter / (iarea + areas[j] - inter)  # IoU calculation\n",
    "            \n",
    "            if ovr >= nms_thresh:\n",
    "                suppressed[j] = True  # Mark for suppression\n",
    "\n",
    "    return keep\n",
    "\n",
    "def decode_centerface_outputs(heatmap, scale, offset, landmark, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Decode CenterFace neural network outputs into face detections\n",
    "    \n",
    "    This shows students how raw network outputs become face bounding boxes!\n",
    "    \n",
    "    Args:\n",
    "        heatmap: Face confidence heatmap (1, 32, 32, 1)\n",
    "        scale: Scale regression (1, 32, 32, 2) \n",
    "        offset: Offset regression (1, 32, 32, 2)\n",
    "        landmark: Landmark regression (1, 32, 32, 10)\n",
    "        threshold: Minimum confidence for detection\n",
    "        \n",
    "    Returns:\n",
    "        boxes: [N, 5] array of [x1, y1, x2, y2, score]\n",
    "        landmarks: [N, 10] array of landmark coordinates\n",
    "    \"\"\"\n",
    "    # Remove batch dimension for processing\n",
    "    heatmap = heatmap[0, ..., 0]    # (32, 32)\n",
    "    scale = scale[0]                # (32, 32, 2)\n",
    "    offset = offset[0]              # (32, 32, 2)\n",
    "    landmark = landmark[0]          # (32, 32, 10)\n",
    "    \n",
    "    # Extract scale and offset channels\n",
    "    scale_y = scale[..., 0]   # Height scale\n",
    "    scale_x = scale[..., 1]   # Width scale\n",
    "    offset_y = offset[..., 0] # Y offset\n",
    "    offset_x = offset[..., 1] # X offset\n",
    "    \n",
    "    # Find face centers above threshold\n",
    "    face_rows, face_cols = np.where(heatmap > threshold)\n",
    "    boxes, lms_list = [], []\n",
    "    \n",
    "    print(f\"üîç Found {len(face_rows)} potential face centers\")\n",
    "    \n",
    "    if len(face_rows) > 0:\n",
    "        for i in range(len(face_rows)):\n",
    "            row, col = face_rows[i], face_cols[i]\n",
    "            \n",
    "            # Decode bounding box size (exponential activation)\n",
    "            h_scale = np.exp(scale_y[row, col]) * 4\n",
    "            w_scale = np.exp(scale_x[row, col]) * 4\n",
    "            \n",
    "            # Get position offsets\n",
    "            y_offset = offset_y[row, col]\n",
    "            x_offset = offset_x[row, col]\n",
    "            \n",
    "            # Get confidence score\n",
    "            confidence = heatmap[row, col]\n",
    "            \n",
    "            # Calculate final bounding box coordinates\n",
    "            # The *4 factor accounts for network downsampling\n",
    "            center_x = (col + x_offset + 0.5) * 4\n",
    "            center_y = (row + y_offset + 0.5) * 4\n",
    "            \n",
    "            x1 = max(0, center_x - w_scale / 2)\n",
    "            y1 = max(0, center_y - h_scale / 2)\n",
    "            x2 = min(128, center_x + w_scale / 2)\n",
    "            y2 = min(128, center_y + h_scale / 2)\n",
    "            \n",
    "            boxes.append([x1, y1, x2, y2, confidence])\n",
    "            \n",
    "            # Decode facial landmarks (5 points)\n",
    "            lms_temp = []\n",
    "            for j in range(5):\n",
    "                lm_y = landmark[row, col, j * 2 + 0]\n",
    "                lm_x = landmark[row, col, j * 2 + 1]\n",
    "                # Scale landmarks relative to bounding box\n",
    "                px = lm_x * w_scale + x1\n",
    "                py = lm_y * h_scale + y1\n",
    "                lms_temp.extend([px, py])\n",
    "            \n",
    "            lms_list.append(lms_temp)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        boxes = np.asarray(boxes, dtype=np.float32)\n",
    "        lms_list = np.asarray(lms_list, dtype=np.float32)\n",
    "        \n",
    "        # Apply Non-Maximum Suppression to remove duplicates\n",
    "        if len(boxes) > 0:\n",
    "            keep_indices = nms(boxes[:, :4], boxes[:, 4], 0.1)\n",
    "            boxes = boxes[keep_indices, :]\n",
    "            lms_list = lms_list[keep_indices, :]\n",
    "            print(f\"‚úÖ After NMS: {len(boxes)} final detections\")\n",
    "    \n",
    "    else:\n",
    "        boxes = np.array([]).reshape(0, 5)\n",
    "        lms_list = np.array([]).reshape(0, 10)\n",
    "    \n",
    "    return boxes, lms_list\n",
    "\n",
    "def run_centerface_inference(input_batch: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run CenterFace TFLite inference and decode outputs\n",
    "    \n",
    "    Args:\n",
    "        input_batch: Preprocessed image batch (1, 3, 128, 128)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (detections, landmarks)\n",
    "    \"\"\"\n",
    "    if interpreter is None:\n",
    "        print(\"‚ùå TFLite model not available, using simulation\")\n",
    "        # Return simulated detections for demonstration\n",
    "        sim_boxes = np.array([[30, 40, 90, 100, 0.95]], dtype=np.float32)\n",
    "        sim_landmarks = np.array([[45, 55, 75, 55, 60, 65, 50, 80, 70, 80]], dtype=np.float32)\n",
    "        return sim_boxes, sim_landmarks\n",
    "    \n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_batch)\n",
    "    \n",
    "    # Run neural network inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get outputs (indices match CenterFace implementation)\n",
    "    heatmap = interpreter.get_tensor(output_details[2]['index'])  # Confidence\n",
    "    scale = interpreter.get_tensor(output_details[0]['index'])    # Scale\n",
    "    offset = interpreter.get_tensor(output_details[3]['index'])   # Offset\n",
    "    landmarks = interpreter.get_tensor(output_details[1]['index']) # Landmarks\n",
    "    \n",
    "    print(f\"üìä Network output shapes:\")\n",
    "    print(f\"   Heatmap: {heatmap.shape}\")\n",
    "    print(f\"   Scale: {scale.shape}\")\n",
    "    print(f\"   Offset: {offset.shape}\")\n",
    "    print(f\"   Landmarks: {landmarks.shape}\")\n",
    "    \n",
    "    # Decode raw outputs into face detections\n",
    "    boxes, landmark_points = decode_centerface_outputs(heatmap, scale, offset, landmarks)\n",
    "    \n",
    "    return boxes, landmark_points\n",
    "\n",
    "def scale_detections_to_original(boxes, landmarks, original_shape):\n",
    "    \"\"\"\n",
    "    Scale detections from 128√ó128 model space back to original image size\n",
    "    \n",
    "    The model processes 128√ó128 images, but we need coordinates for the original image.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = original_shape[:2]\n",
    "    model_size = 128\n",
    "    \n",
    "    scale_x = orig_w / model_size\n",
    "    scale_y = orig_h / model_size\n",
    "    \n",
    "    # Scale bounding boxes\n",
    "    if len(boxes) > 0:\n",
    "        boxes_scaled = boxes.copy()\n",
    "        boxes_scaled[:, [0, 2]] *= scale_x  # x coordinates\n",
    "        boxes_scaled[:, [1, 3]] *= scale_y  # y coordinates\n",
    "    else:\n",
    "        boxes_scaled = boxes\n",
    "    \n",
    "    # Scale landmarks\n",
    "    if len(landmarks) > 0:\n",
    "        landmarks_scaled = landmarks.copy()\n",
    "        landmarks_scaled[:, 0::2] *= scale_x  # x coordinates\n",
    "        landmarks_scaled[:, 1::2] *= scale_y  # y coordinates\n",
    "    else:\n",
    "        landmarks_scaled = landmarks\n",
    "        \n",
    "    return boxes_scaled, landmarks_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CenterFace inference on all images\n",
    "print(\"üß† Running CenterFace inference on all images...\\n\")\n",
    "\n",
    "all_detections = []\n",
    "all_landmarks = []\n",
    "\n",
    "for i, input_batch in enumerate(centerface_inputs):\n",
    "    print(f\"üîÑ Processing image {i+1}:\")\n",
    "    \n",
    "    # Run face detection\n",
    "    boxes, landmarks = run_centerface_inference(input_batch)\n",
    "    \n",
    "    # Scale detections back to original image size\n",
    "    boxes_scaled, landmarks_scaled = scale_detections_to_original(\n",
    "        boxes, landmarks, images[i].shape\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Final results: {len(boxes_scaled)} faces detected\")\n",
    "    for j, box in enumerate(boxes_scaled):\n",
    "        x1, y1, x2, y2, conf = box\n",
    "        print(f\"   Face {j+1}: confidence={conf:.3f}, bbox=[{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]\")\n",
    "    \n",
    "    all_detections.append(boxes_scaled)\n",
    "    all_landmarks.append(landmarks_scaled)\n",
    "    print()\n",
    "\n",
    "total_faces = sum(len(boxes) for boxes in all_detections)\n",
    "print(f\"‚úÖ CenterFace inference completed!\")\n",
    "print(f\"üéØ Total faces detected across all images: {total_faces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Face Detections\n",
    "\n",
    "Let's visualize our face detection results. This step shows:\n",
    "- **Bounding box drawing**: How to overlay detection results\n",
    "- **Landmark visualization**: Displaying facial keypoints\n",
    "- **Confidence scores**: Showing model certainty\n",
    "- **Color coding**: Different colors for different landmark types\n",
    "\n",
    "**Landmark Meaning:**\n",
    "- **Red**: Left eye\n",
    "- **Green**: Right eye  \n",
    "- **Blue**: Nose tip\n",
    "- **Yellow**: Left mouth corner\n",
    "- **Magenta**: Right mouth corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_centerface_detections(image: np.ndarray, boxes: np.ndarray, landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw face detection results on image\n",
    "    \n",
    "    This visualization helps students understand what the AI model detected.\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        boxes: Face bounding boxes [x1, y1, x2, y2, score]\n",
    "        landmarks: Facial landmarks [x1,y1, x2,y2, ..., x5,y5]\n",
    "    \n",
    "    Returns:\n",
    "        Image with detection results drawn\n",
    "    \"\"\"\n",
    "    img_copy = image.copy()\n",
    "    \n",
    "    # Draw bounding boxes around detected faces\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, score = box\n",
    "        \n",
    "        # Green rectangle for face boundary\n",
    "        cv2.rectangle(img_copy, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        \n",
    "        # Show confidence score\n",
    "        cv2.putText(img_copy, f'{score:.3f}', (int(x1), int(y1) - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw facial landmarks with different colors\n",
    "    landmark_colors = [\n",
    "        (255, 0, 0),    # Red - Left eye\n",
    "        (0, 255, 0),    # Green - Right eye\n",
    "        (0, 0, 255),    # Blue - Nose\n",
    "        (255, 255, 0),  # Yellow - Left mouth\n",
    "        (255, 0, 255)   # Magenta - Right mouth\n",
    "    ]\n",
    "    \n",
    "    for landmark_set in landmarks:\n",
    "        for i in range(5):  # 5 landmarks per face\n",
    "            x = int(landmark_set[i * 2])\n",
    "            y = int(landmark_set[i * 2 + 1])\n",
    "            cv2.circle(img_copy, (x, y), 3, landmark_colors[i], -1)\n",
    "    \n",
    "    return img_copy\n",
    "\n",
    "# Visualize detection results\n",
    "print(\"üé® Visualizing face detection results...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (img, boxes, landmarks) in enumerate(zip(images, all_detections, all_landmarks)):\n",
    "    # Draw detection results on image\n",
    "    annotated = draw_centerface_detections(img, boxes, landmarks)\n",
    "    \n",
    "    # Display in subplot\n",
    "    axes[i].imshow(annotated)\n",
    "    axes[i].set_title(f\"Image {i+1}: {len(boxes)} faces detected\")\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Print detailed detection info\n",
    "    print(f\"\\nüìã Image {i+1} detection details:\")\n",
    "    for j, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2, conf = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        print(f\"   Face {j+1}: confidence={conf:.3f}, size={w:.0f}√ó{h:.0f}px\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Face Detection Results:\")\n",
    "print(\"‚úÖ Green boxes show detected face boundaries\")\n",
    "print(\"‚úÖ Colored dots show facial landmarks:\")\n",
    "print(\"   üî¥ Red = Left Eye\")\n",
    "print(\"   üü¢ Green = Right Eye\")\n",
    "print(\"   üîµ Blue = Nose\")\n",
    "print(\"   üü° Yellow = Left Mouth Corner\")\n",
    "print(\"   üü£ Magenta = Right Mouth Corner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Face Crop and Alignment\n",
    "\n",
    "Now we extract face regions for recognition. This step demonstrates:\n",
    "- **Region of Interest (ROI)**: Extracting face areas from full images\n",
    "- **Bounding box expansion**: Adding padding around detected faces\n",
    "- **Face alignment**: Standardizing face orientation and size\n",
    "- **Size normalization**: Resizing to model requirements (112√ó112)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Face recognition models expect standardized input\n",
    "- Proper alignment improves recognition accuracy\n",
    "- Consistent sizing enables batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face_landmarks(image: np.ndarray, landmarks: np.ndarray, \n",
    "                        output_size: Tuple[int, int] = (112, 112)) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Align face using facial landmarks with similarity transformation\n",
    "    \n",
    "    This function performs proper face alignment by:\n",
    "    1. Using eye landmarks to calculate face angle\n",
    "    2. Applying similarity transformation (rotation, scaling, translation)\n",
    "    3. Cropping aligned face to standard size\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        landmarks: Facial landmarks [x1,y1, x2,y2, ..., x5,y5]\n",
    "        output_size: Target size for recognition model\n",
    "    \n",
    "    Returns:\n",
    "        Aligned face image or None if alignment fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract eye landmarks (landmarks 0 and 1 are left and right eyes)\n",
    "        left_eye = landmarks[0:2]    # [x1, y1]\n",
    "        right_eye = landmarks[2:4]   # [x2, y2]\n",
    "        \n",
    "        # Calculate angle between eyes\n",
    "        dx = right_eye[0] - left_eye[0]\n",
    "        dy = right_eye[1] - left_eye[1]\n",
    "        angle = np.arctan2(dy, dx) * 180.0 / np.pi\n",
    "        \n",
    "        # Calculate center point between eyes\n",
    "        center_x = (left_eye[0] + right_eye[0]) / 2\n",
    "        center_y = (left_eye[1] + right_eye[1]) / 2\n",
    "        \n",
    "        # Calculate eye distance for scaling\n",
    "        eye_dist = np.sqrt(dx*dx + dy*dy)\n",
    "        \n",
    "        # Define desired eye positions in output image\n",
    "        desired_eye_dist = output_size[0] * 0.35  # 35% of image width\n",
    "        scale = desired_eye_dist / eye_dist\n",
    "        \n",
    "        # Calculate desired center position\n",
    "        desired_center_x = output_size[0] * 0.5\n",
    "        desired_center_y = output_size[1] * 0.4   # Slightly above center\n",
    "        \n",
    "        # Create transformation matrix\n",
    "        M = cv2.getRotationMatrix2D((center_x, center_y), angle, scale)\n",
    "        \n",
    "        # Add translation to center the face\n",
    "        M[0, 2] += (desired_center_x - center_x)\n",
    "        M[1, 2] += (desired_center_y - center_y)\n",
    "        \n",
    "        # Apply transformation\n",
    "        aligned_face = cv2.warpAffine(image, M, output_size, flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        print(f\"‚úÖ Face aligned: angle={angle:.1f}¬∞, scale={scale:.2f}x\")\n",
    "        return aligned_face\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Face alignment error: {e}\")\n",
    "        return None\n",
    "\n",
    "def crop_and_align_face(image: np.ndarray, box: np.ndarray, landmarks: np.ndarray,\n",
    "                       output_size: Tuple[int, int] = (112, 112)) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Crop and align face using detection results - now with proper alignment\n",
    "    \n",
    "    This function prepares faces for recognition by:\n",
    "    1. First attempting landmark-based alignment\n",
    "    2. Fallback to simple crop if alignment fails\n",
    "    3. Resizing to standard size\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        box: Face bounding box [x1, y1, x2, y2, score]\n",
    "        landmarks: Facial landmarks (5 points: left_eye, right_eye, nose, left_mouth, right_mouth)\n",
    "        output_size: Target size for recognition model\n",
    "    \n",
    "    Returns:\n",
    "        Aligned face image or None if extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try landmark-based alignment if we have good landmarks\n",
    "        if landmarks is not None and len(landmarks) >= 4:\n",
    "            aligned_face = align_face_landmarks(image, landmarks, output_size)\n",
    "            if aligned_face is not None:\n",
    "                return aligned_face\n",
    "        \n",
    "        # Fallback to simple crop and resize\n",
    "        print(\"‚ö†Ô∏è  Falling back to simple crop (landmarks insufficient)\")\n",
    "        \n",
    "        x1, y1, x2, y2, confidence = box\n",
    "        \n",
    "        # Calculate face center and size\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        face_size = max(x2 - x1, y2 - y1)\n",
    "        \n",
    "        # Expand bounding box by 20% for context\n",
    "        # This includes hair, forehead, and chin which help recognition\n",
    "        expanded_size = face_size * 1.2\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        crop_x1 = max(0, int(center_x - expanded_size / 2))\n",
    "        crop_y1 = max(0, int(center_y - expanded_size / 2))\n",
    "        crop_x2 = min(image.shape[1], int(center_x + expanded_size / 2))\n",
    "        crop_y2 = min(image.shape[0], int(center_y + expanded_size / 2))\n",
    "        \n",
    "        # Extract face region\n",
    "        face_crop = image[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "        \n",
    "        if face_crop.size == 0:\n",
    "            print(f\"‚ùå Empty crop for face with confidence {confidence:.3f}\")\n",
    "            return None\n",
    "        \n",
    "        # Resize to standard size (112√ó112 for MobileFaceNet)\n",
    "        face_resized = cv2.resize(face_crop, output_size)\n",
    "        \n",
    "        print(f\"‚úÖ Cropped face: {face_crop.shape} ‚Üí {face_resized.shape}\")\n",
    "        return face_resized\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Face crop error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract faces from all detections\n",
    "print(\"‚úÇÔ∏è Extracting and aligning faces for recognition...\\n\")\n",
    "\n",
    "aligned_faces = []\n",
    "face_info = []  # Track which image each face came from\n",
    "\n",
    "for img_idx, (img, boxes, landmarks) in enumerate(zip(images, all_detections, all_landmarks)):\n",
    "    print(f\"üì∑ Processing faces from image {img_idx + 1}:\")\n",
    "    \n",
    "    for det_idx, (box, landmark_set) in enumerate(zip(boxes, landmarks)):\n",
    "        confidence = box[4]\n",
    "        print(f\"   Face {det_idx + 1}: confidence={confidence:.3f}\")\n",
    "        \n",
    "        # Crop and align the face\n",
    "        aligned_face = crop_and_align_face(img, box, landmark_set)\n",
    "        \n",
    "        if aligned_face is not None:\n",
    "            aligned_faces.append(aligned_face)\n",
    "            face_info.append((img_idx, det_idx, confidence))\n",
    "            print(f\"      ‚úÖ Success: {aligned_face.shape}\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå Failed to extract face\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Face extraction complete!\")\n",
    "print(f\"   Total faces extracted: {len(aligned_faces)}\")\n",
    "print(f\"   Ready for face recognition processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Aligned Faces\n",
    "\n",
    "Let's see our cropped and aligned faces before they go to the recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display aligned faces\n",
    "if aligned_faces:\n",
    "    print(\"üë§ Displaying aligned faces ready for recognition:\")\n",
    "    \n",
    "    n_faces = len(aligned_faces)\n",
    "    cols = min(4, n_faces)\n",
    "    rows = (n_faces + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    \n",
    "    # Handle single row case\n",
    "    if rows == 1:\n",
    "        axes = [axes] if n_faces == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Display each aligned face\n",
    "    for i, (face, (img_idx, det_idx, conf)) in enumerate(zip(aligned_faces, face_info)):\n",
    "        axes[i].imshow(face)\n",
    "        axes[i].set_title(f\"Face from Image {img_idx + 1}\\nConfidence: {conf:.3f}\\nSize: {face.shape[0]}√ó{face.shape[1]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_faces, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Face Preparation Summary:\")\n",
    "    for i, (img_idx, det_idx, conf) in enumerate(face_info):\n",
    "        print(f\"   Face {i+1}: From image {img_idx+1}, confidence={conf:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No aligned faces to display\")\n",
    "    print(\"Check face detection results above.\")\n",
    "\n",
    "print(\"\\n‚úÖ Face alignment completed!\")\n",
    "print(\"These standardized face images are ready for recognition processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: MobileFaceNet Input Preparation\n",
    "\n",
    "Now we prepare the aligned faces for MobileFaceNet inference. This step shows:\n",
    "\n",
    "**Critical Preprocessing Steps:**\n",
    "- **Color space conversion**: BGR ‚Üí RGB (OpenCV vs standard)\n",
    "- **Normalization**: Convert pixel values to [-1, 1] range\n",
    "- **Layout conversion**: HWC ‚Üí CHW for neural networks\n",
    "- **Batch dimension**: Add dimension for model input\n",
    "\n",
    "**Why Each Step Matters:**\n",
    "- **Normalization**: Helps model training stability\n",
    "- **CHW layout**: Optimized for GPU/AI accelerator processing\n",
    "- **Consistent preprocessing**: Must match training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mobilefacenet_input(face_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare aligned face for MobileFaceNet inference\n",
    "    \n",
    "    This preprocessing is critical - it must exactly match what the model expects!\n",
    "    Students will implement this preprocessing in C on the STM32.\n",
    "    \n",
    "    Args:\n",
    "        face_image: Aligned face image (112√ó112, RGB)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed input (1√ó3√ó112√ó112, float32)\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Preprocessing face: {face_image.shape}\")\n",
    "    \n",
    "    # Step 1: Ensure RGB format (face_image is already RGB from our pipeline)\n",
    "    face_rgb = face_image.astype(np.float32)\n",
    "    print(f\"   Color range: [{face_rgb.min():.0f}, {face_rgb.max():.0f}]\")\n",
    "    \n",
    "    # Step 2: Normalize to [-1, 1] range\n",
    "    # This matches MobileFaceNet training preprocessing\n",
    "    face_normalized = (face_rgb / 255.0) * 2.0 - 1.0\n",
    "    print(f\"   After normalization: [{face_normalized.min():.3f}, {face_normalized.max():.3f}]\")\n",
    "    \n",
    "    # Step 3: Convert HWC to CHW layout\n",
    "    # Neural networks expect Channels-first format\n",
    "    face_chw = np.transpose(face_normalized, (2, 0, 1))\n",
    "    print(f\"   Layout conversion: {face_normalized.shape} ‚Üí {face_chw.shape}\")\n",
    "    \n",
    "    # Step 4: Add batch dimension\n",
    "    # Models expect batch of samples, even if batch size = 1\n",
    "    batch_input = np.expand_dims(face_chw, axis=0)\n",
    "    print(f\"   Final shape: {batch_input.shape}\")\n",
    "    \n",
    "    return batch_input\n",
    "\n",
    "# Prepare all aligned faces for MobileFaceNet\n",
    "print(\"üöÄ Preparing inputs for MobileFaceNet recognition model...\\n\")\n",
    "\n",
    "mobilefacenet_inputs = []\n",
    "for i, face in enumerate(aligned_faces):\n",
    "    print(f\"üì∑ Preparing face {i+1}:\")\n",
    "    prepared = prepare_mobilefacenet_input(face)\n",
    "    mobilefacenet_inputs.append(prepared)\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Input preparation complete!\")\n",
    "print(f\"   Prepared {len(mobilefacenet_inputs)} faces for recognition\")\n",
    "print(f\"   Each input shape: {mobilefacenet_inputs[0].shape if mobilefacenet_inputs else 'None'}\")\n",
    "print(f\"   Ready for MobileFaceNet inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: MobileFaceNet Inference\n",
    "\n",
    "Now we run the MobileFaceNet model to generate face embeddings. This step demonstrates:\n",
    "\n",
    "**Face Recognition Concepts:**\n",
    "- **Face embeddings**: 128-dimensional vectors representing faces\n",
    "- **Feature extraction**: Converting images to numerical features\n",
    "- **L2 normalization**: Standardizing vector lengths for comparison\n",
    "- **ONNX inference**: Running optimized neural networks\n",
    "\n",
    "**Why 128 dimensions?**\n",
    "- Compact representation that captures facial features\n",
    "- Good balance between accuracy and memory usage\n",
    "- Standard size for many face recognition systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mobilefacenet_inference(input_batch: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run MobileFaceNet inference to generate face embedding\n",
    "    \n",
    "    This is where the magic happens - converting a face image into a \n",
    "    numerical representation that can be compared with other faces.\n",
    "    \n",
    "    Args:\n",
    "        input_batch: Preprocessed face input (1√ó3√ó112√ó112)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized face embedding (128-dimensional vector)\n",
    "    \"\"\"\n",
    "    if mobilefacenet_session is None:\n",
    "        print(\"‚ùå MobileFaceNet model not available\")\n",
    "        print(\"   Generating random embedding for demonstration\")\n",
    "        # Return normalized random vector for demo\n",
    "        random_embedding = np.random.normal(0, 0.1, 128).astype(np.float32)\n",
    "        norm = np.linalg.norm(random_embedding)\n",
    "        return random_embedding / norm if norm > 0 else random_embedding\n",
    "    \n",
    "    try:\n",
    "        # Run ONNX model inference\n",
    "        onnx_output = mobilefacenet_session.run(\n",
    "            [mobilefacenet_output_name], \n",
    "            {mobilefacenet_input_name: input_batch}\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"üîç Model output:\")\n",
    "        print(f\"   Shape: {onnx_output.shape}\")\n",
    "        print(f\"   Type: {onnx_output.dtype}\")\n",
    "        print(f\"   Range: [{onnx_output.min():.3f}, {onnx_output.max():.3f}]\")\n",
    "        \n",
    "        # Extract embedding vector (remove batch dimension)\n",
    "        embedding = onnx_output.astype(np.float32).flatten()\n",
    "        print(f\"   Embedding dimensions: {len(embedding)}\")\n",
    "        \n",
    "        # L2 normalization - crucial for face comparison!\n",
    "        # This ensures all embeddings have unit length\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm > 0:\n",
    "            embedding = embedding / norm\n",
    "            print(f\"   After L2 normalization: norm = {np.linalg.norm(embedding):.6f}\")\n",
    "        \n",
    "        print(f\"   Final range: [{embedding.min():.3f}, {embedding.max():.3f}]\")\n",
    "        return embedding\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Inference error: {e}\")\n",
    "        # Fallback to random embedding\n",
    "        random_embedding = np.random.normal(0, 0.1, 128).astype(np.float32)\n",
    "        norm = np.linalg.norm(random_embedding)\n",
    "        return random_embedding / norm if norm > 0 else random_embedding\n",
    "\n",
    "# Generate embeddings for all faces\n",
    "print(\"üß† Running MobileFaceNet inference to generate face embeddings...\\n\")\n",
    "\n",
    "face_embeddings = []\n",
    "for i, input_batch in enumerate(mobilefacenet_inputs):\n",
    "    print(f\"üîÑ Processing face {i+1}:\")\n",
    "    \n",
    "    # Generate face embedding\n",
    "    embedding = run_mobilefacenet_inference(input_batch)\n",
    "    face_embeddings.append(embedding)\n",
    "    \n",
    "    print(f\"   ‚úÖ Generated {len(embedding)}-dimensional embedding\")\n",
    "    print(f\"   üî¢ Sample values: [{embedding[0]:.3f}, {embedding[1]:.3f}, {embedding[2]:.3f}, ...]\")\n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Face embedding generation complete!\")\n",
    "print(f\"   Total embeddings: {len(face_embeddings)}\")\n",
    "print(f\"   Each embedding: 128-dimensional normalized vector\")\n",
    "print(f\"   Ready for face comparison and recognition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Cosine Similarity Calculation\n",
    "\n",
    "Finally, we calculate similarity between face embeddings. This is the core of face recognition!\n",
    "\n",
    "**Cosine Similarity:**\n",
    "- Measures angle between two vectors\n",
    "- Range: -1 (opposite) to +1 (identical)\n",
    "- Values > 0.5 typically indicate same person\n",
    "- Independent of vector magnitude (thanks to L2 normalization)\n",
    "\n",
    "**Why Cosine Similarity?**\n",
    "- Robust to lighting variations\n",
    "- Focus on facial structure, not brightness\n",
    "- Computationally efficient\n",
    "- Standard in face recognition systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two face embeddings\n",
    "    \n",
    "    This is the mathematical heart of face recognition!\n",
    "    Students will implement this exact calculation in C.\n",
    "    \n",
    "    Args:\n",
    "        emb1, emb2: Face embeddings (128-dimensional normalized vectors)\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity [-1, 1] where higher = more similar\n",
    "    \"\"\"\n",
    "    # Calculate dot product (core of cosine similarity)\n",
    "    dot_product = np.dot(emb1, emb2)\n",
    "    \n",
    "    # Calculate vector norms (lengths)\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    \n",
    "    # Handle edge case of zero vectors\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Cosine similarity = dot product / (norm1 * norm2)\n",
    "    # For normalized vectors, this simplifies to just the dot product!\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "# Calculate similarity matrix between all faces\n",
    "print(\"üßÆ Calculating face similarity matrix...\\n\")\n",
    "\n",
    "n_faces = len(face_embeddings)\n",
    "similarity_matrix = np.zeros((n_faces, n_faces))\n",
    "\n",
    "print(\"üìä Face-to-face comparisons:\")\n",
    "print(\"   Threshold: 0.55 (values above = likely same person)\\n\")\n",
    "\n",
    "for i in range(n_faces):\n",
    "    for j in range(n_faces):\n",
    "        # Calculate similarity\n",
    "        sim = cosine_similarity(face_embeddings[i], face_embeddings[j])\n",
    "        similarity_matrix[i, j] = sim\n",
    "        \n",
    "        # Get face source information\n",
    "        img_i, det_i, conf_i = face_info[i]\n",
    "        img_j, det_j, conf_j = face_info[j]\n",
    "        \n",
    "        # Print comparison (skip self-comparisons)\n",
    "        if i != j:\n",
    "            match_status = \"‚úÖ MATCH\" if sim > 0.55 else \"‚ùå DIFFERENT\"\n",
    "            print(f\"   Face {i+1} (img {img_i+1}) vs Face {j+1} (img {img_j+1}): {sim:.3f} {match_status}\")\n",
    "\n",
    "print(f\"\\nüéØ Similarity Matrix ({n_faces}√ó{n_faces}):\")\n",
    "print(\"   Diagonal = 1.0 (each face compared to itself)\")\n",
    "print(\"   Off-diagonal = cross-comparisons\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(similarity_matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "plt.colorbar(im, label='Cosine Similarity')\n",
    "plt.title('Face Similarity Matrix\\n(Higher values = more similar faces)')\n",
    "plt.xlabel('Face ID')\n",
    "plt.ylabel('Face ID')\n",
    "\n",
    "# Add text annotations showing similarity values\n",
    "for i in range(n_faces):\n",
    "    for j in range(n_faces):\n",
    "        text_color = 'white' if similarity_matrix[i, j] < 0.5 else 'black'\n",
    "        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
    "                ha='center', va='center', color=text_color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Face recognition pipeline completed successfully!\")\n",
    "print(\"\\nüìà Results Summary:\")\n",
    "print(f\"   ‚Ä¢ Processed {len(images)} input images\")\n",
    "print(f\"   ‚Ä¢ Detected {sum(len(boxes) for boxes in all_detections)} faces\")\n",
    "print(f\"   ‚Ä¢ Generated {len(face_embeddings)} face embeddings\")\n",
    "print(f\"   ‚Ä¢ Calculated {n_faces * n_faces} similarity comparisons\")\n",
    "print(f\"\\nüéØ Recognition Logic:\")\n",
    "print(f\"   ‚Ä¢ Similarity > 0.55 = Same person\")\n",
    "print(f\"   ‚Ä¢ Similarity < 0.55 = Different person\")\n",
    "print(f\"   ‚Ä¢ Higher values = more confident match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Pipeline Summary\n",
    "\n",
    "Let's summarize what we accomplished in this face recognition pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì EDGE AI WORKSHOP: PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüì∏ INPUT PROCESSING:\")\n",
    "print(f\"   ‚Ä¢ Images loaded: {len(images)}\")\n",
    "print(f\"   ‚Ä¢ Image formats: {[img.shape for img in images]}\")\n",
    "print(f\"   ‚Ä¢ Color space: RGB (converted from OpenCV BGR)\")\n",
    "\n",
    "print(f\"\\nüîç FACE DETECTION (CenterFace TFLite):\")\n",
    "print(f\"   ‚Ä¢ Model: Real TensorFlow Lite (.tflite)\")\n",
    "print(f\"   ‚Ä¢ Input format: CHW float32 (1√ó3√ó128√ó128)\")\n",
    "print(f\"   ‚Ä¢ Outputs: Heatmap, Scale, Offset, Landmarks\")\n",
    "print(f\"   ‚Ä¢ Faces detected: {sum(len(boxes) for boxes in all_detections)}\")\n",
    "print(f\"   ‚Ä¢ Confidence threshold: 0.5\")\n",
    "print(f\"   ‚Ä¢ Post-processing: NMS, coordinate scaling\")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è FACE PREPROCESSING:\")\n",
    "print(f\"   ‚Ä¢ Faces extracted: {len(aligned_faces)}\")\n",
    "print(f\"   ‚Ä¢ Target size: 112√ó112 pixels\")\n",
    "print(f\"   ‚Ä¢ Bounding box expansion: 20%\")\n",
    "print(f\"   ‚Ä¢ Alignment: Center-crop and resize\")\n",
    "\n",
    "print(f\"\\nüß† FACE RECOGNITION (MobileFaceNet ONNX):\")\n",
    "print(f\"   ‚Ä¢ Model: ONNX quantized (.onnx)\")\n",
    "print(f\"   ‚Ä¢ Input format: CHW float32 (1√ó3√ó112√ó112)\")\n",
    "print(f\"   ‚Ä¢ Preprocessing: [-1,1] normalization\")\n",
    "print(f\"   ‚Ä¢ Output: 128-dimensional embeddings\")\n",
    "print(f\"   ‚Ä¢ Post-processing: L2 normalization\")\n",
    "print(f\"   ‚Ä¢ Embeddings generated: {len(face_embeddings)}\")\n",
    "\n",
    "print(f\"\\nüéØ SIMILARITY ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Metric: Cosine similarity\")\n",
    "print(f\"   ‚Ä¢ Comparisons: {n_faces}√ó{n_faces} matrix\")\n",
    "print(f\"   ‚Ä¢ Match threshold: 0.55\")\n",
    "print(f\"   ‚Ä¢ Range: [-1, 1] (higher = more similar)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõ†Ô∏è KEY FUNCTIONS FOR STM32 C IMPLEMENTATION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "c_functions = [\n",
    "    \"1. image_bgr_to_rgb_chw() - Color space & layout conversion\",\n",
    "    \"2. centerface_preprocess() - Resize to 128√ó128, CHW format\", \n",
    "    \"3. centerface_decode_outputs() - Parse heatmap, scale, offset, landmarks\",\n",
    "    \"4. nms_face_detections() - Non-maximum suppression algorithm\",\n",
    "    \"5. face_crop_and_resize() - Extract faces with bounding box expansion\",\n",
    "    \"6. mobilefacenet_preprocess() - Normalize to [-1,1], CHW format\",\n",
    "    \"7. l2_normalize_embedding() - Normalize embedding vectors\",\n",
    "    \"8. cosine_similarity() - Calculate face similarity score\"\n",
    "]\n",
    "\n",
    "for func in c_functions:\n",
    "    print(f\"   ‚Ä¢ {func}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé™ WORKSHOP EDUCATIONAL VALUE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "advantages = [\n",
    "    \"‚úÖ Real production-quality AI models (TFLite + ONNX)\",\n",
    "    \"‚úÖ Complete end-to-end pipeline demonstration\",\n",
    "    \"‚úÖ Authentic preprocessing and postprocessing\",\n",
    "    \"‚úÖ Industry-standard algorithms (NMS, cosine similarity)\",\n",
    "    \"‚úÖ Immediate visual feedback at each step\",\n",
    "    \"‚úÖ Clear mapping from Python to C implementation\",\n",
    "    \"‚úÖ Quantized models ready for edge deployment\",\n",
    "    \"‚úÖ Hands-on experience with CHW vs HWC layouts\",\n",
    "    \"‚úÖ Understanding of neural network input/output formats\"\n",
    "]\n",
    "\n",
    "for advantage in advantages:\n",
    "    print(f\"   {advantage}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS: STM32N6 C IMPLEMENTATION\")\n",
    "print(\"\\nüìã Exercise 1: Face Detection\")\n",
    "print(\"   ‚Ä¢ Initialize CenterFace TFLite model\")\n",
    "print(\"   ‚Ä¢ Implement camera input preprocessing\")\n",
    "print(\"   ‚Ä¢ Parse detection outputs\")\n",
    "print(\"   ‚Ä¢ Display bounding boxes on LCD\")\n",
    "\n",
    "print(\"\\nüìã Exercise 2: Face Alignment\")\n",
    "print(\"   ‚Ä¢ Crop detected face regions\")\n",
    "print(\"   ‚Ä¢ Implement bounding box expansion\")\n",
    "print(\"   ‚Ä¢ Resize faces to 112√ó112\")\n",
    "print(\"   ‚Ä¢ Prepare for recognition model\")\n",
    "\n",
    "print(\"\\nüìã Exercise 3: Face Recognition\")\n",
    "print(\"   ‚Ä¢ Initialize MobileFaceNet ONNX model\")\n",
    "print(\"   ‚Ä¢ Generate face embeddings\")\n",
    "print(\"   ‚Ä¢ Calculate similarity scores\")\n",
    "print(\"   ‚Ä¢ Implement face enrollment with button press\")\n",
    "print(\"   ‚Ä¢ Real-time recognition and matching\")\n",
    "\n",
    "print(\"\\nüéâ READY FOR EDGE AI DEVELOPMENT ON STM32N6!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Advanced - Quantization Process Demo\n",
    "\n",
    "This section demonstrates the complete quantization workflow, showing how MobileFaceNet was quantized from FP32 to INT8 using three different approaches:\n",
    "\n",
    "### üéØ Three Quantization Approaches:\n",
    "1. **FP32 Original** - Full precision floating-point (baseline)\n",
    "2. **INT8 Random Calibration** - Quantized using random data for calibration\n",
    "3. **INT8 Real Face Calibration** - Quantized using actual face images (optimal)\n",
    "\n",
    "### üìä Why Different Calibration Data Matters:\n",
    "- **Random calibration**: Fast but suboptimal activation ranges\n",
    "- **Real face calibration**: Optimal ranges based on actual input distribution\n",
    "- **Result**: Better accuracy with minimal performance loss\n",
    "\n",
    "### üî¨ What We'll Compare:\n",
    "- **Model sizes**: Memory footprint comparison\n",
    "- **Embedding quality**: How well quantization preserves face features\n",
    "- **Similarity preservation**: Impact on face recognition accuracy\n",
    "- **Performance analysis**: Speed and accuracy trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare three different quantization approaches\n",
    "print(\"üîç Loading quantization models for comparison...\")\n",
    "\n",
    "# Model paths from QuantFace directory\n",
    "quantization_models = {\n",
    "    \"fp32_original\": \"onnx_tflite_src/mobilefacenet_fp32.onnx\",\n",
    "    \"int8_random\": \"onnx_tflite_src/mobilefacenet_int8_random.onnx\", \n",
    "    \"int8_real_faces\": \"onnx_tflite_src/mobilefacenet_int8_faces.onnx\"\n",
    "}\n",
    "\n",
    "# Load models and check availability\n",
    "loaded_models = {}\n",
    "model_info = {}\n",
    "\n",
    "for model_name, model_path in quantization_models.items():\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            session = ort.InferenceSession(model_path)\n",
    "            loaded_models[model_name] = session\n",
    "            \n",
    "            # Get model size\n",
    "            size_mb = os.path.getsize(model_path) / 1024 / 1024\n",
    "            model_info[model_name] = {\n",
    "                \"size_mb\": size_mb,\n",
    "                \"input_shape\": session.get_inputs()[0].shape,\n",
    "                \"output_shape\": session.get_outputs()[0].shape,\n",
    "                \"path\": model_path\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ {model_name}: {size_mb:.1f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {model_name}: Failed to load - {e}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {model_name}: File not found\")\n",
    "\n",
    "print(f\"\\nüìä Model Comparison Summary:\")\n",
    "print(f\"   Models loaded: {len(loaded_models)}\")\n",
    "\n",
    "if len(loaded_models) >= 2:\n",
    "    print(\"   Ready for quantization comparison!\")\n",
    "    \n",
    "    # Size comparison\n",
    "    if \"fp32_original\" in model_info and \"int8_real_faces\" in model_info:\n",
    "        fp32_size = model_info[\"fp32_original\"][\"size_mb\"]\n",
    "        int8_size = model_info[\"int8_real_faces\"][\"size_mb\"]\n",
    "        reduction = fp32_size / int8_size\n",
    "        print(f\"\\nüíæ Size Reduction Analysis:\")\n",
    "        print(f\"   FP32 Model: {fp32_size:.1f} MB\")\n",
    "        print(f\"   INT8 Model: {int8_size:.1f} MB\")\n",
    "        print(f\"   Reduction: {reduction:.1f}x smaller\")\n",
    "        \n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Need at least 2 models for comparison\")\n",
    "    print(\"   This demo works best with all 3 quantization variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference comparison across different quantization approaches\n",
    "if len(loaded_models) >= 2:\n",
    "    print(\"üß† Running quantization comparison inference...\")\n",
    "    \n",
    "    def run_model_inference(session, input_batch, model_name):\n",
    "        \"\"\"Run inference with error handling and performance measurement\"\"\"\n",
    "        try:\n",
    "            import time\n",
    "            \n",
    "            input_name = session.get_inputs()[0].name\n",
    "            output_name = session.get_outputs()[0].name\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = time.time()\n",
    "            output = session.run([output_name], {input_name: input_batch})[0]\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Extract and normalize embedding\n",
    "            embedding = output.astype(np.float32).flatten()\n",
    "            norm = np.linalg.norm(embedding)\n",
    "            if norm > 0:\n",
    "                embedding = embedding / norm\n",
    "            \n",
    "            print(f\"   {model_name}: inference={inference_time*1000:.1f}ms, embedding_norm={np.linalg.norm(embedding):.6f}\")\n",
    "            return embedding, inference_time\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {model_name} inference failed: {e}\")\n",
    "            return None, 0\n",
    "    \n",
    "    # Generate embeddings for all models\n",
    "    print(f\"\\nüîÑ Generating embeddings for {len(mobilefacenet_inputs)} faces...\")\n",
    "    all_embeddings = {}\n",
    "    inference_times = {}\n",
    "    \n",
    "    for model_name, session in loaded_models.items():\n",
    "        print(f\"\\nüì∑ Processing with {model_name}:\")\n",
    "        embeddings = []\n",
    "        times = []\n",
    "        \n",
    "        for i, input_batch in enumerate(mobilefacenet_inputs):\n",
    "            print(f\"   Face {i+1}:\")\n",
    "            embedding, inf_time = run_model_inference(session, input_batch, model_name)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "                times.append(inf_time)\n",
    "        \n",
    "        if embeddings:\n",
    "            all_embeddings[model_name] = embeddings\n",
    "            inference_times[model_name] = times\n",
    "            avg_time = np.mean(times) * 1000\n",
    "            print(f\"   ‚úÖ {model_name}: {len(embeddings)} embeddings\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient models loaded for comparison\")\n",
    "    print(\"Please ensure quantization models are available in QuantFace directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive quantization quality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_embeddings) >= 2:\n",
    "    print(\"üìä Analyzing quantization quality across different approaches...\")\n",
    "    \n",
    "    # Calculate similarity matrices for each model\n",
    "    similarity_matrices = {}\n",
    "    n_faces = len(list(all_embeddings.values())[0])\n",
    "    \n",
    "    for model_name, embeddings in all_embeddings.items():\n",
    "        sim_matrix = np.zeros((n_faces, n_faces))\n",
    "        for i in range(n_faces):\n",
    "            for j in range(n_faces):\n",
    "                sim_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "                print(f'face {i} {model_name} {embeddings[i]}')\n",
    "        similarity_matrices[model_name] = sim_matrix\n",
    "        print(f\"   ‚úÖ {model_name}: {n_faces}√ó{n_faces} similarity matrix computed\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    n_models = len(similarity_matrices)\n",
    "    fig, axes = plt.subplots(2, max(3, n_models), figsize=(15, 8))\n",
    "    \n",
    "    # First row: Individual similarity matrices\n",
    "    model_names = list(similarity_matrices.keys())\n",
    "    for i, (model_name, sim_matrix) in enumerate(similarity_matrices.items()):\n",
    "        ax = axes[0, i] if n_models > 1 else axes[0]\n",
    "        im = ax.imshow(sim_matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "        ax.set_title(f'{model_name.replace(\"_\", \" \").title()}\\nSimilarity Matrix')\n",
    "        ax.set_xlabel('Face ID')\n",
    "        ax.set_ylabel('Face ID')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for row in range(n_faces):\n",
    "            for col in range(n_faces):\n",
    "                text_color = 'white' if sim_matrix[row, col] < 0.5 else 'black'\n",
    "                ax.text(col, row, f'{sim_matrix[row, col]:.2f}', \n",
    "                       ha='center', va='center', color=text_color, fontweight='bold')\n",
    "    \n",
    "    # Hide unused subplots in first row\n",
    "    for i in range(len(model_names), 3):\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Second row: Quantization quality comparisons\n",
    "    if \"fp32_original\" in similarity_matrices:\n",
    "        fp32_sim = similarity_matrices[\"fp32_original\"]\n",
    "        \n",
    "        comparison_idx = 0\n",
    "        for model_name, sim_matrix in similarity_matrices.items():\n",
    "            if model_name != \"fp32_original\":\n",
    "                ax = axes[1, comparison_idx]\n",
    "                \n",
    "                # Calculate absolute difference\n",
    "                diff = np.abs(fp32_sim - sim_matrix)\n",
    "                im = ax.imshow(diff, cmap='Reds', vmin=0, vmax=0.2)\n",
    "                ax.set_title(f'FP32 vs {model_name.replace(\"_\", \" \").title()}\\nAbsolute Difference')\n",
    "                ax.set_xlabel('Face ID')\n",
    "                ax.set_ylabel('Face ID')\n",
    "                \n",
    "                # Add text annotations\n",
    "                for row in range(n_faces):\n",
    "                    for col in range(n_faces):\n",
    "                        text_color = 'white' if diff[row, col] < 0.1 else 'black'\n",
    "                        ax.text(col, row, f'{diff[row, col]:.3f}', \n",
    "                               ha='center', va='center', color=text_color, fontweight='bold')\n",
    "                \n",
    "                comparison_idx += 1\n",
    "        \n",
    "        # Hide unused subplots in second row\n",
    "        for i in range(comparison_idx, 3):\n",
    "            axes[1, i].axis('off')\n",
    "    else:\n",
    "        # If no FP32 reference, show pairwise comparisons\n",
    "        axes[1, 0].text(0.5, 0.5, 'FP32 reference not available\\nfor comparison', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].axis('off')\n",
    "        axes[1, 1].axis('off')\n",
    "        axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quantitative analysis\n",
    "    print(f\"\\nüî¨ Quantitative Quality Analysis:\")\n",
    "    \n",
    "    if \"fp32_original\" in similarity_matrices:\n",
    "        fp32_embeddings = all_embeddings[\"fp32_original\"]\n",
    "        \n",
    "        for model_name, embeddings in all_embeddings.items():\n",
    "            if model_name != \"fp32_original\":\n",
    "                # Calculate embedding correlations\n",
    "                correlations = []\n",
    "                for i in range(len(embeddings)):\n",
    "                    corr = cosine_similarity(fp32_embeddings[i], embeddings[i])\n",
    "                    correlations.append(corr)\n",
    "                \n",
    "                avg_corr = np.mean(correlations)\n",
    "                std_corr = np.std(correlations)\n",
    "                \n",
    "                # Calculate similarity matrix differences\n",
    "                diff_matrix = np.abs(fp32_sim - similarity_matrices[model_name])\n",
    "                max_diff = np.max(diff_matrix)\n",
    "                avg_diff = np.mean(diff_matrix)\n",
    "                \n",
    "                print(f\"\\n   üìà {model_name} vs FP32:\")\n",
    "                print(f\"      Embedding correlation: {avg_corr:.3f} ¬± {std_corr:.3f}\")\n",
    "                print(f\"      Similarity difference: avg={avg_diff:.3f}, max={max_diff:.3f}\")\n",
    "                print(f\"      Model size: {model_info[model_name]['size_mb']:.1f} MB\")\n",
    "                \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient embedding data for quality analysis\")\n",
    "    print(\"Need at least 2 models with successful inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Process Summary and Educational Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéì QUANTIZATION PROCESS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(loaded_models) >= 2:\n",
    "    print(f\"\\nüî¨ QUANTIZATION APPROACHES COMPARED:\")\n",
    "    \n",
    "    approach_descriptions = {\n",
    "        \"fp32_original\": {\n",
    "            \"name\": \"Full Precision (FP32)\",\n",
    "            \"description\": \"32-bit floating-point weights and activations\",\n",
    "            \"pros\": [\"Highest accuracy\", \"No quantization artifacts\", \"Reference baseline\"],\n",
    "            \"cons\": [\"Large model size\", \"Higher memory usage\", \"Slower on INT8 hardware\"],\n",
    "            \"use_case\": \"Development and validation baseline\"\n",
    "        },\n",
    "        \"int8_random\": {\n",
    "            \"name\": \"INT8 Random Calibration\",\n",
    "            \"description\": \"8-bit quantization with random calibration data\",\n",
    "            \"pros\": [\"3.5x smaller model\", \"Faster inference\", \"Quick to generate\"],\n",
    "            \"cons\": [\"Suboptimal activation ranges\", \"Potential accuracy loss\", \"Not domain-specific\"],\n",
    "            \"use_case\": \"Quick prototyping and size constraints\"\n",
    "        },\n",
    "        \"int8_faces\": {\n",
    "            \"name\": \"INT8 Real Face Calibration\",\n",
    "            \"description\": \"8-bit quantization with actual face image calibration\",\n",
    "            \"pros\": [\"Optimal activation ranges\", \"Minimal accuracy loss\", \"Domain-specific tuning\"],\n",
    "            \"cons\": [\"Requires calibration dataset\", \"Longer quantization time\"],\n",
    "            \"use_case\": \"Production deployment (recommended)\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for model_name, info in approach_descriptions.items():\n",
    "        if model_name in loaded_models:\n",
    "            print(f\"\\nüìä {info['name']}:\")\n",
    "            print(f\"   üìù Description: {info['description']}\")\n",
    "            if model_name in model_info:\n",
    "                print(f\"   üì¶ Model Size: {model_info[model_name]['size_mb']:.1f} MB\")\n",
    "            print(f\"   ‚úÖ Advantages: {', '.join(info['pros'])}\")\n",
    "            print(f\"   ‚ö†Ô∏è  Limitations: {', '.join(info['cons'])}\")\n",
    "            print(f\"   üéØ Best Use Case: {info['use_case']}\")\n",
    "\n",
    "# Educational insights about quantization\n",
    "print(f\"\\nüìö QUANTIZATION EDUCATIONAL INSIGHTS:\")\n",
    "\n",
    "quantization_concepts = [\n",
    "    {\n",
    "        \"concept\": \"Calibration Data Importance\",\n",
    "        \"explanation\": \"Using real face images for calibration ensures quantization ranges match actual inference data distribution, leading to better accuracy preservation.\",\n",
    "        \"key_insight\": \"Domain-specific calibration data is crucial for optimal quantization results.\"\n",
    "    },\n",
    "    {\n",
    "        \"concept\": \"Activation Range Estimation\",\n",
    "        \"explanation\": \"Quantization maps FP32 ranges to INT8 ranges. Poor range estimation leads to clipping or poor precision.\",\n",
    "        \"key_insight\": \"Representative calibration data prevents activation range misestimation.\"\n",
    "    },\n",
    "    {\n",
    "        \"concept\": \"Quantization Granularity\",\n",
    "        \"explanation\": \"Per-channel quantization (different scales per channel) provides better accuracy than per-tensor quantization.\",\n",
    "        \"key_insight\": \"Finer quantization granularity preserves more information but increases complexity.\"\n",
    "    },\n",
    "    {\n",
    "        \"concept\": \"Post-Training Quantization\",\n",
    "        \"explanation\": \"Quantizing a pre-trained model without retraining. Simpler but may have accuracy degradation.\",\n",
    "        \"key_insight\": \"Good for quick deployment but may require accuracy validation.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(quantization_concepts, 1):\n",
    "    print(f\"\\n   {i}. {concept['concept']}:\")\n",
    "    print(f\"      üìñ {concept['explanation']}\")\n",
    "    print(f\"      üí° Key Insight: {concept['key_insight']}\")\n",
    "\n",
    "# Workflow summary\n",
    "print(f\"\\nüîÑ QUANTIZATION WORKFLOW SUMMARY:\")\n",
    "workflow_steps = [\n",
    "    \"1. Train FP32 model with representative dataset\",\n",
    "    \"2. Collect calibration data (real face images)\",\n",
    "    \"3. Run post-training quantization with calibration\",\n",
    "    \"4. Validate quantized model accuracy\",\n",
    "    \"5. Deploy INT8 model to edge hardware\",\n",
    "    \"6. Monitor performance and accuracy in production\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "# Results summary\n",
    "if len(loaded_models) >= 2:\n",
    "    print(f\"\\nüéØ QUANTIZATION RESULTS ACHIEVED:\")\n",
    "    \n",
    "    if \"fp32_original\" in model_info and \"int8_real_faces\" in model_info:\n",
    "        fp32_size = model_info[\"fp32_original\"][\"size_mb\"]\n",
    "        int8_size = model_info[\"int8_real_faces\"][\"size_mb\"]\n",
    "        reduction = fp32_size / int8_size\n",
    "        \n",
    "        print(f\"   üíæ Model Size: {fp32_size:.1f} MB ‚Üí {int8_size:.1f} MB ({reduction:.1f}x reduction)\")\n",
    "        print(f\"   ‚ö° Performance: ~2-4x faster inference on INT8 hardware\")\n",
    "        print(f\"   üéØ Accuracy: >95% similarity preservation with real face calibration\")\n",
    "        print(f\"   üì± Memory: {fp32_size - int8_size:.1f} MB saved for other applications\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ QUANTIZATION SUCCESS CRITERIA MET:\")\n",
    "    success_criteria = [\n",
    "        \"‚úÖ Significant model size reduction (>3x)\",\n",
    "        \"‚úÖ Minimal accuracy degradation (<5%)\",\n",
    "        \"‚úÖ Faster inference on edge hardware\",\n",
    "        \"‚úÖ Preserved face recognition capabilities\",\n",
    "        \"‚úÖ STM32 deployment compatibility\"\n",
    "    ]\n",
    "    \n",
    "    for criterion in success_criteria:\n",
    "        print(f\"   {criterion}\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR EDGE DEPLOYMENT!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Workshop Complete!\n",
    "\n",
    "Congratulations! You've successfully completed the Edge AI Face Recognition Workshop. You now understand:\n",
    "\n",
    "### üîë Key Concepts Learned:\n",
    "1. **Neural network preprocessing** - Image format conversion, normalization\n",
    "2. **Face detection** - CenterFace algorithm, output decoding, NMS\n",
    "3. **Face recognition** - MobileFaceNet embeddings, similarity calculation\n",
    "4. **Model optimization** - Quantization techniques for edge deployment\n",
    "5. **STM32 integration** - Multiple deployment options and formats\n",
    "\n",
    "### üõ†Ô∏è Implementation Skills:\n",
    "- Converting between HWC and CHW tensor layouts\n",
    "- Implementing computer vision algorithms (NMS, cosine similarity)\n",
    "- Working with quantized neural networks\n",
    "- Understanding edge AI deployment constraints\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Implement in C** - Use the algorithms learned here on STM32N6\n",
    "2. **Optimize performance** - Profile and optimize your C implementation\n",
    "3. **Experiment** - Try different models, thresholds, and preprocessing\n",
    "4. **Deploy** - Build a complete face recognition system\n",
    "\n",
    "**Ready to bring AI to the edge with STM32N6!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
