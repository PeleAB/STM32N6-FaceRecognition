{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection & Recognition Pipeline Simulation\n",
    "\n",
    "This notebook simulates the complete face detection and recognition pipeline that runs on the STM32N6570-DK embedded board. It uses the same trump2.jpg image and models to help validate and understand the pipeline behavior.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Input Processing**: Load and preprocess trump2.jpg (480x480 centered in 800x480 with black padding)\n",
    "2. **Face Detection**: Use CenterFace ONNX model to detect faces\n",
    "3. **Face Cropping**: Extract and align detected face regions\n",
    "4. **Face Recognition**: Use MobileFaceNet ONNX model to generate embeddings\n",
    "5. **Similarity Calculation**: Compare with target embedding\n",
    "\n",
    "## Expected Results\n",
    "Based on the embedded implementation hints:\n",
    "- **Detection coordinates**: cx=245.497, cy=261.098, w=246.817, h=313.832\n",
    "- **Face landmarks**: lx=200.221, ly=226.327, rx=297.419, ry=225.127\n",
    "- **Cropped face first pixels**: [80, 172, 40, 96, 100, ...]\n",
    "- **Recognition embedding sample**: [0.176, -0.224, 0.179, 0.051, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mort\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration matching embedded system\n",
    "NN_WIDTH = 128\n",
    "NN_HEIGHT = 128\n",
    "NN_BPP = 3\n",
    "\n",
    "IMG_BUFFER_WIDTH = 800\n",
    "IMG_BUFFER_HEIGHT = 480\n",
    "\n",
    "FR_WIDTH = 112  # Face recognition input width\n",
    "FR_HEIGHT = 112  # Face recognition input height\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "print(\"🚀 Face Detection & Recognition Pipeline Simulation\")\n",
    "print(f\"📐 Neural Network Input: {NN_WIDTH}x{NN_HEIGHT}x{NN_BPP}\")\n",
    "print(f\"📸 Camera Frame: {IMG_BUFFER_WIDTH}x{IMG_BUFFER_HEIGHT}\")\n",
    "print(f\"👤 Face Recognition: {FR_WIDTH}x{FR_HEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Input Image\n",
    "\n",
    "Load trump2.jpg and create the same dual buffer system used in the embedded implementation:\n",
    "- `img_buffer`: 480x480 centered in 800x480 RGB565 (for face cropping)\n",
    "- `nn_rgb`: 128x128 RGB888 (for neural network input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb888_to_rgb565(r, g, b):\n",
    "    \"\"\"Convert RGB888 to RGB565\"\"\"\n",
    "    r565 = (r >> 3) & 0x1F\n",
    "    g565 = (g >> 2) & 0x3F\n",
    "    b565 = (b >> 3) & 0x1F\n",
    "    return (r565 << 11) | (g565 << 5) | b565\n",
    "\n",
    "def rgb565_to_rgb888(rgb565):\n",
    "    \"\"\"Convert RGB565 back to RGB888\"\"\"\n",
    "    r = ((rgb565 >> 11) & 0x1F) << 3\n",
    "    g = ((rgb565 >> 5) & 0x3F) << 2\n",
    "    b = (rgb565 & 0x1F) << 3\n",
    "    return r, g, b\n",
    "\n",
    "def create_dual_buffers(img_path):\n",
    "    \"\"\"Create dual buffers matching embedded implementation\"\"\"\n",
    "    \n",
    "    # Load trump2.jpg\n",
    "    print(f\"📂 Loading {img_path}...\")\n",
    "    img = Image.open(img_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    print(f\"   Original: {img.size} {img.mode}\")\n",
    "    \n",
    "    # Create img_buffer: 480x480 centered in 800x480\n",
    "    img_buffer_480 = img.resize((480, 480), Image.LANCZOS)\n",
    "    img_buffer_rgb = np.zeros((IMG_BUFFER_HEIGHT, IMG_BUFFER_WIDTH, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Center the 480x480 image in 800x480 canvas\n",
    "    left_padding = (IMG_BUFFER_WIDTH - 480) // 2  # 160 pixels\n",
    "    img_buffer_rgb[:, left_padding:left_padding+480, :] = np.array(img_buffer_480)\n",
    "    \n",
    "    # Create nn_rgb: 128x128 RGB888\n",
    "    nn_rgb = np.array(img.resize((NN_WIDTH, NN_HEIGHT), Image.LANCZOS), dtype=np.uint8)\n",
    "    \n",
    "    print(f\"✅ img_buffer: {img_buffer_rgb.shape} (480x480 centered)\")\n",
    "    print(f\"✅ nn_rgb: {nn_rgb.shape}\")\n",
    "    \n",
    "    return img_buffer_rgb, nn_rgb\n",
    "\n",
    "# Load trump2.jpg and create dual buffers\n",
    "trump2_path = \"Exercises/SamplePics/trump2.jpg\"\n",
    "img_buffer, nn_rgb = create_dual_buffers(trump2_path)\n",
    "\n",
    "# Visualize the dual buffer setup\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(img_buffer)\n",
    "axes[0].set_title(f\"img_buffer ({IMG_BUFFER_WIDTH}x{IMG_BUFFER_HEIGHT})\\n480x480 centered with black padding\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(nn_rgb)\n",
    "axes[1].set_title(f\"nn_rgb ({NN_WIDTH}x{NN_HEIGHT})\\nNeural Network Input\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🔍 First few nn_rgb pixels: {nn_rgb.flatten()[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Face Detection with CenterFace\n",
    "\n",
    "Use the CenterFace ONNX model to detect faces in the 128x128 input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_detection(rgb_img):\n",
    "    \"\"\"Preprocess RGB image for CenterFace model (CHW format, float32)\"\"\"\n",
    "    # Convert to float32 and normalize to [0, 1]\n",
    "    img_float = rgb_img.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert from HWC to CHW format\n",
    "    img_chw = np.transpose(img_float, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_batch = np.expand_dims(img_chw, axis=0)\n",
    "    \n",
    "    print(f\"   Detection input shape: {img_batch.shape}\")\n",
    "    print(f\"   Detection input range: [{img_batch.min():.3f}, {img_batch.max():.3f}]\")\n",
    "    \n",
    "    return img_batch\n",
    "\n",
    "def run_face_detection(nn_rgb_input):\n",
    "    \"\"\"Run CenterFace face detection\"\"\"\n",
    "    \n",
    "    model_path = \"converted_models/centerface_OE_3_2_0.onnx\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🧠 Loading CenterFace model: {model_path}\")\n",
    "    \n",
    "    # Create ONNX Runtime session\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    \n",
    "    # Print model info\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape\n",
    "    print(f\"   Model input: {input_name} {input_shape}\")\n",
    "    \n",
    "    for i, output in enumerate(session.get_outputs()):\n",
    "        print(f\"   Model output {i}: {output.name} {output.shape}\")\n",
    "    \n",
    "    # Preprocess input\n",
    "    detection_input = preprocess_for_detection(nn_rgb_input)\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"🚀 Running face detection inference...\")\n",
    "    outputs = session.run(None, {input_name: detection_input})\n",
    "    \n",
    "    print(f\"✅ Detection complete. Outputs: {len(outputs)}\")\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f\"   Output {i} shape: {output.shape}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Run face detection\n",
    "detection_outputs = run_face_detection(nn_rgb)\n",
    "\n",
    "if detection_outputs:\n",
    "    print(f\"\\n📊 Detection outputs summary:\")\n",
    "    for i, output in enumerate(detection_outputs):\n",
    "        print(f\"   Output {i}: shape={output.shape}, range=[{output.min():.3f}, {output.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Post-process Detection Results\n",
    "\n",
    "Extract face bounding boxes and landmarks from CenterFace outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_centerface(outputs, confidence_threshold=0.5):\n",
    "    \"\"\"Post-process CenterFace outputs to extract face detections\"\"\"\n",
    "    \n",
    "    if not outputs or len(outputs) < 3:\n",
    "        print(\"❌ Invalid detection outputs\")\n",
    "        return []\n",
    "    \n",
    "    # CenterFace typically outputs: [heatmap, scale, offset, landmarks]\n",
    "    heatmap = outputs[0]  # Face confidence heatmap\n",
    "    scale = outputs[1]    # Face scale/size\n",
    "    offset = outputs[2]   # Position offset\n",
    "    \n",
    "    # Optional landmarks if available\n",
    "    landmarks = outputs[3] if len(outputs) > 3 else None\n",
    "    \n",
    "    print(f\"📊 Post-processing CenterFace outputs:\")\n",
    "    print(f\"   Heatmap: {heatmap.shape}\")\n",
    "    print(f\"   Scale: {scale.shape}\")\n",
    "    print(f\"   Offset: {offset.shape}\")\n",
    "    if landmarks is not None:\n",
    "        print(f\"   Landmarks: {landmarks.shape}\")\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    # Remove batch dimension\n",
    "    heatmap = heatmap[0]\n",
    "    scale = scale[0]\n",
    "    offset = offset[0]\n",
    "    if landmarks is not None:\n",
    "        landmarks = landmarks[0]\n",
    "    \n",
    "    # Find peaks in heatmap\n",
    "    h, w = heatmap.shape[1:]\n",
    "    \n",
    "    # Simple peak detection - find maximum confidence point\n",
    "    max_conf = np.max(heatmap)\n",
    "    \n",
    "    if max_conf > confidence_threshold:\n",
    "        # Find position of maximum confidence\n",
    "        max_pos = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
    "        cy, cx = max_pos[1], max_pos[2]  # y, x coordinates\n",
    "        \n",
    "        # Extract scale at this position\n",
    "        face_w = scale[0, cy, cx] * 4  # Scale factor\n",
    "        face_h = scale[1, cy, cx] * 4\n",
    "        \n",
    "        # Extract offset\n",
    "        offset_x = offset[0, cy, cx]\n",
    "        offset_y = offset[1, cy, cx]\n",
    "        \n",
    "        # Calculate final coordinates (scale to original image)\n",
    "        scale_x = NN_WIDTH / w\n",
    "        scale_y = NN_HEIGHT / h\n",
    "        \n",
    "        final_cx = (cx + offset_x) * scale_x\n",
    "        final_cy = (cy + offset_y) * scale_y\n",
    "        final_w = face_w * scale_x\n",
    "        final_h = face_h * scale_y\n",
    "        \n",
    "        detection = {\n",
    "            'confidence': float(max_conf),\n",
    "            'cx': float(final_cx),\n",
    "            'cy': float(final_cy),\n",
    "            'w': float(final_w),\n",
    "            'h': float(final_h)\n",
    "        }\n",
    "        \n",
    "        # Extract landmarks if available\n",
    "        if landmarks is not None:\n",
    "            lx = (cx + landmarks[0, cy, cx]) * scale_x\n",
    "            ly = (cy + landmarks[1, cy, cx]) * scale_y\n",
    "            rx = (cx + landmarks[2, cy, cx]) * scale_x\n",
    "            ry = (cy + landmarks[3, cy, cx]) * scale_y\n",
    "            \n",
    "            detection.update({\n",
    "                'lx': float(lx),\n",
    "                'ly': float(ly),\n",
    "                'rx': float(rx),\n",
    "                'ry': float(ry)\n",
    "            })\n",
    "        \n",
    "        detections.append(detection)\n",
    "        \n",
    "        print(f\"✅ Face detected:\")\n",
    "        print(f\"   Confidence: {detection['confidence']:.3f}\")\n",
    "        print(f\"   Center: ({detection['cx']:.1f}, {detection['cy']:.1f})\")\n",
    "        print(f\"   Size: {detection['w']:.1f} x {detection['h']:.1f}\")\n",
    "        if 'lx' in detection:\n",
    "            print(f\"   Left eye: ({detection['lx']:.1f}, {detection['ly']:.1f})\")\n",
    "            print(f\"   Right eye: ({detection['rx']:.1f}, {detection['ry']:.1f})\")\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Post-process detection results\n",
    "detections = postprocess_centerface(detection_outputs)\n",
    "\n",
    "# Compare with embedded system expected results\n",
    "if detections:\n",
    "    det = detections[0]\n",
    "    print(f\"\\n🎯 Comparison with embedded system:\")\n",
    "    print(f\"   Expected: cx=245.497, cy=261.098, w=246.817, h=313.832\")\n",
    "    print(f\"   Detected: cx={det['cx']:.3f}, cy={det['cy']:.3f}, w={det['w']:.3f}, h={det['h']:.3f}\")\n",
    "    \n",
    "    if 'lx' in det:\n",
    "        print(f\"   Expected landmarks: lx=200.221, ly=226.327, rx=297.419, ry=225.127\")\n",
    "        print(f\"   Detected landmarks: lx={det['lx']:.3f}, ly={det['ly']:.3f}, rx={det['rx']:.3f}, ry={det['ry']:.3f}\")\n",
    "else:\n",
    "    print(\"⚠️ No faces detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Face Cropping and Alignment\n",
    "\n",
    "Crop the detected face region from the original 800x480 image buffer for face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_align_face(img_buffer, detection, target_size=(112, 112)):\n",
    "    \"\"\"Crop and align face from img_buffer based on detection\"\"\"\n",
    "    \n",
    "    if not detection:\n",
    "        return None\n",
    "    \n",
    "    print(f\"✂️ Cropping face region...\")\n",
    "    \n",
    "    # Scale coordinates from 128x128 detection to 800x480 img_buffer\n",
    "    # The face was detected on the centered 480x480 region\n",
    "    scale_x = 480 / NN_WIDTH  # 480/128 = 3.75\n",
    "    scale_y = 480 / NN_HEIGHT  # 480/128 = 3.75\n",
    "    \n",
    "    # Account for the 160-pixel left padding in img_buffer\n",
    "    left_padding = (IMG_BUFFER_WIDTH - 480) // 2  # 160 pixels\n",
    "    \n",
    "    # Scale detection coordinates to 480x480 region\n",
    "    cx_480 = detection['cx'] * scale_x\n",
    "    cy_480 = detection['cy'] * scale_y\n",
    "    w_480 = detection['w'] * scale_x\n",
    "    h_480 = detection['h'] * scale_y\n",
    "    \n",
    "    # Translate to 800x480 coordinates (add left padding)\n",
    "    cx_800 = cx_480 + left_padding\n",
    "    cy_800 = cy_480  # No vertical padding\n",
    "    \n",
    "    print(f\"   Detection in img_buffer: cx={cx_800:.1f}, cy={cy_800:.1f}, w={w_480:.1f}, h={h_480:.1f}\")\n",
    "    \n",
    "    # Calculate crop rectangle\n",
    "    x1 = int(cx_800 - w_480/2)\n",
    "    y1 = int(cy_800 - h_480/2)\n",
    "    x2 = int(cx_800 + w_480/2)\n",
    "    y2 = int(cy_800 + h_480/2)\n",
    "    \n",
    "    # Ensure bounds are within image\n",
    "    x1 = max(0, x1)\n",
    "    y1 = max(0, y1)\n",
    "    x2 = min(IMG_BUFFER_WIDTH, x2)\n",
    "    y2 = min(IMG_BUFFER_HEIGHT, y2)\n",
    "    \n",
    "    print(f\"   Crop rectangle: ({x1}, {y1}) to ({x2}, {y2})\")\n",
    "    \n",
    "    # Crop face region\n",
    "    cropped_face = img_buffer[y1:y2, x1:x2]\n",
    "    \n",
    "    # Resize to target size for face recognition\n",
    "    if cropped_face.size > 0:\n",
    "        cropped_pil = Image.fromarray(cropped_face)\n",
    "        aligned_face = np.array(cropped_pil.resize(target_size, Image.LANCZOS))\n",
    "        \n",
    "        print(f\"   Cropped size: {cropped_face.shape}\")\n",
    "        print(f\"   Aligned size: {aligned_face.shape}\")\n",
    "        print(f\"   First few pixels: {aligned_face.flatten()[:10].tolist()}\")\n",
    "        \n",
    "        return aligned_face\n",
    "    else:\n",
    "        print(\"❌ Invalid crop region\")\n",
    "        return None\n",
    "\n",
    "# Crop face if detection was successful\n",
    "cropped_face = None\n",
    "if detections:\n",
    "    cropped_face = crop_and_align_face(img_buffer, detections[0])\n",
    "    \n",
    "    if cropped_face is not None:\n",
    "        # Visualize the cropping result\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original img_buffer with detection box\n",
    "        img_with_detection = img_buffer.copy()\n",
    "        det = detections[0]\n",
    "        \n",
    "        # Draw detection box\n",
    "        scale_x = 480 / NN_WIDTH\n",
    "        scale_y = 480 / NN_HEIGHT\n",
    "        left_padding = (IMG_BUFFER_WIDTH - 480) // 2\n",
    "        \n",
    "        cx = det['cx'] * scale_x + left_padding\n",
    "        cy = det['cy'] * scale_y\n",
    "        w = det['w'] * scale_x\n",
    "        h = det['h'] * scale_y\n",
    "        \n",
    "        x1, y1 = int(cx - w/2), int(cy - h/2)\n",
    "        x2, y2 = int(cx + w/2), int(cy + h/2)\n",
    "        \n",
    "        cv2.rectangle(img_with_detection, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.circle(img_with_detection, (int(cx), int(cy)), 3, (255, 0, 0), -1)\n",
    "        \n",
    "        axes[0].imshow(img_with_detection)\n",
    "        axes[0].set_title(f\"Detection in img_buffer\\n({IMG_BUFFER_WIDTH}x{IMG_BUFFER_HEIGHT})\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(nn_rgb)\n",
    "        axes[1].set_title(f\"Neural Network Input\\n({NN_WIDTH}x{NN_HEIGHT})\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(cropped_face)\n",
    "        axes[2].set_title(f\"Cropped & Aligned Face\\n({FR_WIDTH}x{FR_HEIGHT})\")\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Compare with embedded system expected results\n",
    "        print(f\"\\n🎯 Comparison with embedded system:\")\n",
    "        print(f\"   Expected first pixels: [80, 172, 40, 96, 100, ...]\")\n",
    "        print(f\"   Actual first pixels: {cropped_face.flatten()[:5].tolist()}\")\n",
    "else:\n",
    "    print(\"⚠️ No face detected, cannot crop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Face Recognition with MobileFaceNet\n",
    "\n",
    "Use the MobileFaceNet ONNX model to generate face embeddings from the cropped face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_recognition(face_rgb):\n",
    "    \"\"\"Preprocess face image for MobileFaceNet model\"\"\"\n",
    "    \n",
    "    # Convert to float32 and normalize to [-1, 1] (typical for face recognition)\n",
    "    img_float = face_rgb.astype(np.float32) / 255.0\n",
    "    img_normalized = (img_float - 0.5) / 0.5  # Scale to [-1, 1]\n",
    "    \n",
    "    # Convert from HWC to CHW format\n",
    "    img_chw = np.transpose(img_normalized, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_batch = np.expand_dims(img_chw, axis=0)\n",
    "    \n",
    "    print(f\"   Recognition input shape: {img_batch.shape}\")\n",
    "    print(f\"   Recognition input range: [{img_batch.min():.3f}, {img_batch.max():.3f}]\")\n",
    "    print(f\"   First 10 values: {img_batch.flatten()[:10]}\")\n",
    "    \n",
    "    return img_batch\n",
    "\n",
    "def run_face_recognition(face_image):\n",
    "    \"\"\"Run MobileFaceNet face recognition\"\"\"\n",
    "    \n",
    "    model_path = \"converted_models/mobilefacenet_int8_faces_OE_3_2_0.onnx\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🧠 Loading MobileFaceNet model: {model_path}\")\n",
    "    \n",
    "    # Create ONNX Runtime session\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    \n",
    "    # Print model info\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape\n",
    "    print(f\"   Model input: {input_name} {input_shape}\")\n",
    "    \n",
    "    for i, output in enumerate(session.get_outputs()):\n",
    "        print(f\"   Model output {i}: {output.name} {output.shape}\")\n",
    "    \n",
    "    # Preprocess input\n",
    "    recognition_input = preprocess_for_recognition(face_image)\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"🚀 Running face recognition inference...\")\n",
    "    outputs = session.run(None, {input_name: recognition_input})\n",
    "    \n",
    "    print(f\"✅ Recognition complete. Outputs: {len(outputs)}\")\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f\"   Output {i} shape: {output.shape}\")\n",
    "    \n",
    "    # Extract embedding (typically the first/only output)\n",
    "    embedding = outputs[0][0]  # Remove batch dimension\n",
    "    \n",
    "    print(f\"🔢 Generated embedding:\")\n",
    "    print(f\"   Shape: {embedding.shape}\")\n",
    "    print(f\"   Range: [{embedding.min():.6f}, {embedding.max():.6f}]\")\n",
    "    print(f\"   First 10 values: {embedding[:10]}\")\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Run face recognition if we have a cropped face\n",
    "embedding = None\n",
    "if cropped_face is not None:\n",
    "    embedding = run_face_recognition(cropped_face)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        # Compare with embedded system expected results\n",
    "        expected_embedding_sample = [0.176, -0.224, 0.179, 0.051, 0.093, 0.068, 0.073, -0.064, 0.009, 0.062]\n",
    "        \n",
    "        print(f\"\\n🎯 Comparison with embedded system:\")\n",
    "        print(f\"   Expected first 10: {expected_embedding_sample}\")\n",
    "        print(f\"   Actual first 10: {embedding[:10].tolist()}\")\n",
    "        \n",
    "        # Calculate similarity between expected and actual\n",
    "        if len(embedding) >= len(expected_embedding_sample):\n",
    "            actual_sample = embedding[:len(expected_embedding_sample)]\n",
    "            cosine_sim = np.dot(actual_sample, expected_embedding_sample) / \\\n",
    "                        (np.linalg.norm(actual_sample) * np.linalg.norm(expected_embedding_sample))\n",
    "            print(f\"   Cosine similarity: {cosine_sim:.6f}\")\n",
    "else:\n",
    "    print(\"⚠️ No cropped face available for recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Similarity Calculation\n",
    "\n",
    "Calculate similarity between generated embedding and target embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(emb1, emb2):\n",
    "    \"\"\"Calculate cosine similarity between two embeddings\"\"\"\n",
    "    if len(emb1) != len(emb2):\n",
    "        print(f\"❌ Embedding size mismatch: {len(emb1)} vs {len(emb2)}\")\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "# Target embedding from embedded system\n",
    "target_embedding = np.array([\n",
    "    -0.00725003, 0.09593857, 0.09686094, -0.10923475, 0.03024563, -0.13004985, -0.12128278, -0.0124026,\n",
    "    0.04728478, 0.12707546, -0.11148556, -0.03225464, -0.12289561, -0.06271324, 0.12324576, 0.07674231,\n",
    "    0.09737839, -0.14817831, -0.10203385, 0.05896754, 0.14928015, 0.01753759, -0.1260086, 0.10908744,\n",
    "    -0.11444927, 0.06037515, 0.01441137, 0.13059959, -0.0344786, -0.15837167, 0.07553311, 0.04526917,\n",
    "    0.04599683, 0.10152522, -0.08480689, 0.02473134, -0.04895976, 0.02287412, 0.08345234, 0.09106179,\n",
    "    -0.17253238, 0.04099809, 0.06850467, -0.00768601, -0.05633068, 0.08909649, 0.07233251, 0.00117169,\n",
    "    0.02267973, 0.15040962, 0.18063718, 0.05905029, -0.00567992, 0.07136418, -0.01371038, 0.13587898,\n",
    "    -0.01705864, -0.05830159, -0.09989999, 0.06535304, 0.12640677, 0.06754113, -0.00900604, -0.08072298,\n",
    "    0.00625439, 0.03872489, 0.15242957, -0.08392397, 0.02590879, 0.06696369, 0.15042576, 0.09741592,\n",
    "    -0.03923986, 0.00168709, -0.07881001, -0.02893417, -0.05657582, -0.103101, 0.04700356, -0.00980601,\n",
    "    -0.09602147, -0.13890378, 0.05948354, 0.11650706, 0.00841825, -0.02130741, 0.11611883, 0.07256811,\n",
    "    -0.01129891, -0.08870837, -0.13336483, 0.0149093, 0.16941628, -0.04602769, 0.07664789, -0.09471361,\n",
    "    -0.13546045, 0.00278911, -0.005896, -0.1697745, -0.09366001, -0.01200075, -0.08691258, 0.17627689,\n",
    "    0.09977022, 0.08791678, 0.17640786, 0.08195918, 0.05133393, -0.08878682, 0.11774844, 0.08634339,\n",
    "    -0.01815109, -0.0249792, -0.05723954, -0.0833647, 0.07762875, 0.10181235, 0.05247587, -0.0012934,\n",
    "    0.01773486, -0.01575616, -0.06047363, -0.00354687, 0.01191169, -0.17828241, 0.05426167, -0.00662142\n",
    "], dtype=np.float32)\n",
    "\n",
    "if embedding is not None:\n",
    "    print(f\"🧮 Calculating similarity with target embedding...\")\n",
    "    \n",
    "    # Ensure both embeddings have the same size\n",
    "    min_size = min(len(embedding), len(target_embedding))\n",
    "    emb_truncated = embedding[:min_size]\n",
    "    target_truncated = target_embedding[:min_size]\n",
    "    \n",
    "    similarity = calculate_cosine_similarity(emb_truncated, target_truncated)\n",
    "    \n",
    "    print(f\"✅ Similarity calculation complete:\")\n",
    "    print(f\"   Embedding size: {len(emb_truncated)}\")\n",
    "    print(f\"   Target size: {len(target_truncated)}\")\n",
    "    print(f\"   Cosine similarity: {similarity:.6f}\")\n",
    "    \n",
    "    # Interpret similarity score\n",
    "    if similarity > 0.8:\n",
    "        status = \"🟢 Very High (Likely same person)\"\n",
    "    elif similarity > 0.6:\n",
    "        status = \"🟡 High (Probably same person)\"\n",
    "    elif similarity > 0.4:\n",
    "        status = \"🟠 Medium (Uncertain)\"\n",
    "    else:\n",
    "        status = \"🔴 Low (Different person)\"\n",
    "    \n",
    "    print(f\"   Match status: {status}\")\n",
    "    \n",
    "    # Visualize similarity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(emb_truncated[:50], 'b-', label='Generated Embedding', alpha=0.7)\n",
    "    plt.plot(target_truncated[:50], 'r-', label='Target Embedding', alpha=0.7)\n",
    "    plt.title(f'Embedding Comparison (First 50 dimensions)\\nCosine Similarity: {similarity:.6f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(target_truncated, emb_truncated, alpha=0.6)\n",
    "    plt.xlabel('Target Embedding')\n",
    "    plt.ylabel('Generated Embedding')\n",
    "    plt.title('Embedding Correlation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal line for perfect correlation\n",
    "    min_val = min(np.min(target_truncated), np.min(emb_truncated))\n",
    "    max_val = max(np.max(target_truncated), np.max(emb_truncated))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='Perfect Correlation')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No embedding available for similarity calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook simulates the complete face detection and recognition pipeline that runs on the STM32N6570-DK embedded board. \n",
    "\n",
    "### Pipeline Results:\n",
    "1. **Input Processing**: ✅ Loaded trump2.jpg and created dual buffers\n",
    "2. **Face Detection**: ✅ Used CenterFace ONNX model \n",
    "3. **Face Cropping**: ✅ Extracted and aligned face region\n",
    "4. **Face Recognition**: ✅ Used MobileFaceNet ONNX model\n",
    "5. **Similarity**: ✅ Calculated cosine similarity with target\n",
    "\n",
    "### Key Insights:\n",
    "- The pipeline processes the same trump2.jpg image used in the embedded system\n",
    "- Results can be compared with embedded system outputs for validation\n",
    "- This simulation helps understand the complete AI pipeline behavior\n",
    "- Useful for debugging and validating student implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
