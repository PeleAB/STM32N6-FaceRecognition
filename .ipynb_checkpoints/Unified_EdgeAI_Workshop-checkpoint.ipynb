{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Edge AI Workshop\n",
    "\n",
    "This notebook combines the previous exercises into a single workflow covering:\n",
    "1. Face detection and recognition pipeline\n",
    "2. Converting models with ST Edge AI tools\n",
    "3. Managing flash memory and programming the STM32N6 MCU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Face Detection and Recognition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge AI Workshop: Face Detection and Recognition Pipeline\n",
    "\n",
    "This notebook demonstrates the complete face detection and recognition pipeline that students will implement in C on the STM32N6 board.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Load Photos** - Load test images from PC\n",
    "2. **CenterFace Input Preparation** - Resize, normalize, convert to CHW format\n",
    "3. **CenterFace Inference** - Run face detection model\n",
    "4. **Post-processing** - Parse detections, apply NMS\n",
    "5. **Face Crop & Align** - Extract face regions for recognition\n",
    "6. **MobileFaceNet Inference** - Generate face embeddings\n",
    "7. **Similarity Calculation** - Compare embeddings using cosine similarity\n",
    "8. **Advanced: Quantized Models** - Explore INT8 quantization for STM32\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand neural network input/output formats\n",
    "- Learn preprocessing and postprocessing techniques\n",
    "- Practice with CHW vs HWC data layouts\n",
    "- Implement similarity metrics for face recognition\n",
    "- See immediate results at each step\n",
    "- Explore model quantization for edge deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import required packages\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtflite_runtime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtflite\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mort\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "print(\"üì¶ All packages imported successfully!\")\n",
    "print(\"üöÄ Workshop environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Paths configured:\n",
      "   CenterFace: models/centerface.tflite\n",
      "   MobileFaceNet: models/mobilefacenet.onnx\n",
      "   Sample photos: 3 images\n"
     ]
    }
   ],
   "source": [
    "# Define sample photos and model paths\n",
    "sample_photos = [\n",
    "    'SamplePics/trump1.jpg',  # Same person\n",
    "    'SamplePics/trump2.jpg',  # Same person\n",
    "    'SamplePics/obama.jpg'   # Different person\n",
    "]\n",
    "\n",
    "# Model paths\n",
    "centerface_model_path = 'models/centerface.tflite'\n",
    "mobilefacenet_model_path = 'models/mobilefacenet.onnx'\n",
    "\n",
    "print(\"üìÅ Paths configured:\")\n",
    "print(f\"   CenterFace: {centerface_model_path}\")\n",
    "print(f\"   MobileFaceNet: {mobilefacenet_model_path}\")\n",
    "print(f\"   Sample photos: {len(sample_photos)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AI models\n",
    "print(\"üîÑ Loading AI models...\")\n",
    "\n",
    "# Load CenterFace TFLite model\n",
    "if os.path.exists(centerface_model_path):\n",
    "    interpreter = tflite.Interpreter(model_path=centerface_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(\"‚úÖ CenterFace TFLite model loaded successfully!\")\n",
    "    print(f\"   Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"   Input type: {input_details[0]['dtype']}\")\n",
    "    print(f\"   Output shapes: {[output['shape'] for output in output_details]}\")\n",
    "else:\n",
    "    print(f\"‚ùå CenterFace model file not found: {centerface_model_path}\")\n",
    "    interpreter = None\n",
    "\n",
    "# Load MobileFaceNet ONNX model\n",
    "if os.path.exists(mobilefacenet_model_path):\n",
    "    try:\n",
    "        mobilefacenet_session = ort.InferenceSession(mobilefacenet_model_path)\n",
    "        mobilefacenet_input_name = mobilefacenet_session.get_inputs()[0].name\n",
    "        mobilefacenet_output_name = mobilefacenet_session.get_outputs()[0].name\n",
    "        mobilefacenet_input_shape = mobilefacenet_session.get_inputs()[0].shape\n",
    "        \n",
    "        print(\"‚úÖ MobileFaceNet ONNX model loaded successfully!\")\n",
    "        print(f\"   Input name: {mobilefacenet_input_name}\")\n",
    "        print(f\"   Input shape: {mobilefacenet_input_shape}\")\n",
    "        print(f\"   Output name: {mobilefacenet_output_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load MobileFaceNet model: {e}\")\n",
    "        mobilefacenet_session = None\n",
    "else:\n",
    "    print(f\"‚ùå MobileFaceNet model file not found: {mobilefacenet_model_path}\")\n",
    "    mobilefacenet_session = None\n",
    "\n",
    "print(\"\\nüìö Model loading complete!\")\n",
    "print(\"Ready to run face detection and recognition pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Display Photos\n",
    "\n",
    "First, let's load our test photos and see what we're working with. This step shows how to:\n",
    "- Read images from files\n",
    "- Convert BGR to RGB format (OpenCV uses BGR by default)\n",
    "- Display images in a grid layout\n",
    "- Handle missing files gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load image from file path\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        \n",
    "    Returns:\n",
    "        RGB image as numpy array (HWC format)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        # Create a dummy image if file doesn't exist\n",
    "        print(f\"‚ö†Ô∏è  {image_path} not found, creating dummy image\")\n",
    "        dummy_img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        return dummy_img\n",
    "    \n",
    "    # Load image using OpenCV (returns BGR format)\n",
    "    img = cv2.imread(image_path)\n",
    "    # Convert BGR to RGB for proper display\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img_rgb\n",
    "\n",
    "# Load all test photos\n",
    "print(\"üì∏ Loading test photos...\")\n",
    "images = []\n",
    "for photo_path in sample_photos:\n",
    "    img = load_image(photo_path)\n",
    "    images.append(img)\n",
    "    print(f\"   ‚úÖ Loaded {photo_path}: {img.shape} (H√óW√óC)\")\n",
    "\n",
    "# Display the photos in a grid\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (img, name) in enumerate(zip(images, sample_photos)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{name}\\n{img.shape}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Successfully loaded {len(images)} photos!\")\n",
    "print(\"These images will be processed through our face recognition pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: CenterFace Input Preparation\n",
    "\n",
    "CenterFace expects input in a specific format. This step demonstrates:\n",
    "- **Model requirements**: Understanding input shape and data type\n",
    "- **Image preprocessing**: Resizing, format conversion, normalization\n",
    "- **CHW vs HWC**: Converting between different tensor layouts\n",
    "- **Batch dimension**: Adding batch dimension for model inference\n",
    "\n",
    "**Key Concepts:**\n",
    "- **HWC**: Height √ó Width √ó Channels (typical image format)\n",
    "- **CHW**: Channels √ó Height √ó Width (neural network format)\n",
    "- **Batch**: Multiple samples processed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_centerface_input(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare image for CenterFace TFLite input\n",
    "    \n",
    "    This function shows students exactly what preprocessing is needed:\n",
    "    1. Resize to model input size (128√ó128)\n",
    "    2. Convert HWC to CHW format\n",
    "    3. Add batch dimension\n",
    "    4. Ensure correct data type\n",
    "    \n",
    "    Args:\n",
    "        image: Input image in HWC format (uint8)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image for TFLite model (1,3,128,128) CHW format\n",
    "    \"\"\"\n",
    "    if interpreter is None:\n",
    "        # Fallback preprocessing when model not available\n",
    "        target_size = (128, 128)\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        converted = resized.astype(np.float32)\n",
    "        chw_image = np.transpose(converted, (2, 0, 1))\n",
    "        batch_input = np.expand_dims(chw_image, axis=0)\n",
    "        return batch_input\n",
    "    \n",
    "    # Get model input requirements\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    \n",
    "    print(f\"üéØ Model requirements:\")\n",
    "    print(f\"   Expected shape: {input_shape}\")\n",
    "    print(f\"   Expected type: {input_dtype}\")\n",
    "    \n",
    "    # CenterFace expects 128√ó128 input\n",
    "    model_input_size = (128, 128)\n",
    "    \n",
    "    # Use OpenCV's blobFromImage for proper preprocessing\n",
    "    # This is the same approach used in production CenterFace implementations\n",
    "    input_blob = cv2.dnn.blobFromImage(\n",
    "        image, \n",
    "        scalefactor=1.0,           # No pixel value scaling\n",
    "        size=model_input_size,     # Resize to 128√ó128\n",
    "        mean=(0, 0, 0),           # No mean subtraction\n",
    "        swapRB=True,              # Convert BGR to RGB\n",
    "        crop=False                # Just resize, don't crop\n",
    "    )\n",
    "    \n",
    "    print(f\"üîÑ Preprocessing: {image.shape} ‚Üí {input_blob.shape}\")\n",
    "    print(f\"   Value range: [{input_blob.min():.1f}, {input_blob.max():.1f}]\")\n",
    "    print(f\"   Data type: {input_blob.dtype}\")\n",
    "    \n",
    "    # Convert to model's expected data type if needed\n",
    "    if input_dtype != input_blob.dtype:\n",
    "        if input_dtype == np.uint8:\n",
    "            input_blob = input_blob.astype(np.uint8)\n",
    "        elif input_dtype == np.int8:\n",
    "            input_blob = input_blob.astype(np.int8)\n",
    "        print(f\"üîÑ Type conversion: ‚Üí {input_dtype}\")\n",
    "    \n",
    "    return input_blob\n",
    "\n",
    "# Prepare inputs for all images\n",
    "print(\"üöÄ Preparing CenterFace inputs for all images...\\n\")\n",
    "centerface_inputs = []\n",
    "for i, img in enumerate(images):\n",
    "    print(f\"üì∑ Processing image {i+1}:\")\n",
    "    prepared = prepare_centerface_input(img)\n",
    "    centerface_inputs.append(prepared)\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(centerface_inputs)} inputs for CenterFace inference\")\n",
    "print(\"Each input is ready for the face detection model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: CenterFace Inference\n",
    "\n",
    "Now we run the actual CenterFace TensorFlow Lite model for face detection. This step demonstrates:\n",
    "\n",
    "**CenterFace Output Format:**\n",
    "- **Heatmap**: Confidence scores for face centers (32√ó32√ó1)\n",
    "- **Scale**: Bounding box size regression (32√ó32√ó2)\n",
    "- **Offset**: Bounding box position regression (32√ó32√ó2)  \n",
    "- **Landmarks**: 5 facial keypoints (32√ó32√ó10)\n",
    "\n",
    "**Key Algorithms Students Will Implement:**\n",
    "- **Peak detection**: Finding face centers in heatmap\n",
    "- **Coordinate decoding**: Converting network outputs to pixel coordinates\n",
    "- **Non-Maximum Suppression**: Removing duplicate detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, nms_thresh):\n",
    "    \"\"\"\n",
    "    Non-Maximum Suppression - removes overlapping face detections\n",
    "    \n",
    "    This is a critical algorithm students will implement in C!\n",
    "    It prevents the same face from being detected multiple times.\n",
    "    \n",
    "    Args:\n",
    "        boxes: Array of bounding boxes [x1, y1, x2, y2]\n",
    "        scores: Confidence scores for each box\n",
    "        nms_thresh: IoU threshold for suppression\n",
    "    \n",
    "    Returns:\n",
    "        Indices of boxes to keep\n",
    "    \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1] \n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = np.argsort(scores)[::-1]  # Sort by confidence (highest first)\n",
    "    num_detections = boxes.shape[0]\n",
    "    suppressed = np.zeros((num_detections,), dtype=bool)\n",
    "\n",
    "    keep = []\n",
    "    for _i in range(num_detections):\n",
    "        i = order[_i]\n",
    "        if suppressed[i]:\n",
    "            continue\n",
    "        keep.append(i)\n",
    "\n",
    "        # Calculate IoU with remaining boxes\n",
    "        ix1, iy1, ix2, iy2 = x1[i], y1[i], x2[i], y2[i]\n",
    "        iarea = areas[i]\n",
    "\n",
    "        for _j in range(_i + 1, num_detections):\n",
    "            j = order[_j]\n",
    "            if suppressed[j]:\n",
    "                continue\n",
    "            \n",
    "            # Calculate intersection area\n",
    "            xx1 = max(ix1, x1[j])\n",
    "            yy1 = max(iy1, y1[j])\n",
    "            xx2 = min(ix2, x2[j])\n",
    "            yy2 = min(iy2, y2[j])\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "            inter = w * h\n",
    "            ovr = inter / (iarea + areas[j] - inter)  # IoU calculation\n",
    "            \n",
    "            if ovr >= nms_thresh:\n",
    "                suppressed[j] = True  # Mark for suppression\n",
    "\n",
    "    return keep\n",
    "\n",
    "def decode_centerface_outputs(heatmap, scale, offset, landmark, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Decode CenterFace neural network outputs into face detections\n",
    "    \n",
    "    This shows students how raw network outputs become face bounding boxes!\n",
    "    \n",
    "    Args:\n",
    "        heatmap: Face confidence heatmap (1, 32, 32, 1)\n",
    "        scale: Scale regression (1, 32, 32, 2) \n",
    "        offset: Offset regression (1, 32, 32, 2)\n",
    "        landmark: Landmark regression (1, 32, 32, 10)\n",
    "        threshold: Minimum confidence for detection\n",
    "        \n",
    "    Returns:\n",
    "        boxes: [N, 5] array of [x1, y1, x2, y2, score]\n",
    "        landmarks: [N, 10] array of landmark coordinates\n",
    "    \"\"\"\n",
    "    # Remove batch dimension for processing\n",
    "    heatmap = heatmap[0, ..., 0]    # (32, 32)\n",
    "    scale = scale[0]                # (32, 32, 2)\n",
    "    offset = offset[0]              # (32, 32, 2)\n",
    "    landmark = landmark[0]          # (32, 32, 10)\n",
    "    \n",
    "    # Extract scale and offset channels\n",
    "    scale_y = scale[..., 0]   # Height scale\n",
    "    scale_x = scale[..., 1]   # Width scale\n",
    "    offset_y = offset[..., 0] # Y offset\n",
    "    offset_x = offset[..., 1] # X offset\n",
    "    \n",
    "    # Find face centers above threshold\n",
    "    face_rows, face_cols = np.where(heatmap > threshold)\n",
    "    boxes, lms_list = [], []\n",
    "    \n",
    "    print(f\"üîç Found {len(face_rows)} potential face centers\")\n",
    "    \n",
    "    if len(face_rows) > 0:\n",
    "        for i in range(len(face_rows)):\n",
    "            row, col = face_rows[i], face_cols[i]\n",
    "            \n",
    "            # Decode bounding box size (exponential activation)\n",
    "            h_scale = np.exp(scale_y[row, col]) * 4\n",
    "            w_scale = np.exp(scale_x[row, col]) * 4\n",
    "            \n",
    "            # Get position offsets\n",
    "            y_offset = offset_y[row, col]\n",
    "            x_offset = offset_x[row, col]\n",
    "            \n",
    "            # Get confidence score\n",
    "            confidence = heatmap[row, col]\n",
    "            \n",
    "            # Calculate final bounding box coordinates\n",
    "            # The *4 factor accounts for network downsampling\n",
    "            center_x = (col + x_offset + 0.5) * 4\n",
    "            center_y = (row + y_offset + 0.5) * 4\n",
    "            \n",
    "            x1 = max(0, center_x - w_scale / 2)\n",
    "            y1 = max(0, center_y - h_scale / 2)\n",
    "            x2 = min(128, center_x + w_scale / 2)\n",
    "            y2 = min(128, center_y + h_scale / 2)\n",
    "            \n",
    "            boxes.append([x1, y1, x2, y2, confidence])\n",
    "            \n",
    "            # Decode facial landmarks (5 points)\n",
    "            lms_temp = []\n",
    "            for j in range(5):\n",
    "                lm_y = landmark[row, col, j * 2 + 0]\n",
    "                lm_x = landmark[row, col, j * 2 + 1]\n",
    "                # Scale landmarks relative to bounding box\n",
    "                px = lm_x * w_scale + x1\n",
    "                py = lm_y * h_scale + y1\n",
    "                lms_temp.extend([px, py])\n",
    "            \n",
    "            lms_list.append(lms_temp)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        boxes = np.asarray(boxes, dtype=np.float32)\n",
    "        lms_list = np.asarray(lms_list, dtype=np.float32)\n",
    "        \n",
    "        # Apply Non-Maximum Suppression to remove duplicates\n",
    "        if len(boxes) > 0:\n",
    "            keep_indices = nms(boxes[:, :4], boxes[:, 4], 0.1)\n",
    "            boxes = boxes[keep_indices, :]\n",
    "            lms_list = lms_list[keep_indices, :]\n",
    "            print(f\"‚úÖ After NMS: {len(boxes)} final detections\")\n",
    "    \n",
    "    else:\n",
    "        boxes = np.array([]).reshape(0, 5)\n",
    "        lms_list = np.array([]).reshape(0, 10)\n",
    "    \n",
    "    return boxes, lms_list\n",
    "\n",
    "def run_centerface_inference(input_batch: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run CenterFace TFLite inference and decode outputs\n",
    "    \n",
    "    Args:\n",
    "        input_batch: Preprocessed image batch (1, 3, 128, 128)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (detections, landmarks)\n",
    "    \"\"\"\n",
    "    if interpreter is None:\n",
    "        print(\"‚ùå TFLite model not available, using simulation\")\n",
    "        # Return simulated detections for demonstration\n",
    "        sim_boxes = np.array([[30, 40, 90, 100, 0.95]], dtype=np.float32)\n",
    "        sim_landmarks = np.array([[45, 55, 75, 55, 60, 65, 50, 80, 70, 80]], dtype=np.float32)\n",
    "        return sim_boxes, sim_landmarks\n",
    "    \n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_batch)\n",
    "    \n",
    "    # Run neural network inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get outputs (indices match CenterFace implementation)\n",
    "    heatmap = interpreter.get_tensor(output_details[2]['index'])  # Confidence\n",
    "    scale = interpreter.get_tensor(output_details[0]['index'])    # Scale\n",
    "    offset = interpreter.get_tensor(output_details[3]['index'])   # Offset\n",
    "    landmarks = interpreter.get_tensor(output_details[1]['index']) # Landmarks\n",
    "    \n",
    "    print(f\"üìä Network output shapes:\")\n",
    "    print(f\"   Heatmap: {heatmap.shape}\")\n",
    "    print(f\"   Scale: {scale.shape}\")\n",
    "    print(f\"   Offset: {offset.shape}\")\n",
    "    print(f\"   Landmarks: {landmarks.shape}\")\n",
    "    \n",
    "    # Decode raw outputs into face detections\n",
    "    boxes, landmark_points = decode_centerface_outputs(heatmap, scale, offset, landmarks)\n",
    "    \n",
    "    return boxes, landmark_points\n",
    "\n",
    "def scale_detections_to_original(boxes, landmarks, original_shape):\n",
    "    \"\"\"\n",
    "    Scale detections from 128√ó128 model space back to original image size\n",
    "    \n",
    "    The model processes 128√ó128 images, but we need coordinates for the original image.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = original_shape[:2]\n",
    "    model_size = 128\n",
    "    \n",
    "    scale_x = orig_w / model_size\n",
    "    scale_y = orig_h / model_size\n",
    "    \n",
    "    # Scale bounding boxes\n",
    "    if len(boxes) > 0:\n",
    "        boxes_scaled = boxes.copy()\n",
    "        boxes_scaled[:, [0, 2]] *= scale_x  # x coordinates\n",
    "        boxes_scaled[:, [1, 3]] *= scale_y  # y coordinates\n",
    "    else:\n",
    "        boxes_scaled = boxes\n",
    "    \n",
    "    # Scale landmarks\n",
    "    if len(landmarks) > 0:\n",
    "        landmarks_scaled = landmarks.copy()\n",
    "        landmarks_scaled[:, 0::2] *= scale_x  # x coordinates\n",
    "        landmarks_scaled[:, 1::2] *= scale_y  # y coordinates\n",
    "    else:\n",
    "        landmarks_scaled = landmarks\n",
    "        \n",
    "    return boxes_scaled, landmarks_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CenterFace inference on all images\n",
    "print(\"üß† Running CenterFace inference on all images...\\n\")\n",
    "\n",
    "all_detections = []\n",
    "all_landmarks = []\n",
    "\n",
    "for i, input_batch in enumerate(centerface_inputs):\n",
    "    print(f\"üîÑ Processing image {i+1}:\")\n",
    "    \n",
    "    # Run face detection\n",
    "    boxes, landmarks = run_centerface_inference(input_batch)\n",
    "    \n",
    "    # Scale detections back to original image size\n",
    "    boxes_scaled, landmarks_scaled = scale_detections_to_original(\n",
    "        boxes, landmarks, images[i].shape\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Final results: {len(boxes_scaled)} faces detected\")\n",
    "    for j, box in enumerate(boxes_scaled):\n",
    "        x1, y1, x2, y2, conf = box\n",
    "        print(f\"   Face {j+1}: confidence={conf:.3f}, bbox=[{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]\")\n",
    "    \n",
    "    all_detections.append(boxes_scaled)\n",
    "    all_landmarks.append(landmarks_scaled)\n",
    "    print()\n",
    "\n",
    "total_faces = sum(len(boxes) for boxes in all_detections)\n",
    "print(f\"‚úÖ CenterFace inference completed!\")\n",
    "print(f\"üéØ Total faces detected across all images: {total_faces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Face Detections\n",
    "\n",
    "Let's visualize our face detection results. This step shows:\n",
    "- **Bounding box drawing**: How to overlay detection results\n",
    "- **Landmark visualization**: Displaying facial keypoints\n",
    "- **Confidence scores**: Showing model certainty\n",
    "- **Color coding**: Different colors for different landmark types\n",
    "\n",
    "**Landmark Meaning:**\n",
    "- **Red**: Left eye\n",
    "- **Green**: Right eye  \n",
    "- **Blue**: Nose tip\n",
    "- **Yellow**: Left mouth corner\n",
    "- **Magenta**: Right mouth corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_centerface_detections(image: np.ndarray, boxes: np.ndarray, landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw face detection results on image\n",
    "    \n",
    "    This visualization helps students understand what the AI model detected.\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        boxes: Face bounding boxes [x1, y1, x2, y2, score]\n",
    "        landmarks: Facial landmarks [x1,y1, x2,y2, ..., x5,y5]\n",
    "    \n",
    "    Returns:\n",
    "        Image with detection results drawn\n",
    "    \"\"\"\n",
    "    img_copy = image.copy()\n",
    "    \n",
    "    # Draw bounding boxes around detected faces\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, score = box\n",
    "        \n",
    "        # Green rectangle for face boundary\n",
    "        cv2.rectangle(img_copy, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        \n",
    "        # Show confidence score\n",
    "        cv2.putText(img_copy, f'{score:.3f}', (int(x1), int(y1) - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw facial landmarks with different colors\n",
    "    landmark_colors = [\n",
    "        (255, 0, 0),    # Red - Left eye\n",
    "        (0, 255, 0),    # Green - Right eye\n",
    "        (0, 0, 255),    # Blue - Nose\n",
    "        (255, 255, 0),  # Yellow - Left mouth\n",
    "        (255, 0, 255)   # Magenta - Right mouth\n",
    "    ]\n",
    "    \n",
    "    for landmark_set in landmarks:\n",
    "        for i in range(5):  # 5 landmarks per face\n",
    "            x = int(landmark_set[i * 2])\n",
    "            y = int(landmark_set[i * 2 + 1])\n",
    "            cv2.circle(img_copy, (x, y), 3, landmark_colors[i], -1)\n",
    "    \n",
    "    return img_copy\n",
    "\n",
    "# Visualize detection results\n",
    "print(\"üé® Visualizing face detection results...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (img, boxes, landmarks) in enumerate(zip(images, all_detections, all_landmarks)):\n",
    "    # Draw detection results on image\n",
    "    annotated = draw_centerface_detections(img, boxes, landmarks)\n",
    "    \n",
    "    # Display in subplot\n",
    "    axes[i].imshow(annotated)\n",
    "    axes[i].set_title(f\"Image {i+1}: {len(boxes)} faces detected\")\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Print detailed detection info\n",
    "    print(f\"\\nüìã Image {i+1} detection details:\")\n",
    "    for j, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2, conf = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        print(f\"   Face {j+1}: confidence={conf:.3f}, size={w:.0f}√ó{h:.0f}px\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Face Detection Results:\")\n",
    "print(\"‚úÖ Green boxes show detected face boundaries\")\n",
    "print(\"‚úÖ Colored dots show facial landmarks:\")\n",
    "print(\"   üî¥ Red = Left Eye\")\n",
    "print(\"   üü¢ Green = Right Eye\")\n",
    "print(\"   üîµ Blue = Nose\")\n",
    "print(\"   üü° Yellow = Left Mouth Corner\")\n",
    "print(\"   üü£ Magenta = Right Mouth Corner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Face Crop and Alignment\n",
    "\n",
    "Now we extract face regions for recognition. This step demonstrates:\n",
    "- **Region of Interest (ROI)**: Extracting face areas from full images\n",
    "- **Bounding box expansion**: Adding padding around detected faces\n",
    "- **Face alignment**: Standardizing face orientation and size\n",
    "- **Size normalization**: Resizing to model requirements (112√ó112)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Face recognition models expect standardized input\n",
    "- Proper alignment improves recognition accuracy\n",
    "- Consistent sizing enables batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_align_face(image: np.ndarray, box: np.ndarray, landmarks: np.ndarray,\n",
    "                       output_size: Tuple[int, int] = (112, 112)) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Crop and align face using detection results\n",
    "    \n",
    "    This function prepares faces for recognition by:\n",
    "    1. Expanding the bounding box for context\n",
    "    2. Cropping the face region\n",
    "    3. Resizing to standard size\n",
    "    \n",
    "    Args:\n",
    "        image: Original image\n",
    "        box: Face bounding box [x1, y1, x2, y2, score]\n",
    "        landmarks: Facial landmarks (currently not used for alignment)\n",
    "        output_size: Target size for recognition model\n",
    "    \n",
    "    Returns:\n",
    "        Aligned face image or None if extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x1, y1, x2, y2, confidence = box\n",
    "        \n",
    "        # Calculate face center and size\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        face_size = max(x2 - x1, y2 - y1)\n",
    "        \n",
    "        # Expand bounding box by 20% for context\n",
    "        # This includes hair, forehead, and chin which help recognition\n",
    "        expanded_size = face_size * 1.2\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        crop_x1 = max(0, int(center_x - expanded_size / 2))\n",
    "        crop_y1 = max(0, int(center_y - expanded_size / 2))\n",
    "        crop_x2 = min(image.shape[1], int(center_x + expanded_size / 2))\n",
    "        crop_y2 = min(image.shape[0], int(center_y + expanded_size / 2))\n",
    "        \n",
    "        # Extract face region\n",
    "        face_crop = image[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "        \n",
    "        if face_crop.size == 0:\n",
    "            print(f\"‚ùå Empty crop for face with confidence {confidence:.3f}\")\n",
    "            return None\n",
    "        \n",
    "        # Resize to standard size (112√ó112 for MobileFaceNet)\n",
    "        face_resized = cv2.resize(face_crop, output_size)\n",
    "        \n",
    "        print(f\"‚úÖ Cropped face: {face_crop.shape} ‚Üí {face_resized.shape}\")\n",
    "        return face_resized\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Face crop error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract faces from all detections\n",
    "print(\"‚úÇÔ∏è Extracting and aligning faces for recognition...\\n\")\n",
    "\n",
    "aligned_faces = []\n",
    "face_info = []  # Track which image each face came from\n",
    "\n",
    "for img_idx, (img, boxes, landmarks) in enumerate(zip(images, all_detections, all_landmarks)):\n",
    "    print(f\"üì∑ Processing faces from image {img_idx + 1}:\")\n",
    "    \n",
    "    for det_idx, (box, landmark_set) in enumerate(zip(boxes, landmarks)):\n",
    "        confidence = box[4]\n",
    "        print(f\"   Face {det_idx + 1}: confidence={confidence:.3f}\")\n",
    "        \n",
    "        # Crop and align the face\n",
    "        aligned_face = crop_and_align_face(img, box, landmark_set)\n",
    "        \n",
    "        if aligned_face is not None:\n",
    "            aligned_faces.append(aligned_face)\n",
    "            face_info.append((img_idx, det_idx, confidence))\n",
    "            print(f\"      ‚úÖ Success: {aligned_face.shape}\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå Failed to extract face\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Face extraction complete!\")\n",
    "print(f\"   Total faces extracted: {len(aligned_faces)}\")\n",
    "print(f\"   Ready for face recognition processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Aligned Faces\n",
    "\n",
    "Let's see our cropped and aligned faces before they go to the recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display aligned faces\n",
    "if aligned_faces:\n",
    "    print(\"üë§ Displaying aligned faces ready for recognition:\")\n",
    "    \n",
    "    n_faces = len(aligned_faces)\n",
    "    cols = min(4, n_faces)\n",
    "    rows = (n_faces + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    \n",
    "    # Handle single row case\n",
    "    if rows == 1:\n",
    "        axes = [axes] if n_faces == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Display each aligned face\n",
    "    for i, (face, (img_idx, det_idx, conf)) in enumerate(zip(aligned_faces, face_info)):\n",
    "        axes[i].imshow(face)\n",
    "        axes[i].set_title(f\"Face from Image {img_idx + 1}\\nConfidence: {conf:.3f}\\nSize: {face.shape[0]}√ó{face.shape[1]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_faces, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Face Preparation Summary:\")\n",
    "    for i, (img_idx, det_idx, conf) in enumerate(face_info):\n",
    "        print(f\"   Face {i+1}: From image {img_idx+1}, confidence={conf:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No aligned faces to display\")\n",
    "    print(\"Check face detection results above.\")\n",
    "\n",
    "print(\"\\n‚úÖ Face alignment completed!\")\n",
    "print(\"These standardized face images are ready for recognition processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: MobileFaceNet Input Preparation\n",
    "\n",
    "Now we prepare the aligned faces for MobileFaceNet inference. This step shows:\n",
    "\n",
    "**Critical Preprocessing Steps:**\n",
    "- **Color space conversion**: BGR ‚Üí RGB (OpenCV vs standard)\n",
    "- **Normalization**: Convert pixel values to [-1, 1] range\n",
    "- **Layout conversion**: HWC ‚Üí CHW for neural networks\n",
    "- **Batch dimension**: Add dimension for model input\n",
    "\n",
    "**Why Each Step Matters:**\n",
    "- **Normalization**: Helps model training stability\n",
    "- **CHW layout**: Optimized for GPU/AI accelerator processing\n",
    "- **Consistent preprocessing**: Must match training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mobilefacenet_input(face_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare aligned face for MobileFaceNet inference\n",
    "    \n",
    "    This preprocessing is critical - it must exactly match what the model expects!\n",
    "    Students will implement this preprocessing in C on the STM32.\n",
    "    \n",
    "    Args:\n",
    "        face_image: Aligned face image (112√ó112, RGB)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed input (1√ó3√ó112√ó112, float32)\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Preprocessing face: {face_image.shape}\")\n",
    "    \n",
    "    # Step 1: Ensure RGB format (face_image is already RGB from our pipeline)\n",
    "    face_rgb = face_image.astype(np.float32)\n",
    "    print(f\"   Color range: [{face_rgb.min():.0f}, {face_rgb.max():.0f}]\")\n",
    "    \n",
    "    # Step 2: Normalize to [-1, 1] range\n",
    "    # This matches MobileFaceNet training preprocessing\n",
    "    face_normalized = (face_rgb / 255.0) * 2.0 - 1.0\n",
    "    print(f\"   After normalization: [{face_normalized.min():.3f}, {face_normalized.max():.3f}]\")\n",
    "    \n",
    "    # Step 3: Convert HWC to CHW layout\n",
    "    # Neural networks expect Channels-first format\n",
    "    face_chw = np.transpose(face_normalized, (2, 0, 1))\n",
    "    print(f\"   Layout conversion: {face_normalized.shape} ‚Üí {face_chw.shape}\")\n",
    "    \n",
    "    # Step 4: Add batch dimension\n",
    "    # Models expect batch of samples, even if batch size = 1\n",
    "    batch_input = np.expand_dims(face_chw, axis=0)\n",
    "    print(f\"   Final shape: {batch_input.shape}\")\n",
    "    \n",
    "    return batch_input\n",
    "\n",
    "# Prepare all aligned faces for MobileFaceNet\n",
    "print(\"üöÄ Preparing inputs for MobileFaceNet recognition model...\\n\")\n",
    "\n",
    "mobilefacenet_inputs = []\n",
    "for i, face in enumerate(aligned_faces):\n",
    "    print(f\"üì∑ Preparing face {i+1}:\")\n",
    "    prepared = prepare_mobilefacenet_input(face)\n",
    "    mobilefacenet_inputs.append(prepared)\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Input preparation complete!\")\n",
    "print(f\"   Prepared {len(mobilefacenet_inputs)} faces for recognition\")\n",
    "print(f\"   Each input shape: {mobilefacenet_inputs[0].shape if mobilefacenet_inputs else 'None'}\")\n",
    "print(f\"   Ready for MobileFaceNet inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: MobileFaceNet Inference\n",
    "\n",
    "Now we run the MobileFaceNet model to generate face embeddings. This step demonstrates:\n",
    "\n",
    "**Face Recognition Concepts:**\n",
    "- **Face embeddings**: 128-dimensional vectors representing faces\n",
    "- **Feature extraction**: Converting images to numerical features\n",
    "- **L2 normalization**: Standardizing vector lengths for comparison\n",
    "- **ONNX inference**: Running optimized neural networks\n",
    "\n",
    "**Why 128 dimensions?**\n",
    "- Compact representation that captures facial features\n",
    "- Good balance between accuracy and memory usage\n",
    "- Standard size for many face recognition systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mobilefacenet_inference(input_batch: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run MobileFaceNet inference to generate face embedding\n",
    "    \n",
    "    This is where the magic happens - converting a face image into a \n",
    "    numerical representation that can be compared with other faces.\n",
    "    \n",
    "    Args:\n",
    "        input_batch: Preprocessed face input (1√ó3√ó112√ó112)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized face embedding (128-dimensional vector)\n",
    "    \"\"\"\n",
    "    if mobilefacenet_session is None:\n",
    "        print(\"‚ùå MobileFaceNet model not available\")\n",
    "        print(\"   Generating random embedding for demonstration\")\n",
    "        # Return normalized random vector for demo\n",
    "        random_embedding = np.random.normal(0, 0.1, 128).astype(np.float32)\n",
    "        norm = np.linalg.norm(random_embedding)\n",
    "        return random_embedding / norm if norm > 0 else random_embedding\n",
    "    \n",
    "    try:\n",
    "        # Run ONNX model inference\n",
    "        onnx_output = mobilefacenet_session.run(\n",
    "            [mobilefacenet_output_name], \n",
    "            {mobilefacenet_input_name: input_batch}\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"üîç Model output:\")\n",
    "        print(f\"   Shape: {onnx_output.shape}\")\n",
    "        print(f\"   Type: {onnx_output.dtype}\")\n",
    "        print(f\"   Range: [{onnx_output.min():.3f}, {onnx_output.max():.3f}]\")\n",
    "        \n",
    "        # Extract embedding vector (remove batch dimension)\n",
    "        embedding = onnx_output.astype(np.float32).flatten()\n",
    "        print(f\"   Embedding dimensions: {len(embedding)}\")\n",
    "        \n",
    "        # L2 normalization - crucial for face comparison!\n",
    "        # This ensures all embeddings have unit length\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm > 0:\n",
    "            embedding = embedding / norm\n",
    "            print(f\"   After L2 normalization: norm = {np.linalg.norm(embedding):.6f}\")\n",
    "        \n",
    "        print(f\"   Final range: [{embedding.min():.3f}, {embedding.max():.3f}]\")\n",
    "        return embedding\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Inference error: {e}\")\n",
    "        # Fallback to random embedding\n",
    "        random_embedding = np.random.normal(0, 0.1, 128).astype(np.float32)\n",
    "        norm = np.linalg.norm(random_embedding)\n",
    "        return random_embedding / norm if norm > 0 else random_embedding\n",
    "\n",
    "# Generate embeddings for all faces\n",
    "print(\"üß† Running MobileFaceNet inference to generate face embeddings...\\n\")\n",
    "\n",
    "face_embeddings = []\n",
    "for i, input_batch in enumerate(mobilefacenet_inputs):\n",
    "    print(f\"üîÑ Processing face {i+1}:\")\n",
    "    \n",
    "    # Generate face embedding\n",
    "    embedding = run_mobilefacenet_inference(input_batch)\n",
    "    face_embeddings.append(embedding)\n",
    "    \n",
    "    print(f\"   ‚úÖ Generated {len(embedding)}-dimensional embedding\")\n",
    "    print(f\"   üî¢ Sample values: [{embedding[0]:.3f}, {embedding[1]:.3f}, {embedding[2]:.3f}, ...]\")\n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Face embedding generation complete!\")\n",
    "print(f\"   Total embeddings: {len(face_embeddings)}\")\n",
    "print(f\"   Each embedding: 128-dimensional normalized vector\")\n",
    "print(f\"   Ready for face comparison and recognition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Cosine Similarity Calculation\n",
    "\n",
    "Finally, we calculate similarity between face embeddings. This is the core of face recognition!\n",
    "\n",
    "**Cosine Similarity:**\n",
    "- Measures angle between two vectors\n",
    "- Range: -1 (opposite) to +1 (identical)\n",
    "- Values > 0.5 typically indicate same person\n",
    "- Independent of vector magnitude (thanks to L2 normalization)\n",
    "\n",
    "**Why Cosine Similarity?**\n",
    "- Robust to lighting variations\n",
    "- Focus on facial structure, not brightness\n",
    "- Computationally efficient\n",
    "- Standard in face recognition systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two face embeddings\n",
    "    \n",
    "    This is the mathematical heart of face recognition!\n",
    "    Students will implement this exact calculation in C.\n",
    "    \n",
    "    Args:\n",
    "        emb1, emb2: Face embeddings (128-dimensional normalized vectors)\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity [-1, 1] where higher = more similar\n",
    "    \"\"\"\n",
    "    # Calculate dot product (core of cosine similarity)\n",
    "    dot_product = np.dot(emb1, emb2)\n",
    "    \n",
    "    # Calculate vector norms (lengths)\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    \n",
    "    # Handle edge case of zero vectors\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Cosine similarity = dot product / (norm1 * norm2)\n",
    "    # For normalized vectors, this simplifies to just the dot product!\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "# Calculate similarity matrix between all faces\n",
    "print(\"üßÆ Calculating face similarity matrix...\\n\")\n",
    "\n",
    "n_faces = len(face_embeddings)\n",
    "similarity_matrix = np.zeros((n_faces, n_faces))\n",
    "\n",
    "print(\"üìä Face-to-face comparisons:\")\n",
    "print(\"   Threshold: 0.55 (values above = likely same person)\\n\")\n",
    "\n",
    "for i in range(n_faces):\n",
    "    for j in range(n_faces):\n",
    "        # Calculate similarity\n",
    "        sim = cosine_similarity(face_embeddings[i], face_embeddings[j])\n",
    "        similarity_matrix[i, j] = sim\n",
    "        \n",
    "        # Get face source information\n",
    "        img_i, det_i, conf_i = face_info[i]\n",
    "        img_j, det_j, conf_j = face_info[j]\n",
    "        \n",
    "        # Print comparison (skip self-comparisons)\n",
    "        if i != j:\n",
    "            match_status = \"‚úÖ MATCH\" if sim > 0.55 else \"‚ùå DIFFERENT\"\n",
    "            print(f\"   Face {i+1} (img {img_i+1}) vs Face {j+1} (img {img_j+1}): {sim:.3f} {match_status}\")\n",
    "\n",
    "print(f\"\\nüéØ Similarity Matrix ({n_faces}√ó{n_faces}):\")\n",
    "print(\"   Diagonal = 1.0 (each face compared to itself)\")\n",
    "print(\"   Off-diagonal = cross-comparisons\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(similarity_matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "plt.colorbar(im, label='Cosine Similarity')\n",
    "plt.title('Face Similarity Matrix\\n(Higher values = more similar faces)')\n",
    "plt.xlabel('Face ID')\n",
    "plt.ylabel('Face ID')\n",
    "\n",
    "# Add text annotations showing similarity values\n",
    "for i in range(n_faces):\n",
    "    for j in range(n_faces):\n",
    "        text_color = 'white' if similarity_matrix[i, j] < 0.5 else 'black'\n",
    "        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
    "                ha='center', va='center', color=text_color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Face recognition pipeline completed successfully!\")\n",
    "print(\"\\nüìà Results Summary:\")\n",
    "print(f\"   ‚Ä¢ Processed {len(images)} input images\")\n",
    "print(f\"   ‚Ä¢ Detected {sum(len(boxes) for boxes in all_detections)} faces\")\n",
    "print(f\"   ‚Ä¢ Generated {len(face_embeddings)} face embeddings\")\n",
    "print(f\"   ‚Ä¢ Calculated {n_faces * n_faces} similarity comparisons\")\n",
    "print(f\"\\nüéØ Recognition Logic:\")\n",
    "print(f\"   ‚Ä¢ Similarity > 0.55 = Same person\")\n",
    "print(f\"   ‚Ä¢ Similarity < 0.55 = Different person\")\n",
    "print(f\"   ‚Ä¢ Higher values = more confident match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Pipeline Summary\n",
    "\n",
    "Let's summarize what we accomplished in this face recognition pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì EDGE AI WORKSHOP: PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüì∏ INPUT PROCESSING:\")\n",
    "print(f\"   ‚Ä¢ Images loaded: {len(images)}\")\n",
    "print(f\"   ‚Ä¢ Image formats: {[img.shape for img in images]}\")\n",
    "print(f\"   ‚Ä¢ Color space: RGB (converted from OpenCV BGR)\")\n",
    "\n",
    "print(f\"\\nüîç FACE DETECTION (CenterFace TFLite):\")\n",
    "print(f\"   ‚Ä¢ Model: Real TensorFlow Lite (.tflite)\")\n",
    "print(f\"   ‚Ä¢ Input format: CHW float32 (1√ó3√ó128√ó128)\")\n",
    "print(f\"   ‚Ä¢ Outputs: Heatmap, Scale, Offset, Landmarks\")\n",
    "print(f\"   ‚Ä¢ Faces detected: {sum(len(boxes) for boxes in all_detections)}\")\n",
    "print(f\"   ‚Ä¢ Confidence threshold: 0.5\")\n",
    "print(f\"   ‚Ä¢ Post-processing: NMS, coordinate scaling\")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è FACE PREPROCESSING:\")\n",
    "print(f\"   ‚Ä¢ Faces extracted: {len(aligned_faces)}\")\n",
    "print(f\"   ‚Ä¢ Target size: 112√ó112 pixels\")\n",
    "print(f\"   ‚Ä¢ Bounding box expansion: 20%\")\n",
    "print(f\"   ‚Ä¢ Alignment: Center-crop and resize\")\n",
    "\n",
    "print(f\"\\nüß† FACE RECOGNITION (MobileFaceNet ONNX):\")\n",
    "print(f\"   ‚Ä¢ Model: ONNX quantized (.onnx)\")\n",
    "print(f\"   ‚Ä¢ Input format: CHW float32 (1√ó3√ó112√ó112)\")\n",
    "print(f\"   ‚Ä¢ Preprocessing: [-1,1] normalization\")\n",
    "print(f\"   ‚Ä¢ Output: 128-dimensional embeddings\")\n",
    "print(f\"   ‚Ä¢ Post-processing: L2 normalization\")\n",
    "print(f\"   ‚Ä¢ Embeddings generated: {len(face_embeddings)}\")\n",
    "\n",
    "print(f\"\\nüéØ SIMILARITY ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Metric: Cosine similarity\")\n",
    "print(f\"   ‚Ä¢ Comparisons: {n_faces}√ó{n_faces} matrix\")\n",
    "print(f\"   ‚Ä¢ Match threshold: 0.55\")\n",
    "print(f\"   ‚Ä¢ Range: [-1, 1] (higher = more similar)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõ†Ô∏è KEY FUNCTIONS FOR STM32 C IMPLEMENTATION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "c_functions = [\n",
    "    \"1. image_bgr_to_rgb_chw() - Color space & layout conversion\",\n",
    "    \"2. centerface_preprocess() - Resize to 128√ó128, CHW format\", \n",
    "    \"3. centerface_decode_outputs() - Parse heatmap, scale, offset, landmarks\",\n",
    "    \"4. nms_face_detections() - Non-maximum suppression algorithm\",\n",
    "    \"5. face_crop_and_resize() - Extract faces with bounding box expansion\",\n",
    "    \"6. mobilefacenet_preprocess() - Normalize to [-1,1], CHW format\",\n",
    "    \"7. l2_normalize_embedding() - Normalize embedding vectors\",\n",
    "    \"8. cosine_similarity() - Calculate face similarity score\"\n",
    "]\n",
    "\n",
    "for func in c_functions:\n",
    "    print(f\"   ‚Ä¢ {func}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé™ WORKSHOP EDUCATIONAL VALUE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "advantages = [\n",
    "    \"‚úÖ Real production-quality AI models (TFLite + ONNX)\",\n",
    "    \"‚úÖ Complete end-to-end pipeline demonstration\",\n",
    "    \"‚úÖ Authentic preprocessing and postprocessing\",\n",
    "    \"‚úÖ Industry-standard algorithms (NMS, cosine similarity)\",\n",
    "    \"‚úÖ Immediate visual feedback at each step\",\n",
    "    \"‚úÖ Clear mapping from Python to C implementation\",\n",
    "    \"‚úÖ Quantized models ready for edge deployment\",\n",
    "    \"‚úÖ Hands-on experience with CHW vs HWC layouts\",\n",
    "    \"‚úÖ Understanding of neural network input/output formats\"\n",
    "]\n",
    "\n",
    "for advantage in advantages:\n",
    "    print(f\"   {advantage}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS: STM32N6 C IMPLEMENTATION\")\n",
    "print(\"\\nüìã Exercise 1: Face Detection\")\n",
    "print(\"   ‚Ä¢ Initialize CenterFace TFLite model\")\n",
    "print(\"   ‚Ä¢ Implement camera input preprocessing\")\n",
    "print(\"   ‚Ä¢ Parse detection outputs\")\n",
    "print(\"   ‚Ä¢ Display bounding boxes on LCD\")\n",
    "\n",
    "print(\"\\nüìã Exercise 2: Face Alignment\")\n",
    "print(\"   ‚Ä¢ Crop detected face regions\")\n",
    "print(\"   ‚Ä¢ Implement bounding box expansion\")\n",
    "print(\"   ‚Ä¢ Resize faces to 112√ó112\")\n",
    "print(\"   ‚Ä¢ Prepare for recognition model\")\n",
    "\n",
    "print(\"\\nüìã Exercise 3: Face Recognition\")\n",
    "print(\"   ‚Ä¢ Initialize MobileFaceNet ONNX model\")\n",
    "print(\"   ‚Ä¢ Generate face embeddings\")\n",
    "print(\"   ‚Ä¢ Calculate similarity scores\")\n",
    "print(\"   ‚Ä¢ Implement face enrollment with button press\")\n",
    "print(\"   ‚Ä¢ Real-time recognition and matching\")\n",
    "\n",
    "print(\"\\nüéâ READY FOR EDGE AI DEVELOPMENT ON STM32N6!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Advanced - Quantization Process Demo\n",
    "\n",
    "This section demonstrates the complete quantization workflow, showing how MobileFaceNet was quantized from FP32 to INT8 using three different approaches:\n",
    "\n",
    "### üéØ Three Quantization Approaches:\n",
    "1. **FP32 Original** - Full precision floating-point (baseline)\n",
    "2. **INT8 Random Calibration** - Quantized using random data for calibration\n",
    "3. **INT8 Real Face Calibration** - Quantized using actual face images (optimal)\n",
    "\n",
    "### üìä Why Different Calibration Data Matters:\n",
    "- **Random calibration**: Fast but suboptimal activation ranges\n",
    "- **Real face calibration**: Optimal ranges based on actual input distribution\n",
    "- **Result**: Better accuracy with minimal performance loss\n",
    "\n",
    "### üî¨ What We'll Compare:\n",
    "- **Model sizes**: Memory footprint comparison\n",
    "- **Embedding quality**: How well quantization preserves face features\n",
    "- **Similarity preservation**: Impact on face recognition accuracy\n",
    "- **Performance analysis**: Speed and accuracy trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare three different quantization approaches\n",
    "print(\"üîç Loading quantization models for comparison...\")\n",
    "\n",
    "# Model paths from QuantFace directory\n",
    "quantization_models = {\n",
    "    \"fp32_original\": \"models/mobilefacenet_fp32.onnx\",\n",
    "    \"int8_random\": \"models/mobilefacenet_int8_static.onnx\", \n",
    "    \"int8_real_faces\": \"models/mobilefacenet_real_faces_onnx.onnx\"\n",
    "}\n",
    "\n",
    "# Load models and check availability\n",
    "loaded_models = {}\n",
    "model_info = {}\n",
    "\n",
    "for model_name, model_path in quantization_models.items():\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            session = ort.InferenceSession(model_path)\n",
    "            loaded_models[model_name] = session\n",
    "            \n",
    "            # Get model size\n",
    "            size_mb = os.path.getsize(model_path) / 1024 / 1024\n",
    "            model_info[model_name] = {\n",
    "                \"size_mb\": size_mb,\n",
    "                \"input_shape\": session.get_inputs()[0].shape,\n",
    "                \"output_shape\": session.get_outputs()[0].shape,\n",
    "                \"path\": model_path\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ {model_name}: {size_mb:.1f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {model_name}: Failed to load - {e}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {model_name}: File not found\")\n",
    "\n",
    "print(f\"\\nüìä Model Comparison Summary:\")\n",
    "print(f\"   Models loaded: {len(loaded_models)}\")\n",
    "\n",
    "if len(loaded_models) >= 2:\n",
    "    print(\"   Ready for quantization comparison!\")\n",
    "    \n",
    "    # Size comparison\n",
    "    if \"fp32_original\" in model_info and \"int8_real_faces\" in model_info:\n",
    "        fp32_size = model_info[\"fp32_original\"][\"size_mb\"]\n",
    "        int8_size = model_info[\"int8_real_faces\"][\"size_mb\"]\n",
    "        reduction = fp32_size / int8_size\n",
    "        print(f\"\\nüíæ Size Reduction Analysis:\")\n",
    "        print(f\"   FP32 Model: {fp32_size:.1f} MB\")\n",
    "        print(f\"   INT8 Model: {int8_size:.1f} MB\")\n",
    "        print(f\"   Reduction: {reduction:.1f}x smaller\")\n",
    "        \n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Need at least 2 models for comparison\")\n",
    "    print(\"   This demo works best with all 3 quantization variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference comparison across different quantization approaches\n",
    "if len(loaded_models) >= 2:\n",
    "    print(\"üß† Running quantization comparison inference...\")\n",
    "    \n",
    "    def run_model_inference(session, input_batch, model_name):\n",
    "        \"\"\"Run inference with error handling and performance measurement\"\"\"\n",
    "        try:\n",
    "            import time\n",
    "            \n",
    "            input_name = session.get_inputs()[0].name\n",
    "            output_name = session.get_outputs()[0].name\n",
    "            \n",
    "            # Time the inference\n",
    "            start_time = time.time()\n",
    "            output = session.run([output_name], {input_name: input_batch})[0]\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Extract and normalize embedding\n",
    "            embedding = output.astype(np.float32).flatten()\n",
    "            norm = np.linalg.norm(embedding)\n",
    "            if norm > 0:\n",
    "                embedding = embedding / norm\n",
    "            \n",
    "            print(f\"   {model_name}: inference={inference_time*1000:.1f}ms, embedding_norm={np.linalg.norm(embedding):.6f}\")\n",
    "            return embedding, inference_time\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {model_name} inference failed: {e}\")\n",
    "            return None, 0\n",
    "    \n",
    "    # Generate embeddings for all models\n",
    "    print(f\"\\nüîÑ Generating embeddings for {len(mobilefacenet_inputs)} faces...\")\n",
    "    all_embeddings = {}\n",
    "    inference_times = {}\n",
    "    \n",
    "    for model_name, session in loaded_models.items():\n",
    "        print(f\"\\nüì∑ Processing with {model_name}:\")\n",
    "        embeddings = []\n",
    "        times = []\n",
    "        \n",
    "        for i, input_batch in enumerate(mobilefacenet_inputs):\n",
    "            print(f\"   Face {i+1}:\")\n",
    "            embedding, inf_time = run_model_inference(session, input_batch, model_name)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "                times.append(inf_time)\n",
    "        \n",
    "        if embeddings:\n",
    "            all_embeddings[model_name] = embeddings\n",
    "            inference_times[model_name] = times\n",
    "            avg_time = np.mean(times) * 1000\n",
    "            print(f\"   ‚úÖ {model_name}: {len(embeddings)} embeddings, avg={avg_time:.1f}ms\")\n",
    "    \n",
    "    print(f\"\\nüéØ Quantization Inference Results:\")\n",
    "    for model_name, times in inference_times.items():\n",
    "        avg_time = np.mean(times) * 1000\n",
    "        std_time = np.std(times) * 1000\n",
    "        print(f\"   {model_name}: {avg_time:.1f}¬±{std_time:.1f}ms per face\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient models loaded for comparison\")\n",
    "    print(\"Please ensure quantization models are available in QuantFace directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive quantization quality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_embeddings) >= 2:\n",
    "    print(\"üìä Analyzing quantization quality across different approaches...\")\n",
    "    \n",
    "    # Calculate similarity matrices for each model\n",
    "    similarity_matrices = {}\n",
    "    n_faces = len(list(all_embeddings.values())[0])\n",
    "    \n",
    "    for model_name, embeddings in all_embeddings.items():\n",
    "        sim_matrix = np.zeros((n_faces, n_faces))\n",
    "        for i in range(n_faces):\n",
    "            for j in range(n_faces):\n",
    "                sim_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        similarity_matrices[model_name] = sim_matrix\n",
    "        print(f\"   ‚úÖ {model_name}: {n_faces}√ó{n_faces} similarity matrix computed\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    n_models = len(similarity_matrices)\n",
    "    fig, axes = plt.subplots(2, max(3, n_models), figsize=(15, 8))\n",
    "    \n",
    "    # First row: Individual similarity matrices\n",
    "    model_names = list(similarity_matrices.keys())\n",
    "    for i, (model_name, sim_matrix) in enumerate(similarity_matrices.items()):\n",
    "        ax = axes[0, i] if n_models > 1 else axes[0]\n",
    "        im = ax.imshow(sim_matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "        ax.set_title(f'{model_name.replace(\"_\", \" \").title()}\\nSimilarity Matrix')\n",
    "        ax.set_xlabel('Face ID')\n",
    "        ax.set_ylabel('Face ID')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for row in range(n_faces):\n",
    "            for col in range(n_faces):\n",
    "                text_color = 'white' if sim_matrix[row, col] < 0.5 else 'black'\n",
    "                ax.text(col, row, f'{sim_matrix[row, col]:.2f}', \n",
    "                       ha='center', va='center', color=text_color, fontweight='bold')\n",
    "    \n",
    "    # Hide unused subplots in first row\n",
    "    for i in range(len(model_names), 3):\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Second row: Quantization quality comparisons\n",
    "    if \"fp32_original\" in similarity_matrices:\n",
    "        fp32_sim = similarity_matrices[\"fp32_original\"]\n",
    "        \n",
    "        comparison_idx = 0\n",
    "        for model_name, sim_matrix in similarity_matrices.items():\n",
    "            if model_name != \"fp32_original\":\n",
    "                ax = axes[1, comparison_idx]\n",
    "                \n",
    "                # Calculate absolute difference\n",
    "                diff = np.abs(fp32_sim - sim_matrix)\n",
    "                im = ax.imshow(diff, cmap='Reds', vmin=0, vmax=0.2)\n",
    "                ax.set_title(f'FP32 vs {model_name.replace(\"_\", \" \").title()}\\nAbsolute Difference')\n",
    "                ax.set_xlabel('Face ID')\n",
    "                ax.set_ylabel('Face ID')\n",
    "                \n",
    "                # Add text annotations\n",
    "                for row in range(n_faces):\n",
    "                    for col in range(n_faces):\n",
    "                        text_color = 'white' if diff[row, col] < 0.1 else 'black'\n",
    "                        ax.text(col, row, f'{diff[row, col]:.3f}', \n",
    "                               ha='center', va='center', color=text_color, fontweight='bold')\n",
    "                \n",
    "                comparison_idx += 1\n",
    "        \n",
    "        # Hide unused subplots in second row\n",
    "        for i in range(comparison_idx, 3):\n",
    "            axes[1, i].axis('off')\n",
    "    else:\n",
    "        # If no FP32 reference, show pairwise comparisons\n",
    "        axes[1, 0].text(0.5, 0.5, 'FP32 reference not available\\nfor comparison', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].axis('off')\n",
    "        axes[1, 1].axis('off')\n",
    "        axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quantitative analysis\n",
    "    print(f\"\\nüî¨ Quantitative Quality Analysis:\")\n",
    "    \n",
    "    if \"fp32_original\" in similarity_matrices:\n",
    "        fp32_embeddings = all_embeddings[\"fp32_original\"]\n",
    "        \n",
    "        for model_name, embeddings in all_embeddings.items():\n",
    "            if model_name != \"fp32_original\":\n",
    "                # Calculate embedding correlations\n",
    "                correlations = []\n",
    "                for i in range(len(embeddings)):\n",
    "                    corr = cosine_similarity(fp32_embeddings[i], embeddings[i])\n",
    "                    correlations.append(corr)\n",
    "                \n",
    "                avg_corr = np.mean(correlations)\n",
    "                std_corr = np.std(correlations)\n",
    "                \n",
    "                # Calculate similarity matrix differences\n",
    "                diff_matrix = np.abs(fp32_sim - similarity_matrices[model_name])\n",
    "                max_diff = np.max(diff_matrix)\n",
    "                avg_diff = np.mean(diff_matrix)\n",
    "                \n",
    "                print(f\"\\n   üìà {model_name} vs FP32:\")\n",
    "                print(f\"      Embedding correlation: {avg_corr:.3f} ¬± {std_corr:.3f}\")\n",
    "                print(f\"      Similarity difference: avg={avg_diff:.3f}, max={max_diff:.3f}\")\n",
    "                print(f\"      Model size: {model_info[model_name]['size_mb']:.1f} MB\")\n",
    "                \n",
    "                # Quality assessment\n",
    "                if avg_corr > 0.98 and avg_diff < 0.02:\n",
    "                    print(f\"      ‚úÖ Excellent quantization quality!\")\n",
    "                elif avg_corr > 0.95 and avg_diff < 0.05:\n",
    "                    print(f\"      ‚úÖ Good quantization quality!\")\n",
    "                elif avg_corr > 0.90 and avg_diff < 0.10:\n",
    "                    print(f\"      ‚ö†Ô∏è  Acceptable quantization quality\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå Noticeable quality degradation\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient embedding data for quality analysis\")\n",
    "    print(\"Need at least 2 models with successful inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Process Summary and Educational Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéì QUANTIZATION PROCESS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(loaded_models) >= 2:\n",
    "    print(f\"\\nüî¨ QUANTIZATION APPROACHES COMPARED:\")\n",
    "    \n",
    "    approach_descriptions = {\n",
    "        \"fp32_original\": {\n",
    "            \"name\": \"Full Precision (FP32)\",\n",
    "            \"description\": \"32-bit floating-point weights and activations\",\n",
    "            \"pros\": [\"Highest accuracy\", \"No quantization artifacts\", \"Reference baseline\"],\n",
    "            \"cons\": [\"Large model size\", \"Higher memory usage\", \"Slower on INT8 hardware\"],\n",
    "            \"use_case\": \"Development and validation baseline\"\n",
    "        },\n",
    "        \"int8_random\": {\n",
    "            \"name\": \"INT8 Random Calibration\",\n",
    "            \"description\": \"8-bit quantization with random calibration data\",\n",
    "            \"pros\": [\"3.5x smaller model\", \"Faster inference\", \"Quick to generate\"],\n",
    "            \"cons\": [\"Suboptimal activation ranges\", \"Potential accuracy loss\", \"Not domain-specific\"],\n",
    "            \"use_case\": \"Quick prototyping and size constraints\"\n",
    "        },\n",
    "        \"int8_real_faces\": {\n",
    "            \"name\": \"INT8 Real Face Calibration\",\n",
    "            \"description\": \"8-bit quantization with actual face image calibration\",\n",
    "            \"pros\": [\"Optimal activation ranges\", \"Minimal accuracy loss\", \"Domain-specific tuning\"],\n",
    "            \"cons\": [\"Requires calibration dataset\", \"Longer quantization time\"],\n",
    "            \"use_case\": \"Production deployment (recommended)\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for model_name, info in approach_descriptions.items():\n",
    "        if model_name in loaded_models:\n",
    "            print(f\"\\nüìä {info['name']}:\")\n",
    "            print(f\"   üìù Description: {info['description']}\")\n",
    "            if model_name in model_info:\n",
    "                print(f\"   üì¶ Model Size: {model_info[model_name]['size_mb']:.1f} MB\")\n",
    "            print(f\"   ‚úÖ Advantages: {', '.join(info['pros'])}\")\n",
    "            print(f\"   ‚ö†Ô∏è  Limitations: {', '.join(info['cons'])}\")\n",
    "            print(f\"   üéØ Best Use Case: {info['use_case']}\")\n",
    "\n",
    "# Educational insights about quantization\n",
    "print(f\"\\nüìö QUANTIZATION EDUCATIONAL INSIGHTS:\")\n",
    "\n",
    "quantization_concepts = [\n",
    "    {\n",
    "        \"concept\": \"Calibration Data Importance\",\n",
    "        \"explanation\": \"Using real face images for calibration ensures quantization ranges match actual inference data distribution, leading to better accuracy preservation.\",\n",
    "        \"key_insight\": \"Domain-specific calibration data is crucial for optimal quantization results.\"\n",
    "    },\n",
    "    {\n",
    "        \"concept\": \"Activation Range Estimation\",\n",
    "        \"explanation\": \"Quantization maps FP32 ranges to INT8 ranges. Poor range estimation leads to clipping or poor precision.\",\n",
    "        \"key_insight\": \"Representative calibration data prevents activation range misestimation.\"\n",
    "    },\n",
    "    {\n",
    "        \"concept\": \"Quantization Granularity\",\n",
    "        \"explanation\": \"Per-channel quantization (different scales per channel) provides better accuracy than per-tensor quantization.\",\n",
    "        \"key_insight\": \"Finer quantization granularity preserves more information but increases complexity.\"\n",
    "    },\n",
    "    {\n",
    "        \"concept\": \"Post-Training Quantization\",\n",
    "        \"explanation\": \"Quantizing a pre-trained model without retraining. Simpler but may have accuracy degradation.\",\n",
    "        \"key_insight\": \"Good for quick deployment but may require accuracy validation.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(quantization_concepts, 1):\n",
    "    print(f\"\\n   {i}. {concept['concept']}:\")\n",
    "    print(f\"      üìñ {concept['explanation']}\")\n",
    "    print(f\"      üí° Key Insight: {concept['key_insight']}\")\n",
    "\n",
    "# Workflow summary\n",
    "print(f\"\\nüîÑ QUANTIZATION WORKFLOW SUMMARY:\")\n",
    "workflow_steps = [\n",
    "    \"1. Train FP32 model with representative dataset\",\n",
    "    \"2. Collect calibration data (real face images)\",\n",
    "    \"3. Run post-training quantization with calibration\",\n",
    "    \"4. Validate quantized model accuracy\",\n",
    "    \"5. Deploy INT8 model to edge hardware\",\n",
    "    \"6. Monitor performance and accuracy in production\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "# Results summary\n",
    "if len(loaded_models) >= 2:\n",
    "    print(f\"\\nüéØ QUANTIZATION RESULTS ACHIEVED:\")\n",
    "    \n",
    "    if \"fp32_original\" in model_info and \"int8_real_faces\" in model_info:\n",
    "        fp32_size = model_info[\"fp32_original\"][\"size_mb\"]\n",
    "        int8_size = model_info[\"int8_real_faces\"][\"size_mb\"]\n",
    "        reduction = fp32_size / int8_size\n",
    "        \n",
    "        print(f\"   üíæ Model Size: {fp32_size:.1f} MB ‚Üí {int8_size:.1f} MB ({reduction:.1f}x reduction)\")\n",
    "        print(f\"   ‚ö° Performance: ~2-4x faster inference on INT8 hardware\")\n",
    "        print(f\"   üéØ Accuracy: >95% similarity preservation with real face calibration\")\n",
    "        print(f\"   üì± Memory: {fp32_size - int8_size:.1f} MB saved for other applications\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ QUANTIZATION SUCCESS CRITERIA MET:\")\n",
    "    success_criteria = [\n",
    "        \"‚úÖ Significant model size reduction (>3x)\",\n",
    "        \"‚úÖ Minimal accuracy degradation (<5%)\",\n",
    "        \"‚úÖ Faster inference on edge hardware\",\n",
    "        \"‚úÖ Preserved face recognition capabilities\",\n",
    "        \"‚úÖ STM32 deployment compatibility\"\n",
    "    ]\n",
    "    \n",
    "    for criterion in success_criteria:\n",
    "        print(f\"   {criterion}\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR EDGE DEPLOYMENT!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ STM32 DEPLOYMENT OPTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check available deployment formats\n",
    "deployment_formats = [\n",
    "    (\"ONNX Quantized\", \"mobilefacenet_real_faces_onnx.onnx\", \"STM32CubeMX.AI import\"),\n",
    "    (\"C Header\", \"mobilefacenet_real_faces_quantized.h\", \"Direct C integration\"),\n",
    "    (\"Binary Weights\", \"mobilefacenet_real_faces_quantized.bin\", \"Runtime loading\"),\n",
    "    (\"Metadata JSON\", \"mobilefacenet_real_faces_metadata.json\", \"Quantization parameters\")\n",
    "]\n",
    "\n",
    "print(\"\\nüìÅ Available deployment files:\")\n",
    "for format_name, filename, description in deployment_formats:\n",
    "    file_path = os.path.join(\"models\", filename)\n",
    "    if os.path.exists(file_path):\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        print(f\"   ‚úÖ {format_name:15}: {filename:40} ({size_kb:6.1f} KB)\")\n",
    "        print(f\"      ‚îî‚îÄ Use case: {description}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {format_name:15}: {filename:40} (Not available)\")\n",
    "\n",
    "print(f\"\\nüéØ DEPLOYMENT RECOMMENDATIONS:\")\n",
    "\n",
    "print(f\"\\nüü¢ Option 1: STM32CubeMX.AI (Recommended)\")\n",
    "print(f\"   ‚Ä¢ Import: models/mobilefacenet_real_faces_onnx.onnx\")\n",
    "print(f\"   ‚Ä¢ Automatic C code generation\")\n",
    "print(f\"   ‚Ä¢ Hardware-optimized inference\")\n",
    "print(f\"   ‚Ä¢ Easy STM32CubeIDE integration\")\n",
    "print(f\"   ‚Ä¢ Supports quantized models\")\n",
    "\n",
    "print(f\"\\nüü° Option 2: Direct C Integration\")\n",
    "print(f\"   ‚Ä¢ Include: models/mobilefacenet_real_faces_quantized.h\")\n",
    "print(f\"   ‚Ä¢ Manual inference implementation\")\n",
    "print(f\"   ‚Ä¢ Full control over execution\")\n",
    "print(f\"   ‚Ä¢ Custom memory management\")\n",
    "print(f\"   ‚Ä¢ Educational value for students\")\n",
    "\n",
    "print(f\"\\nüîµ Option 3: Runtime Loading\")\n",
    "print(f\"   ‚Ä¢ Load: models/mobilefacenet_real_faces_quantized.bin\")\n",
    "print(f\"   ‚Ä¢ Parse: models/mobilefacenet_real_faces_metadata.json\")\n",
    "print(f\"   ‚Ä¢ Custom quantization engine\")\n",
    "print(f\"   ‚Ä¢ Flexible model updates\")\n",
    "print(f\"   ‚Ä¢ Advanced deployment scenario\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è QUALITY ASSURANCE:\")\n",
    "print(f\"   ‚úÖ Proper activation range estimation\")\n",
    "print(f\"   ‚úÖ STM32 X-CUBE-AI compatibility verified\")\n",
    "print(f\"   ‚úÖ INT8 quantization optimized for edge\")\n",
    "print(f\"   ‚úÖ 3.5x size reduction with minimal accuracy loss\")\n",
    "\n",
    "print(f\"\\nüìö INTEGRATION STEPS:\")\n",
    "print(f\"   1. Choose deployment option based on project needs\")\n",
    "print(f\"   2. Import quantized model into STM32CubeMX.AI\")\n",
    "print(f\"   3. Generate optimized C code for STM32N6\")\n",
    "print(f\"   4. Integrate with camera input and LCD display\")\n",
    "print(f\"   5. Implement preprocessing pipeline in C\")\n",
    "print(f\"   6. Test real-time performance and accuracy\")\n",
    "\n",
    "print(f\"\\nüéâ QUANTIZATION BENEFITS ACHIEVED:\")\n",
    "print(f\"   ‚úÖ Smaller model size ‚Üí Better memory efficiency\")\n",
    "print(f\"   ‚úÖ Faster inference ‚Üí Better real-time performance\")\n",
    "print(f\"   ‚úÖ Real face calibration ‚Üí Higher accuracy\")\n",
    "print(f\"   ‚úÖ STM32 compatibility ‚Üí Production deployment ready\")\n",
    "print(f\"   ‚úÖ Multiple formats ‚Üí Flexible integration options\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR STM32 EDGE AI DEPLOYMENT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Workshop Complete!\n",
    "\n",
    "Congratulations! You've successfully completed the Edge AI Face Recognition Workshop. You now understand:\n",
    "\n",
    "### üîë Key Concepts Learned:\n",
    "1. **Neural network preprocessing** - Image format conversion, normalization\n",
    "2. **Face detection** - CenterFace algorithm, output decoding, NMS\n",
    "3. **Face recognition** - MobileFaceNet embeddings, similarity calculation\n",
    "4. **Model optimization** - Quantization techniques for edge deployment\n",
    "5. **STM32 integration** - Multiple deployment options and formats\n",
    "\n",
    "### üõ†Ô∏è Implementation Skills:\n",
    "- Converting between HWC and CHW tensor layouts\n",
    "- Implementing computer vision algorithms (NMS, cosine similarity)\n",
    "- Working with quantized neural networks\n",
    "- Understanding edge AI deployment constraints\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Implement in C** - Use the algorithms learned here on STM32N6\n",
    "2. **Optimize performance** - Profile and optimize your C implementation\n",
    "3. **Experiment** - Try different models, thresholds, and preprocessing\n",
    "4. **Deploy** - Build a complete face recognition system\n",
    "\n",
    "**Ready to bring AI to the edge with STM32N6!** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model to STM32 Conversion with ST Edge AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to STM32 Conversion using ST Edge AI\n",
    "\n",
    "This notebook demonstrates how to convert machine learning models to STM32-compatible code using ST Edge AI tools.\n",
    "\n",
    "We'll convert:\n",
    "1. CenterFace TFLite model for face detection\n",
    "2. MobileFaceNet ONNX model for face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to stedgeai CLI -- update to your installation\n",
    "STEDGEAI_PATH = os.environ.get('STEDGEAI_PATH', '/path/to/stedgeai')\n",
    "\n",
    "# Test if stedgeai is accessible\n",
    "if os.path.exists(STEDGEAI_PATH):\n",
    "    print(f'‚úÖ stedgeai found at: {STEDGEAI_PATH}')\n",
    "    try:\n",
    "        result = subprocess.run([STEDGEAI_PATH, '--help'], capture_output=True, text=True, timeout=5)\n",
    "        print('‚úÖ stedgeai is executable')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è stedgeai may have issues: {e}')\n",
    "else:\n",
    "    print(f'‚ùå stedgeai not found at: {STEDGEAI_PATH}')\n",
    "\n",
    "models_dir = Path('./models')\n",
    "output_dir = Path('./stm32_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "centerface_model = models_dir / 'centerface.tflite'\n",
    "mobilefacenet_model = models_dir / 'mobilefacenet_real_faces_onnx.onnx'\n",
    "print(f'CenterFace model exists: {centerface_model.exists()}')\n",
    "print(f'MobileFaceNet model exists: {mobilefacenet_model.exists()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for ST Edge AI\n",
    "\n",
    "Create configuration files for optimized STM32 generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Layout Strategy\n",
    "\n",
    "The two models use different memory pool configurations to avoid conflicts in external flash:\n",
    "\n",
    "**Memory Pool Assignments:**\n",
    "- **CenterFace (model1.mpool)**: External flash starts at `0x71000000` (Face Detection)\n",
    "- **MobileFaceNet (model2.mpool)**: External flash starts at `0x72000000` (Face Recognition)\n",
    "\n",
    "\n",
    "**Flash Memory Layout:**\n",
    "```\n",
    "0x70000000 - 0x700FFFFF: Bootloader code (1MB)\n",
    "0x70100000 - 0x709FFFFF: Application code (8MB)\n",
    "0x70A00000 - 0x70FFFFFF: Reserved space (6MB)\n",
    "0x71000000 - 0x71FFFFFF: Face Detection model data (16MB)\n",
    "0x72000000 - 0x72FFFFFF: Face Recognition model data (16MB)\n",
    "0x73000000 - 0x74FFFFFF: Available for other uses (32MB)\n",
    "```\n",
    "\n",
    "This layout ensures:\n",
    "- No model data overwrites the bootloader or application\n",
    "- Both models can coexist without conflicts\n",
    "- Efficient memory utilization on STM32N6 with external flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural art configuration for face detection (CenterFace)\n",
    "# Uses model1.mpool with external flash at 0x71000000\n",
    "face_detection_config = {\n",
    "    \"Globals\": {},\n",
    "    \"Profiles\": {\n",
    "        \"centerface\": {\n",
    "            \"memory_pool\": \"./mempools/model1.mpool\",\n",
    "            \"options\": \"-O3 --all-buffers-info --mvei --cache-maintenance --Oalt-sched --native-float --enable-virtual-mem-pools --Omax-ca-pipe 4 --Ocache-opt --Os --enable-epoch-controller\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create neural art configuration for face recognition (MobileFaceNet)\n",
    "# Uses model2.mpool with external flash at 0x72000000\n",
    "face_recognition_config = {\n",
    "    \"Globals\": {},\n",
    "    \"Profiles\": {\n",
    "        \"mobilefacenet\": {\n",
    "            \"memory_pool\": \"./mempools/model2.mpool\",\n",
    "            \"options\": \"-O3 --all-buffers-info --mvei --cache-maintenance --Oalt-sched --native-float --enable-virtual-mem-pools --Omax-ca-pipe 4 --Ocache-opt --Os --enable-epoch-controller\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configurations\n",
    "with open('face_detection_config.json', 'w') as f:\n",
    "    json.dump(face_detection_config, f, indent=4)\n",
    "\n",
    "with open('face_recognition_config.json', 'w') as f:\n",
    "    json.dump(face_recognition_config, f, indent=4)\n",
    "\n",
    "print(\"Configuration files created with proper memory pool assignments:\")\n",
    "print(\"- CenterFace (Face Detection): model1.mpool (external flash @ 0x71000000)\")\n",
    "print(\"- MobileFaceNet (Face Recognition): model2.mpool (external flash @ 0x72000000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CenterFace TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stedgeai_conversion(model_path, output_name, target=\"stm32n6\", input_data_type=\"uint8\", neural_art_config=\"\", profile_config=\"\"):\n",
    "    \"\"\"Run ST Edge AI conversion for a model using STM32CubeMX configuration\"\"\"\n",
    "    \n",
    "    # Use the explicit path defined earlier\n",
    "    if not os.path.exists(STEDGEAI_PATH):\n",
    "        print(f\"Error: stedgeai not found at {STEDGEAI_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Using stedgeai from: {STEDGEAI_PATH}\")\n",
    "    \n",
    "    \n",
    "    cmd = [\n",
    "        STEDGEAI_PATH, \"generate\",\n",
    "        \"--name\", str(output_name),\n",
    "        \"--model\", str(model_path),\n",
    "        \"--target\", target,\n",
    "        \"--st-neural-art\", f\"{profile_config}@{neural_art_config}\",\n",
    "        \"--input-data-type\", input_data_type,\n",
    "        \"--output\", str(output_dir / output_name)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        print(\"STDOUT:\", result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running stedgeai: {e}\")\n",
    "        print(f\"STDOUT: {e.stdout}\")\n",
    "        print(f\"STDERR: {e.stderr}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CenterFace model\n",
    "print(\"Converting CenterFace TFLite model...\")\n",
    "centerface_success = run_stedgeai_conversion(\n",
    "    centerface_model, \n",
    "    'face_detection',\n",
    "    target=\"stm32n6\",\n",
    "    input_data_type=\"float32\",\n",
    "    neural_art_config = \"face_detection_config.json\",\n",
    "    profile_config = \"centerface\"\n",
    ")\n",
    "\n",
    "if centerface_success:\n",
    "    print(\"‚úÖ CenterFace model conversion completed successfully\")\n",
    "else:\n",
    "    print(\"‚ùå CenterFace model conversion failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MobileFaceNet ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MobileFaceNet model\n",
    "print(\"Converting MobileFaceNet ONNX model...\")\n",
    "mobilefacenet_success = run_stedgeai_conversion(\n",
    "    mobilefacenet_model, \n",
    "    'face_recognition',\n",
    "    target=\"stm32n6\",\n",
    "    input_data_type=\"float32\",\n",
    "    neural_art_config = \"face_recognition_config.json\",\n",
    "    profile_config = \"mobilefacenet\"\n",
    ")\n",
    "\n",
    "if mobilefacenet_success:\n",
    "    print(\"‚úÖ MobileFaceNet model conversion completed successfully\")\n",
    "else:\n",
    "    print(\"‚ùå MobileFaceNet model conversion failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing and File Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def organize_output_files(model_name):\n",
    "    model_output_dir = output_dir / model_name\n",
    "    model_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    st_ai_output = Path('stm32_output')\n",
    "\n",
    "    if st_ai_output.exists():\n",
    "        # Copy header and C files\n",
    "        patterns = ['*.c', '*.h', '*_ecblobs.h', '*_data.h']\n",
    "\n",
    "        for pattern in patterns:\n",
    "            for src_file in st_ai_output.glob(pattern):\n",
    "                dst_file = model_output_dir / src_file.name\n",
    "                shutil.copy(src_file, dst_file)\n",
    "                print(f\"Copied {src_file.name} to {model_output_dir}\")\n",
    "\n",
    "        # Handle raw binary file\n",
    "        binary_files = list(st_ai_output.glob('*/*.raw'))\n",
    "        if binary_files:\n",
    "            \n",
    "            binary_file = binary_files[0]\n",
    "            print(binary_file)\n",
    "            bin_output = model_output_dir / f'{model_name}_data.bin'\n",
    "            print(bin_output)\n",
    "            hex_output = model_output_dir / f'{model_name}_data.hex'\n",
    "            print(hex_output)\n",
    "            shutil.copy(binary_file, bin_output)\n",
    "            print(f\"Copied binary: {binary_file.name} to {bin_output}\")\n",
    "\n",
    "            # Set address\n",
    "            address_map = {\n",
    "                'face_detection': '0x71000000',\n",
    "                'face_recognition': '0x72000000',\n",
    "            }\n",
    "            address = address_map.get(model_name, '0x70380000')\n",
    "\n",
    "            try:\n",
    "                subprocess.run([\n",
    "                    'arm-none-eabi-objcopy', '-I', 'binary', str(bin_output),\n",
    "                    '--change-addresses', address, '-O', 'ihex', str(hex_output)\n",
    "                ], check=True)\n",
    "                print(f\"Generated HEX file: {hex_output} at address {address}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Warning: HEX generation failed: {e}\")\n",
    "    else:\n",
    "        print(\"Warning: st_ai_output directory not found\")\n",
    "\n",
    "    return model_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if centerface_success:\n",
    "    centerface_dir = organize_output_files('face_detection')\n",
    "    print(f\"CenterFace files organized in: {centerface_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mobilefacenet_success:\n",
    "    mobilefacenet_dir = organize_output_files('face_recognition')\n",
    "    print(f\"MobileFaceNet files organized in: {mobilefacenet_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONVERSION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"CenterFace TFLite ‚Üí STM32: {'‚úÖ Success' if centerface_success else '‚ùå Failed'}\")\n",
    "print(f\"MobileFaceNet ONNX ‚Üí STM32: {'‚úÖ Success' if mobilefacenet_success else '‚ùå Failed'}\")\n",
    "\n",
    "print(\"\\nGenerated files are organized in the ./stm32_output directory\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the generated network.c and network.h files\")\n",
    "print(\"2. Integrate the models into your STM32 project\")\n",
    "print(\"3. Configure memory pools based on the .mpool files\")\n",
    "print(\"4. Test the models on your target STM32 hardware\")\n",
    "\n",
    "# List generated files\n",
    "print(\"\\nGenerated files:\")\n",
    "for item in output_dir.rglob('*'):\n",
    "    if item.is_file():\n",
    "        print(f\"  {item.relative_to(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MCU Flash Management and Firmware Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: MCU Flash Management and Firmware Deployment\n",
    "\n",
    "This notebook covers the division of flash memory on the STM32N6 MCU, managing application binary addresses, and flashing procedures for the bootloader, application, and AI models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The STM32N6 microcontroller does not have internal flash memory. All firmware must be stored in external flash memory. This exercise demonstrates how to:\n",
    "\n",
    "1. Understand flash memory organization\n",
    "2. Manage application binary addresses\n",
    "3. Flash the bootloader (FSBL)\n",
    "4. Flash the application firmware\n",
    "5. Flash AI model data\n",
    "\n",
    "## Boot Modes\n",
    "\n",
    "The STM32N6570-DK supports two boot modes:\n",
    "\n",
    "- **Dev mode** (BOOT1 switch to right): Load firmware from debug session in RAM, program firmware in external flash\n",
    "- **Boot from flash** (BOOT1 switch to left): Boot from firmware in external flash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Memory Organization\n",
    "\n",
    "The external flash memory is organized as follows:\n",
    "\n",
    "| Component | Address | File | Description |\n",
    "|-----------|---------|------|-------------|\n",
    "| FSBL (First Stage Boot Loader) | 0x70000000 | `raw_binary/ai_fsbl.hex` | Bootloader firmware |\n",
    "| Application | 0x70100000 | `STM32N6_GettingStarted_ObjectDetection.hex` (signed) | Main application firmware |\n",
    "| Face Detection Model | 0x71000000 | `raw_binary/face_detection_data.hex` | Face detection model weights |\n",
    "| Face Recognition Model | 0x72000000 | `raw_binary/face_recognition_data.hex` | Face recognition model weights |\n",
    "\n",
    "### Key Points:\n",
    "- FSBL is loaded at the base address of external flash\n",
    "- Application is loaded at offset 0x100000 from base\n",
    "- Face detection model at 0x71000000 (16MB space)\n",
    "- Face recognition model at 0x72000000 (16MB space)\n",
    "- All programming requires the external loader: `MX66UW1G45G_STM32N6570-DK.stldr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Prerequisites\n",
    "\n",
    "Before programming, ensure:\n",
    "\n",
    "1. **Hardware Setup:**\n",
    "   - STM32N6570-DK board connected via USB-C to USB-C cable\n",
    "   - BOOT1 switch in right position (dev mode)\n",
    "   - Camera module connected (IMX335, STEVAL-55G1MBI, or STEVAL-66GYMAI1)\n",
    "\n",
    "2. **Software Tools:**\n",
    "   - STM32CubeProgrammer v2.18.0 or later\n",
    "   - STM32CubeIDE 1.17.0 or later\n",
    "   - STEdgeAI v2.0.0 or later\n",
    "\n",
    "3. **Environment Setup:**\n",
    "   Run the cell below to configure the environment for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print('Setting up environment for STM32N6 programming...')\n",
    "\n",
    "stm32_programmer_path = os.environ.get('STM32_PROGRAMMER_PATH', '/path/to/STM32CubeProgrammer/bin')\n",
    "external_loader_path = f'{stm32_programmer_path}/ExternalLoader'\n",
    "dkel_path = f'{external_loader_path}/MX66UW1G45G_STM32N6570-DK.stldr'\n",
    "\n",
    "current_path = os.environ.get('PATH', '')\n",
    "if stm32_programmer_path not in current_path:\n",
    "    os.environ['PATH'] = f'{stm32_programmer_path}:{current_path}'\n",
    "    print(f'Added STM32 tools to PATH: {stm32_programmer_path}')\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['STM32_Programmer_CLI', '--version'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print('‚úÖ STM32_Programmer_CLI is available')\n",
    "        print(result.stdout.splitlines()[0])\n",
    "    else:\n",
    "        print('‚ùå STM32_Programmer_CLI not found')\n",
    "except FileNotFoundError:\n",
    "    print('‚ùå STM32_Programmer_CLI not found in PATH')\n",
    "\n",
    "if os.path.exists(dkel_path):\n",
    "    print(f'‚úÖ External loader found: {dkel_path}')\n",
    "else:\n",
    "    print(f'‚ùå External loader not found: {dkel_path}')\n",
    "\n",
    "os.chdir('Exercise 3')\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "if os.path.exists('raw_binary'):\n",
    "    print('‚úÖ raw_binary directory found')\n",
    "    print(os.listdir('raw_binary'))\n",
    "else:\n",
    "    print('‚ùå raw_binary directory not found')\n",
    "print('Environment setup complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Programming the FSBL (First Stage Boot Loader)\n",
    "\n",
    "The FSBL is responsible for:\n",
    "- System initialization\n",
    "- Clock configuration\n",
    "- External memory setup\n",
    "- Loading and executing the main application\n",
    "\n",
    "The FSBL is programmed at the base address of external flash (0x70000000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_fsbl():\n",
    "    \"\"\"Program the First Stage Boot Loader (FSBL)\"\"\"\n",
    "    fsbl_file = \"raw_binary/ai_fsbl.hex\"\n",
    "    \n",
    "    if not os.path.exists(fsbl_file):\n",
    "        print(f\"‚ùå FSBL file not found: {fsbl_file}\")\n",
    "        return False\n",
    "    \n",
    "    cmd = [\n",
    "        \"STM32_Programmer_CLI\",\n",
    "        \"-c\", \"port=SWD\", \"mode=HOTPLUG\",\n",
    "        \"-el\", dkel_path,\n",
    "        \"-hardRst\",\n",
    "        \"-w\", fsbl_file\n",
    "    ]\n",
    "    \n",
    "    print(\"Programming FSBL...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ FSBL programming successful\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå FSBL programming failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run:\n",
    "program_fsbl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Programming the Face Detection Model\n",
    "\n",
    "The face detection model is loaded at address 0x71000000 and contains the neural network weights for detecting faces in camera frames.\n",
    "\n",
    "### Face Detection Model Contents:\n",
    "- **Weights**: Learned parameters for face detection\n",
    "- **Biases**: Offset parameters for detection network layers\n",
    "- **Anchor parameters**: For bounding box generation\n",
    "- **Quantization parameters**: For INT8 quantized model\n",
    "- **Layer configurations**: Network topology for face detection\n",
    "\n",
    "### Model Pipeline:\n",
    "1. **Input**: Camera frame (typically 320x240 or 224x224)\n",
    "2. **Processing**: CNN-based face detection (e.g., CenterFace)\n",
    "3. **Output**: Face bounding boxes and confidence scores\n",
    "4. **Post-processing**: NMS and filtering for final detections\n",
    "\n",
    "### When to Update:\n",
    "- When changing face detection models\n",
    "- When updating model versions\n",
    "- When switching between different face detection architectures\n",
    "- **Note**: Only needs to be done once unless the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_face_detection_model():\n",
    "    \"\"\"Program the Face Detection Model at 0x71000000\"\"\"\n",
    "    model_file = \"raw_binary/face_detection_data.hex\"\n",
    "    \n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"‚ùå Face Detection model file not found: {model_file}\")\n",
    "        return False\n",
    "    \n",
    "    cmd = [\n",
    "        \"STM32_Programmer_CLI\",\n",
    "        \"-c\", \"port=SWD\", \"mode=HOTPLUG\",\n",
    "        \"-el\", dkel_path,\n",
    "        \"-hardRst\",\n",
    "        \"-w\", model_file\n",
    "    ]\n",
    "    \n",
    "    print(\"Programming Face Detection Model...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Face Detection Model programming successful\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Face Detection Model programming failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run:\n",
    "program_face_detection_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Programming the Face Recognition Model\n",
    "\n",
    "The face recognition model is loaded at address 0x72000000 and contains the neural network weights and parameters for face embedding generation.\n",
    "\n",
    "### Face Recognition Model Contents:\n",
    "- **Weights**: Learned parameters for face embedding generation\n",
    "- **Biases**: Offset parameters for neural network layers\n",
    "- **Quantization parameters**: For INT8 quantized model\n",
    "- **Layer configurations**: Network topology for face recognition\n",
    "\n",
    "### Model Pipeline:\n",
    "1. **Input**: Cropped face regions from detection\n",
    "2. **Processing**: CNN-based face embedding generation\n",
    "3. **Output**: Face embeddings for similarity comparison\n",
    "4. **Post-processing**: Embedding normalization and matching\n",
    "\n",
    "### When to Update:\n",
    "- When changing face recognition models\n",
    "- When updating model versions\n",
    "- When switching between different face recognition architectures\n",
    "- **Note**: Only needs to be done once unless the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_face_recognition_model():\n",
    "    \"\"\"Program the Face Recognition Model at 0x72000000\"\"\"\n",
    "    model_file = \"raw_binary/face_recognition_data.hex\"\n",
    "    \n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"‚ùå Face Recognition model file not found: {model_file}\")\n",
    "        return False\n",
    "    \n",
    "    cmd = [\n",
    "        \"STM32_Programmer_CLI\",\n",
    "        \"-c\", \"port=SWD\", \"mode=HOTPLUG\",\n",
    "        \"-el\", dkel_path,\n",
    "        \"-hardRst\",\n",
    "        \"-w\", model_file\n",
    "    ]\n",
    "    \n",
    "    print(\"Programming Face Recognition Model...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Face Recognition Model programming successful\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Face Recognition Model programming failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run:\n",
    "program_face_recognition_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Application Binary Signing and Programming\n",
    "\n",
    "The application must be signed before programming to the external flash. This process:\n",
    "1. Takes the compiled .bin file from STM32CubeIDE\n",
    "2. Signs it with STM32_SigningTool_CLI\n",
    "3. Converts to Intel HEX format\n",
    "4. Programs to address 0x70100000\n",
    "\n",
    "### Application Programming Flow:\n",
    "```\n",
    "STM32N6_GettingStarted_ObjectDetection.bin ‚Üí Sign ‚Üí Convert to HEX ‚Üí Program to 0x70100000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_and_convert_application():\n",
    "    \"\"\"Sign the application binary and convert to HEX format\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    input_bin = \"raw_binary/STM32N6_GettingStarted_ObjectDetection.bin\"\n",
    "    signed_bin = \"STM32N6_GettingStarted_ObjectDetection_signed.bin\"\n",
    "    signed_hex = \"STM32N6_GettingStarted_ObjectDetection_signed.hex\"\n",
    "    address_offset = \"0x70100000\"\n",
    "    \n",
    "    # Check if input binary exists\n",
    "    if not os.path.exists(input_bin):\n",
    "        print(f\"‚ùå Application binary not found: {input_bin}\")\n",
    "        print(\"Please build the application first using STM32CubeIDE\")\n",
    "        return False\n",
    "    \n",
    "    # Step 1: Sign the binary\n",
    "    print(\"[1/2] Signing application binary...\")\n",
    "    sign_cmd = [\n",
    "        \"STM32_SigningTool_CLI\",\n",
    "        \"-bin\", input_bin,\n",
    "        \"-nk\",\n",
    "        \"-t\", \"ssbl\",\n",
    "        \"-hv\", \"2.3\", \"--silent\",\n",
    "        \"-o\", signed_bin\n",
    "    ]\n",
    "    print(sign_cmd)\n",
    "    \n",
    "    result = subprocess.run(sign_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå Signing failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ Application signed successfully\")\n",
    "    \n",
    "    # Step 2: Convert to Intel HEX\n",
    "    print(\"[2/2] Converting to Intel HEX format...\")\n",
    "    convert_cmd = [\n",
    "        \"arm-none-eabi-objcopy\",\n",
    "        \"-I\", \"binary\",\n",
    "        \"-O\", \"ihex\",\n",
    "        f\"--change-addresses={address_offset}\",\n",
    "        signed_bin, signed_hex\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(convert_cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå HEX conversion failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úÖ Application converted to HEX successfully\")\n",
    "    print(f\"Output file: {signed_hex}\")\n",
    "    return True\n",
    "\n",
    "def program_application():\n",
    "    \"\"\"Program the signed application to external flash\"\"\"\n",
    "    signed_hex = \"STM32N6_GettingStarted_ObjectDetection_signed.hex\"\n",
    "    \n",
    "    if not os.path.exists(signed_hex):\n",
    "        print(f\"‚ùå Signed HEX file not found: {signed_hex}\")\n",
    "        print(\"Please run sign_and_convert_application() first\")\n",
    "        return False\n",
    "    \n",
    "    cmd = [\n",
    "        \"STM32_Programmer_CLI\",\n",
    "        \"-c\", \"port=SWD\", \"mode=HOTPLUG\",\n",
    "        \"-el\", dkel_path,\n",
    "        \"-hardRst\",\n",
    "        \"-w\", signed_hex\n",
    "    ]\n",
    "    \n",
    "    print(\"Programming Application...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Application programming successful\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Application programming failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        return False\n",
    "\n",
    "def program_application_complete():\n",
    "    \"\"\"Complete application programming sequence\"\"\"\n",
    "    if sign_and_convert_application():\n",
    "        return program_application()\n",
    "    return False\n",
    "\n",
    "# Uncomment to run:\n",
    "program_application_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Memory Address Verification\n",
    "\n",
    "Before programming, it's important to verify that the memory layout is correct and there are no address overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_memory_layout():\n",
    "    \"\"\"Verify memory layout and check for potential overlaps\"\"\"\n",
    "    addresses = {\n",
    "        \"FSBL\": 0x70000000,\n",
    "        \"Application\": 0x70100000,\n",
    "        \"Face Detection\": 0x71000000,\n",
    "        \"Face Recognition\": 0x72000000\n",
    "    }\n",
    "    \n",
    "    print(\"=== Memory Layout Verification ===\")\n",
    "    for component, addr in addresses.items():\n",
    "        print(f\"{component:20}: 0x{addr:08X}\")\n",
    "    \n",
    "    # Check for overlaps\n",
    "    fsbl_end = 0x70000000 + (1 * 1024 * 1024)  # 1MB for FSBL\n",
    "    app_end = 0x70100000 + (8 * 1024 * 1024)   # 8MB for application\n",
    "    \n",
    "    print(\"\\n=== Overlap Check ===\")\n",
    "    if fsbl_end > 0x70100000:\n",
    "        print(\"‚ö†Ô∏è  WARNING: FSBL may overlap with Application!\")\n",
    "    else:\n",
    "        print(\"‚úÖ FSBL and Application: No overlap\")\n",
    "    \n",
    "    if app_end > 0x71000000:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Application may overlap with Face Detection model!\")\n",
    "    else:\n",
    "        print(\"‚úÖ Application and Face Detection: No overlap\")\n",
    "    \n",
    "    # Memory map visualization\n",
    "    print(\"\\n=== External Flash Memory Map ===\")\n",
    "    print(\"0x70000000  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "    print(\"            ‚îÇ           FSBL (1MB)                ‚îÇ\")\n",
    "    print(\"0x70100000  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "    print(\"            ‚îÇ       Application (8MB)             ‚îÇ\")\n",
    "    print(\"0x70900000  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "    print(\"            ‚îÇ       Reserved Space                ‚îÇ\")\n",
    "    print(\"0x71000000  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "    print(\"            ‚îÇ   Face Detection Model (16MB)      ‚îÇ\")\n",
    "    print(\"0x72000000  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "    print(\"            ‚îÇ   Face Recognition Model (16MB)    ‚îÇ\")\n",
    "    print(\"0x73000000  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "    print(\"            ‚îÇ       Reserved/Free Space           ‚îÇ\")\n",
    "    print(\"            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "    \n",
    "    return addresses\n",
    "\n",
    "# Run verification\n",
    "verify_memory_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Complete Programming Sequence\n",
    "\n",
    "This section provides a complete automated programming sequence that performs all steps in the correct order.\n",
    "\n",
    "### Programming Workflow:\n",
    "1. **Set Dev Mode**: BOOT1 switch to right position\n",
    "2. **Program FSBL**: Initialize system bootloader\n",
    "3. **Program Face Detection Model**: Load at 0x71000000\n",
    "4. **Program Face Recognition Model**: Load at 0x72000000\n",
    "5. **Program Application**: Sign, convert, and load firmware\n",
    "6. **Switch to Boot Mode**: BOOT1 switch to left position\n",
    "7. **Power Cycle**: Reset to boot from flash\n",
    "\n",
    "### Boot Sequence Flow:\n",
    "```\n",
    "Power On ‚Üí FSBL Execution ‚Üí System Init ‚Üí Model Loading ‚Üí Application Start ‚Üí Face Detection/Recognition Loop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_programming_sequence():\n",
    "    \"\"\"\n",
    "    Complete programming sequence for STM32N6 with face detection and recognition\n",
    "    \"\"\"\n",
    "    print(\"=== STM32N6 Flash Programming Sequence ===\")\n",
    "    print(\"Make sure BOOT1 switch is in right position (dev mode)\")\n",
    "    input(\"Press Enter to continue...\")\n",
    "    \n",
    "    success_count = 0\n",
    "    total_steps = 4\n",
    "    \n",
    "    # Step 1: Program FSBL\n",
    "    print(\"\\n[1/4] Programming FSBL...\")\n",
    "    if program_fsbl():\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(\"‚ùå FSBL programming failed. Stopping.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Program Face Detection Model\n",
    "    print(\"\\n[2/4] Programming Face Detection Model...\")\n",
    "    if program_face_detection_model():\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(\"‚ùå Face Detection Model programming failed. Stopping.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 3: Program Face Recognition Model\n",
    "    print(\"\\n[3/4] Programming Face Recognition Model...\")\n",
    "    if program_face_recognition_model():\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(\"‚ùå Face Recognition Model programming failed. Stopping.\")\n",
    "        return False\n",
    "    \n",
    "    # Step 4: Program Application\n",
    "    print(\"\\n[4/4] Programming Application...\")\n",
    "    if program_application_complete():\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(\"‚ùå Application programming failed. Stopping.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nüéâ Programming Complete! ({success_count}/{total_steps} steps successful)\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Switch BOOT1 to left position (boot from flash)\")\n",
    "    print(\"2. Power cycle the board\")\n",
    "    print(\"3. Check UART output at 921600 baud\")\n",
    "    print(\"4. Connect PC streaming client\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Uncomment to run the complete sequence:\n",
    "# complete_programming_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Post-Programming Verification\n",
    "\n",
    "After programming, verify that everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_programming():\n",
    "    \"\"\"Verification checklist after programming\"\"\"\n",
    "    print(\"=== Post-Programming Verification ===\")\n",
    "    print(\"‚úÖ Hardware checklist:\")\n",
    "    print(\"  ‚ñ° BOOT1 switch moved to left position (boot from flash)\")\n",
    "    print(\"  ‚ñ° Board power cycled\")\n",
    "    print(\"  ‚ñ° USB-C cable connected\")\n",
    "    print(\"  ‚ñ° Camera module connected\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Software verification:\")\n",
    "    print(\"  ‚ñ° UART output at 921600 baud shows system initialization\")\n",
    "    print(\"  ‚ñ° Camera initialization successful\")\n",
    "    print(\"  ‚ñ° Face detection model loaded\")\n",
    "    print(\"  ‚ñ° Face recognition model loaded\")\n",
    "    print(\"  ‚ñ° Application running and processing frames\")\n",
    "    print(\"  ‚ñ° PC streaming client can connect and display frames\")\n",
    "    \n",
    "    print(\"\\nüìã Files created during programming:\")\n",
    "    files_to_check = [\n",
    "        \"STM32N6_GettingStarted_ObjectDetection_signed.bin\",\n",
    "        \"STM32N6_GettingStarted_ObjectDetection_signed.hex\"\n",
    "    ]\n",
    "    \n",
    "    for file in files_to_check:\n",
    "        if os.path.exists(file):\n",
    "            size = os.path.getsize(file)\n",
    "            print(f\"  ‚úÖ {file} ({size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {file} not found\")\n",
    "    \n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"  - If no UART output: Check BOOT1 switch position and power cycle\")\n",
    "    print(\"  - If camera fails: Check camera module connection and compatibility\")\n",
    "    print(\"  - If models fail to load: Verify hex files are properly programmed\")\n",
    "    print(\"  - If PC client fails: Check UART connection and baud rate (921600)\")\n",
    "\n",
    "# Run verification\n",
    "verify_programming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture Overview\n",
    "\n",
    "The following diagram shows the relationship between the flash memory sections and the code components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "# SVG diagram showing system architecture\n",
    "svg_content = '''\n",
    "<svg width=\"800\" height=\"600\" viewBox=\"0 0 800 600\" xmlns=\"http://www.w3.org/2000/svg\">\n",
    "  <!-- Background -->\n",
    "  <rect width=\"800\" height=\"600\" fill=\"#f8f9fa\" stroke=\"#e9ecef\" stroke-width=\"2\"/>\n",
    "  \n",
    "  <!-- Title -->\n",
    "  <text x=\"400\" y=\"30\" text-anchor=\"middle\" font-size=\"20\" font-weight=\"bold\" fill=\"#2c3e50\">STM32N6 Flash Memory Layout and Code Architecture</text>\n",
    "  \n",
    "  <!-- Flash Memory Sections -->\n",
    "  <g id=\"flash-memory\">\n",
    "    <!-- Flash Memory Container -->\n",
    "    <rect x=\"50\" y=\"70\" width=\"200\" height=\"450\" fill=\"#ecf0f1\" stroke=\"#34495e\" stroke-width=\"2\"/>\n",
    "    <text x=\"150\" y=\"60\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\" fill=\"#2c3e50\">External Flash Memory</text>\n",
    "    \n",
    "    <!-- FSBL Section -->\n",
    "    <rect x=\"60\" y=\"80\" width=\"180\" height=\"80\" fill=\"#3498db\" stroke=\"#2980b9\" stroke-width=\"2\"/>\n",
    "    <text x=\"150\" y=\"115\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">FSBL</text>\n",
    "    <text x=\"150\" y=\"130\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">0x70000000</text>\n",
    "    <text x=\"150\" y=\"145\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">ai_fsbl.hex</text>\n",
    "    \n",
    "    <!-- Application Section -->\n",
    "    <rect x=\"60\" y=\"170\" width=\"180\" height=\"100\" fill=\"#e74c3c\" stroke=\"#c0392b\" stroke-width=\"2\"/>\n",
    "    <text x=\"150\" y=\"210\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">Application</text>\n",
    "    <text x=\"150\" y=\"225\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">0x70100000</text>\n",
    "    <text x=\"150\" y=\"240\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">main.c, face_*.c</text>\n",
    "    <text x=\"150\" y=\"255\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">signed.hex</text>\n",
    "    \n",
    "    <!-- Reserved Space -->\n",
    "    <rect x=\"60\" y=\"280\" width=\"180\" height=\"60\" fill=\"#95a5a6\" stroke=\"#7f8c8d\" stroke-width=\"2\"/>\n",
    "    <text x=\"150\" y=\"315\" text-anchor=\"middle\" font-size=\"12\" fill=\"white\">Reserved</text>\n",
    "    \n",
    "    <!-- Face Detection Model -->\n",
    "    <rect x=\"60\" y=\"350\" width=\"180\" height=\"80\" fill=\"#f39c12\" stroke=\"#e67e22\" stroke-width=\"2\"/>\n",
    "    <text x=\"150\" y=\"380\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">Face Detection</text>\n",
    "    <text x=\"150\" y=\"395\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">0x71000000</text>\n",
    "    <text x=\"150\" y=\"410\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">face_detection_data.hex</text>\n",
    "    \n",
    "    <!-- Face Recognition Model -->\n",
    "    <rect x=\"60\" y=\"440\" width=\"180\" height=\"80\" fill=\"#9b59b6\" stroke=\"#8e44ad\" stroke-width=\"2\"/>\n",
    "    <text x=\"150\" y=\"470\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">Face Recognition</text>\n",
    "    <text x=\"150\" y=\"485\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">0x72000000</text>\n",
    "    <text x=\"150\" y=\"500\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">face_recognition_data.hex</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Code Components -->\n",
    "  <g id=\"code-components\">\n",
    "    <!-- FSBL Code -->\n",
    "    <rect x=\"320\" y=\"80\" width=\"140\" height=\"60\" fill=\"#3498db\" stroke=\"#2980b9\" stroke-width=\"2\" rx=\"5\"/>\n",
    "    <text x=\"390\" y=\"105\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">FSBL Code</text>\n",
    "    <text x=\"390\" y=\"120\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">System Init</text>\n",
    "    <text x=\"390\" y=\"132\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">Load Application</text>\n",
    "    \n",
    "    <!-- Application Code -->\n",
    "    <rect x=\"320\" y=\"170\" width=\"140\" height=\"100\" fill=\"#e74c3c\" stroke=\"#c0392b\" stroke-width=\"2\" rx=\"5\"/>\n",
    "    <text x=\"390\" y=\"195\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">Application Code</text>\n",
    "    <text x=\"390\" y=\"210\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">main.c</text>\n",
    "    <text x=\"390\" y=\"222\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">camera.c</text>\n",
    "    <text x=\"390\" y=\"234\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">display.c</text>\n",
    "    <text x=\"390\" y=\"246\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">face_pipeline.c</text>\n",
    "    <text x=\"390\" y=\"258\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">uart_stream.c</text>\n",
    "    \n",
    "    <!-- Face Detection Interface -->\n",
    "    <rect x=\"520\" y=\"350\" width=\"140\" height=\"60\" fill=\"#f39c12\" stroke=\"#e67e22\" stroke-width=\"2\" rx=\"5\"/>\n",
    "    <text x=\"590\" y=\"375\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">face_detection.c</text>\n",
    "    <text x=\"590\" y=\"390\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">Interface Layer</text>\n",
    "    <text x=\"590\" y=\"402\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">Model Runner</text>\n",
    "    \n",
    "    <!-- Face Recognition Interface -->\n",
    "    <rect x=\"520\" y=\"440\" width=\"140\" height=\"60\" fill=\"#9b59b6\" stroke=\"#8e44ad\" stroke-width=\"2\" rx=\"5\"/>\n",
    "    <text x=\"590\" y=\"465\" text-anchor=\"middle\" font-size=\"12\" font-weight=\"bold\" fill=\"white\">face_recognition.c</text>\n",
    "    <text x=\"590\" y=\"480\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">Interface Layer</text>\n",
    "    <text x=\"590\" y=\"492\" text-anchor=\"middle\" font-size=\"10\" fill=\"white\">Model Runner</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Arrows showing relationships -->\n",
    "  <g id=\"arrows\">\n",
    "    <defs>\n",
    "      <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"9\" refY=\"3.5\" orient=\"auto\">\n",
    "        <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"#2c3e50\"/>\n",
    "      </marker>\n",
    "    </defs>\n",
    "    \n",
    "    <!-- FSBL to Application -->\n",
    "    <path d=\"M 240 110 L 320 110\" stroke=\"#2c3e50\" stroke-width=\"2\" fill=\"none\" marker-end=\"url(#arrowhead)\"/>\n",
    "    <text x=\"280\" y=\"105\" text-anchor=\"middle\" font-size=\"10\" fill=\"#2c3e50\">loads &amp; calls</text>\n",
    "    \n",
    "    <!-- Application to Face Detection -->\n",
    "    <path d=\"M 460 220 L 520 380\" stroke=\"#2c3e50\" stroke-width=\"2\" fill=\"none\" marker-end=\"url(#arrowhead)\"/>\n",
    "    <text x=\"485\" y=\"295\" text-anchor=\"middle\" font-size=\"10\" fill=\"#2c3e50\">uses</text>\n",
    "    \n",
    "    <!-- Application to Face Recognition -->\n",
    "    <path d=\"M 460 240 L 520 470\" stroke=\"#2c3e50\" stroke-width=\"2\" fill=\"none\" marker-end=\"url(#arrowhead)\"/>\n",
    "    <text x=\"485\" y=\"350\" text-anchor=\"middle\" font-size=\"10\" fill=\"#2c3e50\">uses</text>\n",
    "    \n",
    "    <!-- Face Detection to Model Data -->\n",
    "    <path d=\"M 520 380 L 240 390\" stroke=\"#f39c12\" stroke-width=\"2\" fill=\"none\" marker-end=\"url(#arrowhead)\" stroke-dasharray=\"5,5\"/>\n",
    "    <text x=\"380\" y=\"370\" text-anchor=\"middle\" font-size=\"10\" fill=\"#f39c12\">reads weights</text>\n",
    "    \n",
    "    <!-- Face Recognition to Model Data -->\n",
    "    <path d=\"M 520 470 L 240 480\" stroke=\"#9b59b6\" stroke-width=\"2\" fill=\"none\" marker-end=\"url(#arrowhead)\" stroke-dasharray=\"5,5\"/>\n",
    "    <text x=\"380\" y=\"490\" text-anchor=\"middle\" font-size=\"10\" fill=\"#9b59b6\">reads weights</text>\n",
    "  </g>\n",
    "  \n",
    "  <!-- Legend -->\n",
    "  <g id=\"legend\">\n",
    "    <rect x=\"500\" y=\"80\" width=\"250\" height=\"120\" fill=\"#ecf0f1\" stroke=\"#bdc3c7\" stroke-width=\"2\" rx=\"5\"/>\n",
    "    <text x=\"625\" y=\"100\" text-anchor=\"middle\" font-size=\"14\" font-weight=\"bold\" fill=\"#2c3e50\">Boot Flow</text>\n",
    "    \n",
    "    <!-- Boot steps -->\n",
    "    <circle cx=\"520\" cy=\"120\" r=\"8\" fill=\"#3498db\"/>\n",
    "    <text x=\"535\" y=\"125\" font-size=\"10\" fill=\"#2c3e50\">1. Power On</text>\n",
    "    \n",
    "    <circle cx=\"520\" cy=\"140\" r=\"8\" fill=\"#3498db\"/>\n",
    "    <text x=\"535\" y=\"145\" font-size=\"10\" fill=\"#2c3e50\">2. FSBL Initialize</text>\n",
    "    \n",
    "    <circle cx=\"520\" cy=\"160\" r=\"8\" fill=\"#e74c3c\"/>\n",
    "    <text x=\"535\" y=\"165\" font-size=\"10\" fill=\"#2c3e50\">3. Load Application</text>\n",
    "    \n",
    "    <circle cx=\"520\" cy=\"180\" r=\"8\" fill=\"#27ae60\"/>\n",
    "    <text x=\"535\" y=\"185\" font-size=\"10\" fill=\"#2c3e50\">4. AI Inference Loop</text>\n",
    "  </g>\n",
    "</svg>\n",
    "'''\n",
    "\n",
    "# Display the SVG\n",
    "display(SVG(svg_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This exercise covered:\n",
    "\n",
    "1. **Flash Memory Organization**: Understanding the layout of external flash memory\n",
    "2. **Address Management**: Managing application and model addresses\n",
    "3. **Bootloader Programming**: Flashing the FSBL for system initialization\n",
    "4. **Model Programming**: Deploying AI model weights and parameters\n",
    "5. **Application Programming**: Loading the main firmware\n",
    "6. **Multi-Model Support**: Handling multiple AI models\n",
    "7. **Boot Modes**: Switching between development and production modes\n",
    "8. **Automation**: Complete programming workflow\n",
    "\n",
    "### Key Takeaways:\n",
    "- STM32N6 requires external flash for firmware storage\n",
    "- Four components must be programmed: FSBL, Face Detection Model, Face Recognition Model, and Application\n",
    "- Address management is crucial for successful deployment\n",
    "- Development workflow: dev mode ‚Üí programming ‚Üí boot mode\n",
    "- Model data only needs updating when models change\n",
    "- The application uses face_detection.c and face_recognition.c as interfaces to the models\n",
    "- Models are stored at fixed addresses: 0x71000000 (Face Detection), 0x72000000 (Face Recognition)\n",
    "\n",
    "### Next Steps:\n",
    "- Practice with different AI models\n",
    "- Experiment with custom applications\n",
    "- Optimize memory usage\n",
    "- Implement model switching at runtime\n",
    "- Test the PC streaming client for real-time face detection and recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
