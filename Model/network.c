/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    network.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "27f5d5bcb9ca9522b73a54d7aec841664ee448cd"
 * GIT_BRANCH      "STAI-2.1"
 * GIT_DESCRIPTION "atonn-v1.1.0-31-g27f5d5bcb"
 *
 * Command Line options:
 * --onnx-input = "C:/Users/pele/.stm32cubemx/network_output/face_detection_front_128_weight_quant_OE_3_2_0.onnx"
 * --out-dir-prefix = "C:/Users/pele/AppData/Local/Temp/mxAI_workspace63158467556540014747187993550021019/neural_art__network/"
 * --all-buffers-info = true
 * --mvei = true
 * --load-mdesc-file = "C:/ST/STEdgeAI/2.1/Utilities/configs/stm32n6"
 * --load-mpool-file = "C:/ST/STEdgeAI/2.1/scripts/N6_scripts/my_mpools/stm32n6__extFlash"
 * --cache-maintenance = true
 * --enable-virtual-mem-pools = true
 * --native-float = true
 * --json-quant-file = "C:/Users/pele/.stm32cubemx/network_output/face_detection_front_128_weight_quant_OE_3_2_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --enable-epoch-controller = true
 * --output-info-file = "c_info"
 * --Oauto-sched = true
 *
 * auto* option expanded into:
 *   alt-scheduler = false
 */

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"
#include "ecloader.h"

#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 0 || LL_ATON_VERSION_DEV != 31
#  warning "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 7 is ? */
/* index=7 file postfix=xSPI1 name=hyperRAM offset=0x90000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=HIGH byte width=2 freq ratio=5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=380 write_power=340 use4initializers=YES score=82  */
/* global pool 8 is 431.32 KB */
/* index=8 file postfix=xSPI2 name=octoFlash offset=0x71000000  absolute_mode size=117440504 READ_ONLY THROUGHPUT=MID LATENCY=HIGH byte width=1 freq ratio=6 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=110 write_power=400 use4initializers=YES score=50  */
/* global pool 1 is 432.00 KB */
/* index=1 file postfix=AXISRAM5 name=npuRAM5 offset=0x342e0000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 2 is 384.00 KB */
/* index=2 file postfix=AXISRAM4 name=npuRAM4 offset=0x34270000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 3 is 384.00 KB */
/* index=3 file postfix=AXISRAM3 name=npuRAM3 offset=0x34200000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 0 is ? */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458744 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */
/* global pool 11 is 2.30 MB */
/* index=11 file postfix=AXISRAM2_AXISRAM3_AXISRAM4_AXISRAM5_AXISRAM6 name=cpuRAM2_npuRAM3_npuRAM4_npuRAM5_npuRAM6 offset=0x34100000  absolute_mode size=2883576 vpool READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=16.201 use4initializers=NO score=85  */
/* global pool 4 is 1.00 MB */
/* index=4 file postfix=AXISRAM2 name=cpuRAM2 offset=0x34100000  absolute_mode size=1048576 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=17.324 write_power=15.321 use4initializers=NO score=84  */
/* global pool 5 is ? */
/* index=5 file postfix=AXISRAM1 name=cpuRAM1 offset=0x34064000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=16.616 write_power=14.522 use4initializers=NO score=84  */
/* global pool 6 is ? */
/* index=6 file postfix=AXIFLEXMEM name=flexMEM offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=9.381 write_power=8.569 use4initializers=NO score=84  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_Default(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_Default(uint32_t num)
{
  { 
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_Default(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_Default(uint32_t num)
{
  { 
    return NULL;
  }
}

#include "network_ecblobs.h"

/* scheduling epoch=0    nodes=97  ------------------------------------------------------------------- */

// Epoch Controller Blob (name='_ec_blob_1') micro instructions needed


/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_2(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_2 */
  Conv_sw_info conv1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 128,
    .general.input.dim.tensor_w = 128,
    .general.input.dim.tensor_c = 3,
    .general.input.dim.num_elem = 49152,
    .general.input.stride.b = 196608,
    .general.input.stride.h = 1536,
    .general.input.stride.w = 12,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 24,
    .weights.dim.tensor_h = 5,
    .weights.dim.tensor_w = 5,
    .weights.dim.tensor_c = 3,
    .weights.dim.num_elem = 1800,
    .weights.stride.b = 300,
    .weights.stride.h = 60,
    .weights.stride.w = 12,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 332160))) /* Equivalent hex address = 0x71051180UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 24,
    .bias.dim.num_elem = 24,
    .bias.stride.b = 96,
    .bias.stride.h = 96,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440560))) /* Equivalent hex address = 0x7106b8f0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 3, 3},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_2 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 393216);

}


/* scheduling epoch=3    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_3(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_3 */
  Activ_sw_info activ2_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_3 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ2_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 393216);

}


/* scheduling epoch=4    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_4(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_4 */
  Conv_sw_info conv3_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 24,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 432,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 427856))) /* Equivalent hex address = 0x71068750UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 12,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_4 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv3_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 393216);

}


/* scheduling epoch=5    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_5(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_4_addbias0 */
  Arith_sw_info arith4_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 24,
    .operand.dim.num_elem = 24,
    .operand.stride.b = 96,
    .operand.stride.h = 96,
    .operand.stride.w = 96,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440368))) /* Equivalent hex address = 0x7106b830UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_4_addbias0 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith4_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 393216);

}


/* scheduling epoch=6    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_6(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_5 */
  Conv_sw_info conv5_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 24,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 24,
    .weights.dim.num_elem = 576,
    .weights.stride.b = 96,
    .weights.stride.h = 96,
    .weights.stride.w = 96,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 418928))) /* Equivalent hex address = 0x71066470UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_5 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv5_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 393216);

}


/* scheduling epoch=7    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_7(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_5_addbias2 */
  Arith_sw_info arith6_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 24,
    .operand.dim.num_elem = 24,
    .operand.stride.b = 96,
    .operand.stride.h = 96,
    .operand.stride.w = 96,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440464))) /* Equivalent hex address = 0x7106b890UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_5_addbias2 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith6_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 393216);

}


/* scheduling epoch=8    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_8(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_6 */
  Arith_sw_info arith7_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 64,
    .operand.dim.tensor_w = 64,
    .operand.dim.tensor_c = 24,
    .operand.dim.num_elem = 98304,
    .operand.stride.b = 393216,
    .operand.stride.h = 6144,
    .operand.stride.w = 96,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_6 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith7_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 393216);

}


/* scheduling epoch=9    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_9(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_7 */
  Activ_sw_info activ8_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_7 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ8_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 393216);

}


/* scheduling epoch=10   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_10(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 393216))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 393216);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_8 */
  static const uint32_t Transpose_8_tensor_shape_in_10_shape_0[] = { 1, 64, 64, 24 };
  static const LL_LIB_TensorShape_TypeDef Transpose_8_tensor_shape_in_10[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_8_tensor_shape_in_10_shape_0,
      .batch = 24,
    }
  };

  static const uint32_t Transpose_8_tensor_axes_offsets_in_10_0[] = { 393216, 6144, 96, 4 };
  static const uint32_t* Transpose_8_tensor_axes_offsets_in_10[] = {
    Transpose_8_tensor_axes_offsets_in_10_0
  };

  static const uint32_t Transpose_8_tensor_shape_out_10_shape_0[] = { 1, 24, 64, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_8_tensor_shape_out_10[] = {
    {
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_8_tensor_shape_out_10_shape_0,
      .batch = 64,
    }
  };

  static const uint32_t Transpose_8_tensor_axes_offsets_out_10_0[] = { 393216, 16384, 256, 4 };
  static const uint32_t* Transpose_8_tensor_axes_offsets_out_10[] = {
    Transpose_8_tensor_axes_offsets_out_10_0
  };

  static const uint8_t Transpose_8_perm_to_use_array_in_10[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_8_target_pos_array_in_10[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_8_tensor_shape_in_10[0], Transpose_8_tensor_axes_offsets_in_10[0], &Transpose_8_tensor_shape_out_10[0], Transpose_8_tensor_axes_offsets_out_10[0], Transpose_8_target_pos_array_in_10, Transpose_8_perm_to_use_array_in_10, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 393216);

}


/* scheduling epoch=11   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_11(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_11 */
  Conv_sw_info conv9_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 24,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 432,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 429584))) /* Equivalent hex address = 0x71068e10UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 12,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_11 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv9_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 393216);

}


/* scheduling epoch=12   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_12(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 458752);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_9 */
  unsigned char* Pad_9_input_start_addr_12 = ((unsigned char *)((0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */;
  unsigned char* Pad_9_output_start_addr_12 = ((unsigned char *)((0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */;
  unsigned char* Pad_9_input_addr_limit_12 = ((unsigned char *)((0x34270000UL + 393280))) /* Equivalent hex address = 0x342d0040UL */;
  unsigned char* Pad_9_output_addr_limit_12 = ((unsigned char *)((0x34100000UL + 458816))) /* Equivalent hex address = 0x34170040UL */;

  static const uint32_t Pad_9_min_shape_12[] = { 1, 24, 64, 64 };

  static const int32_t Pad_9_pad_in_offsets_start_12[] = { 0, 0, 0, 0 };
  static const int32_t Pad_9_pad_in_offsets_end_12[] = { 0, 65536, 0, 0 };

  static const int32_t Pad_9_pad_out_offsets_start_12[] = { 0, 0, 0, 0 };
  static const int32_t Pad_9_pad_out_offsets_end_12[] = { 0, 65536, 0, 0 };

  static const int32_t Pad_9_out_shape_12[] = { 1, 28, 64, 64 };
  static const int32_t Pad_9_out_offsets_12[] = { 458752, 16384, 256, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_9_input_start_addr_12, Pad_9_output_start_addr_12, Pad_9_input_addr_limit_12, Pad_9_output_addr_limit_12, Pad_9_min_shape_12, 0, 4, 114688, *((int32_t*)&pad_const_value_native), 1, 98304, Pad_9_pad_in_offsets_start_12, Pad_9_pad_in_offsets_end_12, Pad_9_pad_out_offsets_start_12, Pad_9_pad_out_offsets_end_12, Pad_9_out_shape_12, Pad_9_out_offsets_12, 4, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 458752);

}


/* scheduling epoch=13   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_13(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_11_addbias4 */
  Arith_sw_info arith10_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 24,
    .operand.dim.num_elem = 24,
    .operand.stride.b = 96,
    .operand.stride.h = 96,
    .operand.stride.w = 96,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440656))) /* Equivalent hex address = 0x7106b950UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 24,
    .general.output.dim.num_elem = 98304,
    .general.output.stride.b = 393216,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 96,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_11_addbias4 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith10_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 393216);

}


/* scheduling epoch=14   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_14(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 917504))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) /* Equivalent hex address = 0x34170000UL */, 458752);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_10 */
  static const uint32_t Transpose_10_tensor_shape_in_14_shape_0[] = { 1, 28, 64, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_10_tensor_shape_in_14[] = {
    {
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 458752,
      .offset_limit = 458816,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_10_tensor_shape_in_14_shape_0,
      .batch = 64,
    }
  };

  static const uint32_t Transpose_10_tensor_axes_offsets_in_14_0[] = { 458752, 16384, 256, 4 };
  static const uint32_t* Transpose_10_tensor_axes_offsets_in_14[] = {
    Transpose_10_tensor_axes_offsets_in_14_0
  };

  static const uint32_t Transpose_10_tensor_shape_out_14_shape_0[] = { 1, 64, 64, 28 };
  static const LL_LIB_TensorShape_TypeDef Transpose_10_tensor_shape_out_14[] = {
    {
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 458752,
      .offset_end = 917504,
      .offset_limit = 917568,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_10_tensor_shape_out_14_shape_0,
      .batch = 28,
    }
  };

  static const uint32_t Transpose_10_tensor_axes_offsets_out_14_0[] = { 458752, 7168, 112, 4 };
  static const uint32_t* Transpose_10_tensor_axes_offsets_out_14[] = {
    Transpose_10_tensor_axes_offsets_out_14_0
  };

  static const uint8_t Transpose_10_perm_to_use_array_in_14[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_10_target_pos_array_in_14[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_10_tensor_shape_in_14[0], Transpose_10_tensor_axes_offsets_in_14[0], &Transpose_10_tensor_shape_out_14[0], Transpose_10_tensor_axes_offsets_out_14[0], Transpose_10_target_pos_array_in_14, Transpose_10_perm_to_use_array_in_14, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 917504))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) /* Equivalent hex address = 0x34170000UL */, 458752);

}


/* scheduling epoch=15   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_15(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_12 */
  Conv_sw_info conv11_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 24,
    .general.input.dim.num_elem = 98304,
    .general.input.stride.b = 393216,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 96,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 28,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 24,
    .weights.dim.num_elem = 672,
    .weights.stride.b = 96,
    .weights.stride.h = 96,
    .weights.stride.w = 96,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 413648))) /* Equivalent hex address = 0x71064fd0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 114688,
    .general.output.stride.b = 458752,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_12 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv11_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 458752);

}


/* scheduling epoch=16   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_16(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_12_addbias6 */
  Arith_sw_info arith12_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 114688,
    .general.input.stride.b = 458752,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 28,
    .operand.dim.num_elem = 28,
    .operand.stride.b = 112,
    .operand.stride.h = 112,
    .operand.stride.w = 112,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440144))) /* Equivalent hex address = 0x7106b750UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 114688,
    .general.output.stride.b = 458752,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 917504))) /* Equivalent hex address = 0x341e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_12_addbias6 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith12_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 917504))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1376256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 917504))) /* Equivalent hex address = 0x341e0000UL */, 458752);

}


/* scheduling epoch=17   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_17(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_13 */
  Arith_sw_info arith13_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 114688,
    .general.input.stride.b = 458752,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) /* Equivalent hex address = 0x34170000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 64,
    .operand.dim.tensor_w = 64,
    .operand.dim.tensor_c = 28,
    .operand.dim.num_elem = 114688,
    .operand.stride.b = 458752,
    .operand.stride.h = 7168,
    .operand.stride.w = 112,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 917504))) /* Equivalent hex address = 0x341e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 114688,
    .general.output.stride.b = 458752,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_13 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith13_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 458752);

}


/* scheduling epoch=18   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_18(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_14 */
  Activ_sw_info activ14_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 114688,
    .general.input.stride.b = 458752,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 64,
    .general.output.dim.tensor_w = 64,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 114688,
    .general.output.stride.b = 458752,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_14 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ14_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 458752))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 458752);

}


/* scheduling epoch=19   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_19(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_15 */
  Pool_sw_info pool15_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 114688,
    .general.input.stride.b = 458752,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 28672,
    .general.output.stride.b = 114688,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 1, 1},
    .strides = {2, 2},
    .k_shape = {2, 2},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_15 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool15_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 114688);

}


/* scheduling epoch=20   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_20(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_19 */
  Conv_sw_info conv16_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 64,
    .general.input.dim.tensor_w = 64,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 114688,
    .general.input.stride.b = 458752,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 28,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 504,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 425840))) /* Equivalent hex address = 0x71067f70UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 28672,
    .general.output.stride.b = 114688,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 14,
    .pads = {0, 0, 2, 2},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_19 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv16_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */, 114688);

}


/* scheduling epoch=21   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_21(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 344064))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */, 114688);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_16 */
  static const uint32_t Transpose_16_tensor_shape_in_21_shape_0[] = { 1, 32, 32, 28 };
  static const LL_LIB_TensorShape_TypeDef Transpose_16_tensor_shape_in_21[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 114688,
      .offset_limit = 114752,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_16_tensor_shape_in_21_shape_0,
      .batch = 28,
    }
  };

  static const uint32_t Transpose_16_tensor_axes_offsets_in_21_0[] = { 114688, 3584, 112, 4 };
  static const uint32_t* Transpose_16_tensor_axes_offsets_in_21[] = {
    Transpose_16_tensor_axes_offsets_in_21_0
  };

  static const uint32_t Transpose_16_tensor_shape_out_21_shape_0[] = { 1, 28, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_16_tensor_shape_out_21[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 229376,
      .offset_end = 344064,
      .offset_limit = 344128,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_16_tensor_shape_out_21_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_16_tensor_axes_offsets_out_21_0[] = { 114688, 4096, 128, 4 };
  static const uint32_t* Transpose_16_tensor_axes_offsets_out_21[] = {
    Transpose_16_tensor_axes_offsets_out_21_0
  };

  static const uint8_t Transpose_16_perm_to_use_array_in_21[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_16_target_pos_array_in_21[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_16_tensor_shape_in_21[0], Transpose_16_tensor_axes_offsets_in_21[0], &Transpose_16_tensor_shape_out_21[0], Transpose_16_tensor_axes_offsets_out_21[0], Transpose_16_target_pos_array_in_21, Transpose_16_perm_to_use_array_in_21, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 344064))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */, 114688);

}


/* scheduling epoch=22   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_22(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_19_addbias8 */
  Arith_sw_info arith17_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 28672,
    .general.input.stride.b = 114688,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 28,
    .operand.dim.num_elem = 28,
    .operand.stride.b = 112,
    .operand.stride.h = 112,
    .operand.stride.w = 112,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440256))) /* Equivalent hex address = 0x7106b7c0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 28,
    .general.output.dim.num_elem = 28672,
    .general.output.stride.b = 114688,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 112,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_19_addbias8 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith17_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 114688))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 114688);

}


/* scheduling epoch=23   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_23(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 131072);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_17 */
  unsigned char* Pad_17_input_start_addr_23 = ((unsigned char *)((0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */;
  unsigned char* Pad_17_output_start_addr_23 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_17_input_addr_limit_23 = ((unsigned char *)((0x342e0000UL + 344128))) /* Equivalent hex address = 0x34334040UL */;
  unsigned char* Pad_17_output_addr_limit_23 = ((unsigned char *)((0x342e0000UL + 131136))) /* Equivalent hex address = 0x34300040UL */;

  static const uint32_t Pad_17_min_shape_23[] = { 1, 28, 32, 32 };

  static const int32_t Pad_17_pad_in_offsets_start_23[] = { 0, 0, 0, 0 };
  static const int32_t Pad_17_pad_in_offsets_end_23[] = { 0, 16384, 0, 0 };

  static const int32_t Pad_17_pad_out_offsets_start_23[] = { 0, 0, 0, 0 };
  static const int32_t Pad_17_pad_out_offsets_end_23[] = { 0, 16384, 0, 0 };

  static const int32_t Pad_17_out_shape_23[] = { 1, 32, 32, 32 };
  static const int32_t Pad_17_out_offsets_23[] = { 131072, 4096, 128, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_17_input_start_addr_23, Pad_17_output_start_addr_23, Pad_17_input_addr_limit_23, Pad_17_output_addr_limit_23, Pad_17_min_shape_23, 0, 4, 32768, *((int32_t*)&pad_const_value_native), 1, 28672, Pad_17_pad_in_offsets_start_23, Pad_17_pad_in_offsets_end_23, Pad_17_pad_out_offsets_start_23, Pad_17_pad_out_offsets_end_23, Pad_17_out_shape_23, Pad_17_out_offsets_23, 4, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 131072);

}


/* scheduling epoch=24   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_24(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_20 */
  Conv_sw_info conv18_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 28,
    .general.input.dim.num_elem = 28672,
    .general.input.stride.b = 114688,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 112,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 28,
    .weights.dim.num_elem = 896,
    .weights.stride.b = 112,
    .weights.stride.h = 112,
    .weights.stride.w = 112,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 403584))) /* Equivalent hex address = 0x71062880UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 32768,
    .general.output.stride.b = 131072,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_20 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv18_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */, 131072);

}


/* scheduling epoch=25   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_25(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */, 131072);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_18 */
  static const uint32_t Transpose_18_tensor_shape_in_25_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_18_tensor_shape_in_25[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 131072,
      .offset_limit = 131136,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_18_tensor_shape_in_25_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_18_tensor_axes_offsets_in_25_0[] = { 131072, 4096, 128, 4 };
  static const uint32_t* Transpose_18_tensor_axes_offsets_in_25[] = {
    Transpose_18_tensor_axes_offsets_in_25_0
  };

  static const uint32_t Transpose_18_tensor_shape_out_25_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_18_tensor_shape_out_25[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 262144,
      .offset_end = 393216,
      .offset_limit = 393280,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_18_tensor_shape_out_25_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_18_tensor_axes_offsets_out_25_0[] = { 131072, 4096, 128, 4 };
  static const uint32_t* Transpose_18_tensor_axes_offsets_out_25[] = {
    Transpose_18_tensor_axes_offsets_out_25_0
  };

  static const uint8_t Transpose_18_perm_to_use_array_in_25[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_18_target_pos_array_in_25[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_18_tensor_shape_in_25[0], Transpose_18_tensor_axes_offsets_in_25[0], &Transpose_18_tensor_shape_out_25[0], Transpose_18_tensor_axes_offsets_out_25[0], Transpose_18_target_pos_array_in_25, Transpose_18_perm_to_use_array_in_25, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */, 131072);

}


/* scheduling epoch=26   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_26(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_20_addbias10 */
  Arith_sw_info arith19_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 32768,
    .general.input.stride.b = 131072,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 32,
    .operand.stride.b = 128,
    .operand.stride.h = 128,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 439760))) /* Equivalent hex address = 0x7106b5d0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 32768,
    .general.output.stride.b = 131072,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_20_addbias10 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith19_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 131072);

}


/* scheduling epoch=27   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_27(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_21 */
  Arith_sw_info arith20_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 32768,
    .general.input.stride.b = 131072,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 32,
    .operand.dim.tensor_w = 32,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 32768,
    .operand.stride.b = 131072,
    .operand.stride.h = 4096,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 32768,
    .general.output.stride.b = 131072,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_21 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith20_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */, 131072);

}


/* scheduling epoch=28   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_28(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_22 */
  Activ_sw_info activ21_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 32768,
    .general.input.stride.b = 131072,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 32768,
    .general.output.stride.b = 131072,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_22 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ21_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */, 131072);

}


/* scheduling epoch=29   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_29(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */, 131072);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_23 */
  static const uint32_t Transpose_23_tensor_shape_in_29_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_23_tensor_shape_in_29[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 262144,
      .offset_limit = 262208,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_23_tensor_shape_in_29_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_23_tensor_axes_offsets_in_29_0[] = { 131072, 4096, 128, 4 };
  static const uint32_t* Transpose_23_tensor_axes_offsets_in_29[] = {
    Transpose_23_tensor_axes_offsets_in_29_0
  };

  static const uint32_t Transpose_23_tensor_shape_out_29_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_23_tensor_shape_out_29[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 262144,
      .offset_end = 393216,
      .offset_limit = 393280,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_23_tensor_shape_out_29_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_23_tensor_axes_offsets_out_29_0[] = { 131072, 4096, 128, 4 };
  static const uint32_t* Transpose_23_tensor_axes_offsets_out_29[] = {
    Transpose_23_tensor_axes_offsets_out_29_0
  };

  static const uint8_t Transpose_23_perm_to_use_array_in_29[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_23_target_pos_array_in_29[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_23_tensor_shape_in_29[0], Transpose_23_tensor_axes_offsets_in_29[0], &Transpose_23_tensor_shape_out_29[0], Transpose_23_tensor_axes_offsets_out_29[0], Transpose_23_target_pos_array_in_29, Transpose_23_perm_to_use_array_in_29, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */, 131072);

}


/* scheduling epoch=30   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_30(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_26 */
  Conv_sw_info conv22_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 32768,
    .general.input.stride.b = 131072,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 576,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 421232))) /* Equivalent hex address = 0x71066d70UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 32768,
    .general.output.stride.b = 131072,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 16,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_26 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv22_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 131072))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 131072);

}


/* scheduling epoch=31   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_31(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 147456);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_24 */
  unsigned char* Pad_24_input_start_addr_31 = ((unsigned char *)((0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */;
  unsigned char* Pad_24_output_start_addr_31 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_24_input_addr_limit_31 = ((unsigned char *)((0x342e0000UL + 393280))) /* Equivalent hex address = 0x34340040UL */;
  unsigned char* Pad_24_output_addr_limit_31 = ((unsigned char *)((0x342e0000UL + 147520))) /* Equivalent hex address = 0x34304040UL */;

  static const uint32_t Pad_24_min_shape_31[] = { 1, 32, 32, 32 };

  static const int32_t Pad_24_pad_in_offsets_start_31[] = { 0, 0, 0, 0 };
  static const int32_t Pad_24_pad_in_offsets_end_31[] = { 0, 16384, 0, 0 };

  static const int32_t Pad_24_pad_out_offsets_start_31[] = { 0, 0, 0, 0 };
  static const int32_t Pad_24_pad_out_offsets_end_31[] = { 0, 16384, 0, 0 };

  static const int32_t Pad_24_out_shape_31[] = { 1, 36, 32, 32 };
  static const int32_t Pad_24_out_offsets_31[] = { 147456, 4096, 128, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_24_input_start_addr_31, Pad_24_output_start_addr_31, Pad_24_input_addr_limit_31, Pad_24_output_addr_limit_31, Pad_24_min_shape_31, 0, 4, 36864, *((int32_t*)&pad_const_value_native), 1, 32768, Pad_24_pad_in_offsets_start_31, Pad_24_pad_in_offsets_end_31, Pad_24_pad_out_offsets_start_31, Pad_24_pad_out_offsets_end_31, Pad_24_out_shape_31, Pad_24_out_offsets_31, 4, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 147456);

}


/* scheduling epoch=32   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_32(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_26_addbias12 */
  Arith_sw_info arith23_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 32768,
    .general.input.stride.b = 131072,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 32,
    .operand.stride.b = 128,
    .operand.stride.h = 128,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 439888))) /* Equivalent hex address = 0x7106b650UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 32768,
    .general.output.stride.b = 131072,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_26_addbias12 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith23_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 425984))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */, 131072);

}


/* scheduling epoch=33   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_33(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 147456);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_25 */
  static const uint32_t Transpose_25_tensor_shape_in_33_shape_0[] = { 1, 36, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_25_tensor_shape_in_33[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 147456,
      .offset_limit = 147520,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_25_tensor_shape_in_33_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_25_tensor_axes_offsets_in_33_0[] = { 147456, 4096, 128, 4 };
  static const uint32_t* Transpose_25_tensor_axes_offsets_in_33[] = {
    Transpose_25_tensor_axes_offsets_in_33_0
  };

  static const uint32_t Transpose_25_tensor_shape_out_33_shape_0[] = { 1, 32, 32, 36 };
  static const LL_LIB_TensorShape_TypeDef Transpose_25_tensor_shape_out_33[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 294912,
      .offset_limit = 294976,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_25_tensor_shape_out_33_shape_0,
      .batch = 36,
    }
  };

  static const uint32_t Transpose_25_tensor_axes_offsets_out_33_0[] = { 147456, 4608, 144, 4 };
  static const uint32_t* Transpose_25_tensor_axes_offsets_out_33[] = {
    Transpose_25_tensor_axes_offsets_out_33_0
  };

  static const uint8_t Transpose_25_perm_to_use_array_in_33[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_25_target_pos_array_in_33[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_25_tensor_shape_in_33[0], Transpose_25_tensor_axes_offsets_in_33[0], &Transpose_25_tensor_shape_out_33[0], Transpose_25_tensor_axes_offsets_out_33[0], Transpose_25_target_pos_array_in_33, Transpose_25_perm_to_use_array_in_33, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 147456);

}


/* scheduling epoch=34   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_34(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_27 */
  Conv_sw_info conv24_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 32768,
    .general.input.stride.b = 131072,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 36,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 1152,
    .weights.stride.b = 128,
    .weights.stride.h = 128,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 390336))) /* Equivalent hex address = 0x7105f4c0UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 36,
    .bias.dim.num_elem = 36,
    .bias.stride.b = 144,
    .bias.stride.h = 144,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 439472))) /* Equivalent hex address = 0x7106b4b0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 36,
    .general.output.dim.num_elem = 36864,
    .general.output.stride.b = 147456,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 144,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_27 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv24_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 147456);

}


/* scheduling epoch=35   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_35(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_28 */
  Arith_sw_info arith25_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 36,
    .general.input.dim.num_elem = 36864,
    .general.input.stride.b = 147456,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 144,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 32,
    .operand.dim.tensor_w = 32,
    .operand.dim.tensor_c = 36,
    .operand.dim.num_elem = 36864,
    .operand.stride.b = 147456,
    .operand.stride.h = 4608,
    .operand.stride.w = 144,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 36,
    .general.output.dim.num_elem = 36864,
    .general.output.stride.b = 147456,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 144,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_28 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith25_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 442368))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */, 147456);

}


/* scheduling epoch=36   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_36(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_29 */
  Activ_sw_info activ26_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 36,
    .general.input.dim.num_elem = 36864,
    .general.input.stride.b = 147456,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 144,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 36,
    .general.output.dim.num_elem = 36864,
    .general.output.stride.b = 147456,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 144,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_29 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ26_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 442368))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */, 147456);

}


/* scheduling epoch=37   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_37(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 147456);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_30 */
  static const uint32_t Transpose_30_tensor_shape_in_37_shape_0[] = { 1, 32, 32, 36 };
  static const LL_LIB_TensorShape_TypeDef Transpose_30_tensor_shape_in_37[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 294912,
      .offset_end = 442368,
      .offset_limit = 442432,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_30_tensor_shape_in_37_shape_0,
      .batch = 36,
    }
  };

  static const uint32_t Transpose_30_tensor_axes_offsets_in_37_0[] = { 147456, 4608, 144, 4 };
  static const uint32_t* Transpose_30_tensor_axes_offsets_in_37[] = {
    Transpose_30_tensor_axes_offsets_in_37_0
  };

  static const uint32_t Transpose_30_tensor_shape_out_37_shape_0[] = { 1, 36, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_30_tensor_shape_out_37[] = {
    {
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 147456,
      .offset_limit = 147520,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_30_tensor_shape_out_37_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_30_tensor_axes_offsets_out_37_0[] = { 147456, 4096, 128, 4 };
  static const uint32_t* Transpose_30_tensor_axes_offsets_out_37[] = {
    Transpose_30_tensor_axes_offsets_out_37_0
  };

  static const uint8_t Transpose_30_perm_to_use_array_in_37[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_30_target_pos_array_in_37[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_30_tensor_shape_in_37[0], Transpose_30_tensor_axes_offsets_in_37[0], &Transpose_30_tensor_shape_out_37[0], Transpose_30_tensor_axes_offsets_out_37[0], Transpose_30_target_pos_array_in_37, Transpose_30_perm_to_use_array_in_37, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 147456);

}


/* scheduling epoch=38   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_38(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_33 */
  Conv_sw_info conv27_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 36,
    .general.input.dim.num_elem = 36864,
    .general.input.stride.b = 147456,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 144,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 36,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 648,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 416336))) /* Equivalent hex address = 0x71065a50UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 36,
    .general.output.dim.num_elem = 36864,
    .general.output.stride.b = 147456,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 144,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) /* Equivalent hex address = 0x34294000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 18,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_33 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv27_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 294912))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) /* Equivalent hex address = 0x34294000UL */, 147456);

}


/* scheduling epoch=39   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_39(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 172032);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_31 */
  unsigned char* Pad_31_input_start_addr_39 = ((unsigned char *)((0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */;
  unsigned char* Pad_31_output_start_addr_39 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_31_input_addr_limit_39 = ((unsigned char *)((0x34270000UL + 147520))) /* Equivalent hex address = 0x34294040UL */;
  unsigned char* Pad_31_output_addr_limit_39 = ((unsigned char *)((0x342e0000UL + 172096))) /* Equivalent hex address = 0x3430a040UL */;

  static const uint32_t Pad_31_min_shape_39[] = { 1, 36, 32, 32 };

  static const int32_t Pad_31_pad_in_offsets_start_39[] = { 0, 0, 0, 0 };
  static const int32_t Pad_31_pad_in_offsets_end_39[] = { 0, 24576, 0, 0 };

  static const int32_t Pad_31_pad_out_offsets_start_39[] = { 0, 0, 0, 0 };
  static const int32_t Pad_31_pad_out_offsets_end_39[] = { 0, 24576, 0, 0 };

  static const int32_t Pad_31_out_shape_39[] = { 1, 42, 32, 32 };
  static const int32_t Pad_31_out_offsets_39[] = { 172032, 4096, 128, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_31_input_start_addr_39, Pad_31_output_start_addr_39, Pad_31_input_addr_limit_39, Pad_31_output_addr_limit_39, Pad_31_min_shape_39, 0, 4, 43008, *((int32_t*)&pad_const_value_native), 1, 36864, Pad_31_pad_in_offsets_start_39, Pad_31_pad_in_offsets_end_39, Pad_31_pad_out_offsets_start_39, Pad_31_pad_out_offsets_end_39, Pad_31_out_shape_39, Pad_31_out_offsets_39, 4, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 172032);

}


/* scheduling epoch=40   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_40(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_33_addbias14 */
  Arith_sw_info arith28_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 36,
    .general.input.dim.num_elem = 36864,
    .general.input.stride.b = 147456,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 144,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) /* Equivalent hex address = 0x34294000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 36,
    .operand.dim.num_elem = 36,
    .operand.stride.b = 144,
    .operand.stride.h = 144,
    .operand.stride.w = 144,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 439616))) /* Equivalent hex address = 0x7106b540UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 36,
    .general.output.dim.num_elem = 36864,
    .general.output.stride.b = 147456,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 144,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_33_addbias14 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith28_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 147456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 147456);

}


/* scheduling epoch=41   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_41(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 344064))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) /* Equivalent hex address = 0x3430a000UL */, 172032);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_32 */
  static const uint32_t Transpose_32_tensor_shape_in_41_shape_0[] = { 1, 42, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_32_tensor_shape_in_41[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 172032,
      .offset_limit = 172096,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_32_tensor_shape_in_41_shape_0,
      .batch = 32,
    }
  };

  static const uint32_t Transpose_32_tensor_axes_offsets_in_41_0[] = { 172032, 4096, 128, 4 };
  static const uint32_t* Transpose_32_tensor_axes_offsets_in_41[] = {
    Transpose_32_tensor_axes_offsets_in_41_0
  };

  static const uint32_t Transpose_32_tensor_shape_out_41_shape_0[] = { 1, 32, 32, 42 };
  static const LL_LIB_TensorShape_TypeDef Transpose_32_tensor_shape_out_41[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 172032,
      .offset_end = 344064,
      .offset_limit = 344128,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_32_tensor_shape_out_41_shape_0,
      .batch = 42,
    }
  };

  static const uint32_t Transpose_32_tensor_axes_offsets_out_41_0[] = { 172032, 5376, 168, 4 };
  static const uint32_t* Transpose_32_tensor_axes_offsets_out_41[] = {
    Transpose_32_tensor_axes_offsets_out_41_0
  };

  static const uint8_t Transpose_32_perm_to_use_array_in_41[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_32_target_pos_array_in_41[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_32_tensor_shape_in_41[0], Transpose_32_tensor_axes_offsets_in_41[0], &Transpose_32_tensor_shape_out_41[0], Transpose_32_tensor_axes_offsets_out_41[0], Transpose_32_target_pos_array_in_41, Transpose_32_perm_to_use_array_in_41, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 344064))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) /* Equivalent hex address = 0x3430a000UL */, 172032);

}


/* scheduling epoch=42   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_42(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_34 */
  Conv_sw_info conv29_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 36,
    .general.input.dim.num_elem = 36864,
    .general.input.stride.b = 147456,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 144,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 42,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 36,
    .weights.dim.num_elem = 1512,
    .weights.stride.b = 144,
    .weights.stride.h = 144,
    .weights.stride.w = 144,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 373344))) /* Equivalent hex address = 0x7105b260UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 42,
    .bias.dim.num_elem = 42,
    .bias.stride.b = 168,
    .bias.stride.h = 168,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 439120))) /* Equivalent hex address = 0x7106b350UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 42,
    .general.output.dim.num_elem = 43008,
    .general.output.stride.b = 172032,
    .general.output.stride.h = 5376,
    .general.output.stride.w = 168,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_34 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv29_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 172032);

}


/* scheduling epoch=43   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_43(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_35 */
  Arith_sw_info arith30_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 42,
    .general.input.dim.num_elem = 43008,
    .general.input.stride.b = 172032,
    .general.input.stride.h = 5376,
    .general.input.stride.w = 168,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) /* Equivalent hex address = 0x3430a000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 32,
    .operand.dim.tensor_w = 32,
    .operand.dim.tensor_c = 42,
    .operand.dim.num_elem = 43008,
    .operand.stride.b = 172032,
    .operand.stride.h = 5376,
    .operand.stride.w = 168,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 42,
    .general.output.dim.num_elem = 43008,
    .general.output.stride.b = 172032,
    .general.output.stride.h = 5376,
    .general.output.stride.w = 168,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_35 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith30_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 172032);

}


/* scheduling epoch=44   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_44(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_36 */
  Activ_sw_info activ31_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 42,
    .general.input.dim.num_elem = 43008,
    .general.input.stride.b = 172032,
    .general.input.stride.h = 5376,
    .general.input.stride.w = 168,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 32,
    .general.output.dim.tensor_w = 32,
    .general.output.dim.tensor_c = 42,
    .general.output.dim.num_elem = 43008,
    .general.output.stride.b = 172032,
    .general.output.stride.h = 5376,
    .general.output.stride.w = 168,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_36 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ31_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 172032);

}


/* scheduling epoch=45   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_45(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_37 */
  Pool_sw_info pool32_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 42,
    .general.input.dim.num_elem = 43008,
    .general.input.stride.b = 172032,
    .general.input.stride.h = 5376,
    .general.input.stride.w = 168,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 42,
    .general.output.dim.num_elem = 10752,
    .general.output.stride.b = 43008,
    .general.output.stride.h = 2688,
    .general.output.stride.w = 168,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 1, 1},
    .strides = {2, 2},
    .k_shape = {2, 2},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_37 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool32_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 43008);

}


/* scheduling epoch=46   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_46(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_41 */
  Conv_sw_info conv33_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 32,
    .general.input.dim.tensor_w = 32,
    .general.input.dim.tensor_c = 42,
    .general.input.dim.num_elem = 43008,
    .general.input.stride.b = 172032,
    .general.input.stride.h = 5376,
    .general.input.stride.w = 168,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 42,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 756,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 410624))) /* Equivalent hex address = 0x71064400UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 42,
    .general.output.dim.num_elem = 10752,
    .general.output.stride.b = 43008,
    .general.output.stride.h = 2688,
    .general.output.stride.w = 168,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) /* Equivalent hex address = 0x342ea800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 21,
    .pads = {0, 0, 2, 2},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_41 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv33_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) /* Equivalent hex address = 0x342ea800UL */, 43008);

}


/* scheduling epoch=47   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_47(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) /* Equivalent hex address = 0x342f5000UL */, 43008);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_38 */
  static const uint32_t Transpose_38_tensor_shape_in_47_shape_0[] = { 1, 16, 16, 42 };
  static const LL_LIB_TensorShape_TypeDef Transpose_38_tensor_shape_in_47[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 43008,
      .offset_limit = 43072,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_38_tensor_shape_in_47_shape_0,
      .batch = 42,
    }
  };

  static const uint32_t Transpose_38_tensor_axes_offsets_in_47_0[] = { 43008, 2688, 168, 4 };
  static const uint32_t* Transpose_38_tensor_axes_offsets_in_47[] = {
    Transpose_38_tensor_axes_offsets_in_47_0
  };

  static const uint32_t Transpose_38_tensor_shape_out_47_shape_0[] = { 1, 42, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_38_tensor_shape_out_47[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 86016,
      .offset_end = 129024,
      .offset_limit = 129088,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_38_tensor_shape_out_47_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_38_tensor_axes_offsets_out_47_0[] = { 43008, 1024, 64, 4 };
  static const uint32_t* Transpose_38_tensor_axes_offsets_out_47[] = {
    Transpose_38_tensor_axes_offsets_out_47_0
  };

  static const uint8_t Transpose_38_perm_to_use_array_in_47[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_38_target_pos_array_in_47[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_38_tensor_shape_in_47[0], Transpose_38_tensor_axes_offsets_in_47[0], &Transpose_38_tensor_shape_out_47[0], Transpose_38_tensor_axes_offsets_out_47[0], Transpose_38_target_pos_array_in_47, Transpose_38_perm_to_use_array_in_47, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) /* Equivalent hex address = 0x342f5000UL */, 43008);

}


/* scheduling epoch=48   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_48(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_41_addbias16 */
  Arith_sw_info arith34_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 42,
    .general.input.dim.num_elem = 10752,
    .general.input.stride.b = 43008,
    .general.input.stride.h = 2688,
    .general.input.stride.w = 168,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) /* Equivalent hex address = 0x342ea800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 42,
    .operand.dim.num_elem = 42,
    .operand.stride.b = 168,
    .operand.stride.h = 168,
    .operand.stride.w = 168,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 439296))) /* Equivalent hex address = 0x7106b400UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 42,
    .general.output.dim.num_elem = 10752,
    .general.output.stride.b = 43008,
    .general.output.stride.h = 2688,
    .general.output.stride.w = 168,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) /* Equivalent hex address = 0x342ff800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_41_addbias16 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith34_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) /* Equivalent hex address = 0x342ff800UL */, 43008);

}


/* scheduling epoch=49   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_49(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 49152);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_39 */
  unsigned char* Pad_39_input_start_addr_49 = ((unsigned char *)((0x342e0000UL + 86016))) /* Equivalent hex address = 0x342f5000UL */;
  unsigned char* Pad_39_output_start_addr_49 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_39_input_addr_limit_49 = ((unsigned char *)((0x342e0000UL + 129088))) /* Equivalent hex address = 0x342ff840UL */;
  unsigned char* Pad_39_output_addr_limit_49 = ((unsigned char *)((0x342e0000UL + 49216))) /* Equivalent hex address = 0x342ec040UL */;

  static const uint32_t Pad_39_min_shape_49[] = { 1, 42, 16, 16 };

  static const int32_t Pad_39_pad_in_offsets_start_49[] = { 0, 0, 0, 0 };
  static const int32_t Pad_39_pad_in_offsets_end_49[] = { 0, 6144, 0, 0 };

  static const int32_t Pad_39_pad_out_offsets_start_49[] = { 0, 0, 0, 0 };
  static const int32_t Pad_39_pad_out_offsets_end_49[] = { 0, 6144, 0, 0 };

  static const int32_t Pad_39_out_shape_49[] = { 1, 48, 16, 16 };
  static const int32_t Pad_39_out_offsets_49[] = { 49152, 1024, 64, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_39_input_start_addr_49, Pad_39_output_start_addr_49, Pad_39_input_addr_limit_49, Pad_39_output_addr_limit_49, Pad_39_min_shape_49, 0, 4, 12288, *((int32_t*)&pad_const_value_native), 1, 10752, Pad_39_pad_in_offsets_start_49, Pad_39_pad_in_offsets_end_49, Pad_39_pad_out_offsets_start_49, Pad_39_pad_out_offsets_end_49, Pad_39_out_shape_49, Pad_39_out_offsets_49, 4, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 49152);

}


/* scheduling epoch=50   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_50(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_42 */
  Conv_sw_info conv35_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 42,
    .general.input.dim.num_elem = 10752,
    .general.input.stride.b = 43008,
    .general.input.stride.h = 2688,
    .general.input.stride.w = 168,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) /* Equivalent hex address = 0x342ff800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 48,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 42,
    .weights.dim.num_elem = 2016,
    .weights.stride.b = 168,
    .weights.stride.h = 168,
    .weights.stride.w = 168,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 324096))) /* Equivalent hex address = 0x7104f200UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 48,
    .bias.dim.num_elem = 48,
    .bias.stride.b = 192,
    .bias.stride.h = 192,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 438736))) /* Equivalent hex address = 0x7106b1d0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 48,
    .general.output.dim.num_elem = 12288,
    .general.output.stride.b = 49152,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 192,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) /* Equivalent hex address = 0x342ec000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_42 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv35_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) /* Equivalent hex address = 0x342ec000UL */, 49152);

}


/* scheduling epoch=51   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_51(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) /* Equivalent hex address = 0x342f8000UL */, 49152);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_40 */
  static const uint32_t Transpose_40_tensor_shape_in_51_shape_0[] = { 1, 48, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_40_tensor_shape_in_51[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 49152,
      .offset_limit = 49216,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_40_tensor_shape_in_51_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_40_tensor_axes_offsets_in_51_0[] = { 49152, 1024, 64, 4 };
  static const uint32_t* Transpose_40_tensor_axes_offsets_in_51[] = {
    Transpose_40_tensor_axes_offsets_in_51_0
  };

  static const uint32_t Transpose_40_tensor_shape_out_51_shape_0[] = { 1, 16, 16, 48 };
  static const LL_LIB_TensorShape_TypeDef Transpose_40_tensor_shape_out_51[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 98304,
      .offset_end = 147456,
      .offset_limit = 147520,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_40_tensor_shape_out_51_shape_0,
      .batch = 48,
    }
  };

  static const uint32_t Transpose_40_tensor_axes_offsets_out_51_0[] = { 49152, 3072, 192, 4 };
  static const uint32_t* Transpose_40_tensor_axes_offsets_out_51[] = {
    Transpose_40_tensor_axes_offsets_out_51_0
  };

  static const uint8_t Transpose_40_perm_to_use_array_in_51[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_40_target_pos_array_in_51[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_40_tensor_shape_in_51[0], Transpose_40_tensor_axes_offsets_in_51[0], &Transpose_40_tensor_shape_out_51[0], Transpose_40_tensor_axes_offsets_out_51[0], Transpose_40_target_pos_array_in_51, Transpose_40_perm_to_use_array_in_51, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) /* Equivalent hex address = 0x342f8000UL */, 49152);

}


/* scheduling epoch=52   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_52(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_43 */
  Arith_sw_info arith36_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 48,
    .general.input.dim.num_elem = 12288,
    .general.input.stride.b = 49152,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 192,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) /* Equivalent hex address = 0x342f8000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 16,
    .operand.dim.tensor_w = 16,
    .operand.dim.tensor_c = 48,
    .operand.dim.num_elem = 12288,
    .operand.stride.b = 49152,
    .operand.stride.h = 3072,
    .operand.stride.w = 192,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) /* Equivalent hex address = 0x342ec000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 48,
    .general.output.dim.num_elem = 12288,
    .general.output.stride.b = 49152,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 192,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_43 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith36_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 49152);

}


/* scheduling epoch=53   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_53(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_44 */
  Activ_sw_info activ37_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 48,
    .general.input.dim.num_elem = 12288,
    .general.input.stride.b = 49152,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 192,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 48,
    .general.output.dim.num_elem = 12288,
    .general.output.stride.b = 49152,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 192,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_44 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ37_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 49152);

}


/* scheduling epoch=54   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_54(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 106496))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 49152);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_45 */
  static const uint32_t Transpose_45_tensor_shape_in_54_shape_0[] = { 1, 16, 16, 48 };
  static const LL_LIB_TensorShape_TypeDef Transpose_45_tensor_shape_in_54[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 49152,
      .offset_limit = 49216,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_45_tensor_shape_in_54_shape_0,
      .batch = 48,
    }
  };

  static const uint32_t Transpose_45_tensor_axes_offsets_in_54_0[] = { 49152, 3072, 192, 4 };
  static const uint32_t* Transpose_45_tensor_axes_offsets_in_54[] = {
    Transpose_45_tensor_axes_offsets_in_54_0
  };

  static const uint32_t Transpose_45_tensor_shape_out_54_shape_0[] = { 1, 48, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_45_tensor_shape_out_54[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 106496,
      .offset_limit = 106560,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_45_tensor_shape_out_54_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_45_tensor_axes_offsets_out_54_0[] = { 49152, 1024, 64, 4 };
  static const uint32_t* Transpose_45_tensor_axes_offsets_out_54[] = {
    Transpose_45_tensor_axes_offsets_out_54_0
  };

  static const uint8_t Transpose_45_perm_to_use_array_in_54[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_45_target_pos_array_in_54[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_45_tensor_shape_in_54[0], Transpose_45_tensor_axes_offsets_in_54[0], &Transpose_45_tensor_shape_out_54[0], Transpose_45_tensor_axes_offsets_out_54[0], Transpose_45_target_pos_array_in_54, Transpose_45_perm_to_use_array_in_54, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 106496))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 49152);

}


/* scheduling epoch=55   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_55(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_48 */
  Conv_sw_info conv38_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 48,
    .general.input.dim.num_elem = 12288,
    .general.input.stride.b = 49152,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 192,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 48,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 864,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 407168))) /* Equivalent hex address = 0x71063680UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 48,
    .general.output.dim.num_elem = 12288,
    .general.output.stride.b = 49152,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 192,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 106496))) /* Equivalent hex address = 0x342fa000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 24,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_48 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv38_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 106496))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 155648))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 106496))) /* Equivalent hex address = 0x342fa000UL */, 49152);

}


/* scheduling epoch=56   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_56(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 57344);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_46 */
  unsigned char* Pad_46_input_start_addr_56 = ((unsigned char *)((0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */;
  unsigned char* Pad_46_output_start_addr_56 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_46_input_addr_limit_56 = ((unsigned char *)((0x342e0000UL + 106560))) /* Equivalent hex address = 0x342fa040UL */;
  unsigned char* Pad_46_output_addr_limit_56 = ((unsigned char *)((0x342e0000UL + 57408))) /* Equivalent hex address = 0x342ee040UL */;

  static const uint32_t Pad_46_min_shape_56[] = { 1, 48, 16, 16 };

  static const int32_t Pad_46_pad_in_offsets_start_56[] = { 0, 0, 0, 0 };
  static const int32_t Pad_46_pad_in_offsets_end_56[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_46_pad_out_offsets_start_56[] = { 0, 0, 0, 0 };
  static const int32_t Pad_46_pad_out_offsets_end_56[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_46_out_shape_56[] = { 1, 56, 16, 16 };
  static const int32_t Pad_46_out_offsets_56[] = { 57344, 1024, 64, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_46_input_start_addr_56, Pad_46_output_start_addr_56, Pad_46_input_addr_limit_56, Pad_46_output_addr_limit_56, Pad_46_min_shape_56, 0, 4, 14336, *((int32_t*)&pad_const_value_native), 1, 12288, Pad_46_pad_in_offsets_start_56, Pad_46_pad_in_offsets_end_56, Pad_46_pad_out_offsets_start_56, Pad_46_pad_out_offsets_end_56, Pad_46_out_shape_56, Pad_46_out_offsets_56, 4, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 57344);

}


/* scheduling epoch=57   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_57(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_48_addbias18 */
  Arith_sw_info arith39_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 48,
    .general.input.dim.num_elem = 12288,
    .general.input.stride.b = 49152,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 192,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 106496))) /* Equivalent hex address = 0x342fa000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 48,
    .operand.dim.num_elem = 48,
    .operand.stride.b = 192,
    .operand.stride.h = 192,
    .operand.stride.w = 192,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 438928))) /* Equivalent hex address = 0x7106b290UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 48,
    .general.output.dim.num_elem = 12288,
    .general.output.stride.b = 49152,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 192,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 155648))) /* Equivalent hex address = 0x34306000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_48_addbias18 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith39_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 155648))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 155648))) /* Equivalent hex address = 0x34306000UL */, 49152);

}


/* scheduling epoch=58   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_58(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 57344);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_47 */
  static const uint32_t Transpose_47_tensor_shape_in_58_shape_0[] = { 1, 56, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_47_tensor_shape_in_58[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_47_tensor_shape_in_58_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_47_tensor_axes_offsets_in_58_0[] = { 57344, 1024, 64, 4 };
  static const uint32_t* Transpose_47_tensor_axes_offsets_in_58[] = {
    Transpose_47_tensor_axes_offsets_in_58_0
  };

  static const uint32_t Transpose_47_tensor_shape_out_58_shape_0[] = { 1, 16, 16, 56 };
  static const LL_LIB_TensorShape_TypeDef Transpose_47_tensor_shape_out_58[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_47_tensor_shape_out_58_shape_0,
      .batch = 56,
    }
  };

  static const uint32_t Transpose_47_tensor_axes_offsets_out_58_0[] = { 57344, 3584, 224, 4 };
  static const uint32_t* Transpose_47_tensor_axes_offsets_out_58[] = {
    Transpose_47_tensor_axes_offsets_out_58_0
  };

  static const uint8_t Transpose_47_perm_to_use_array_in_58[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_47_target_pos_array_in_58[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_47_tensor_shape_in_58[0], Transpose_47_tensor_axes_offsets_in_58[0], &Transpose_47_tensor_shape_out_58[0], Transpose_47_tensor_axes_offsets_out_58[0], Transpose_47_target_pos_array_in_58, Transpose_47_perm_to_use_array_in_58, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 57344);

}


/* scheduling epoch=59   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_59(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_49 */
  Conv_sw_info conv40_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 48,
    .general.input.dim.num_elem = 12288,
    .general.input.stride.b = 49152,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 192,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 155648))) /* Equivalent hex address = 0x34306000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 56,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 48,
    .weights.dim.num_elem = 2688,
    .weights.stride.b = 192,
    .weights.stride.h = 192,
    .weights.stride.w = 192,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 313344))) /* Equivalent hex address = 0x7104c800UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 56,
    .bias.dim.num_elem = 56,
    .bias.stride.b = 224,
    .bias.stride.h = 224,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 438288))) /* Equivalent hex address = 0x7106b010UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 56,
    .general.output.dim.num_elem = 14336,
    .general.output.stride.b = 57344,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 224,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_49 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv40_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 57344);

}


/* scheduling epoch=60   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_60(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_50 */
  Arith_sw_info arith41_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 56,
    .general.input.dim.num_elem = 14336,
    .general.input.stride.b = 57344,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 224,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 16,
    .operand.dim.tensor_w = 16,
    .operand.dim.tensor_c = 56,
    .operand.dim.num_elem = 14336,
    .operand.stride.b = 57344,
    .operand.stride.h = 3584,
    .operand.stride.w = 224,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 56,
    .general.output.dim.num_elem = 14336,
    .general.output.stride.b = 57344,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 224,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_50 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith41_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */, 57344);

}


/* scheduling epoch=61   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_61(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_51 */
  Activ_sw_info activ42_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 56,
    .general.input.dim.num_elem = 14336,
    .general.input.stride.b = 57344,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 224,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 56,
    .general.output.dim.num_elem = 14336,
    .general.output.stride.b = 57344,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 224,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_51 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ42_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */, 57344);

}


/* scheduling epoch=62   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_62(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) /* Equivalent hex address = 0x3430a000UL */, 57344);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_52 */
  static const uint32_t Transpose_52_tensor_shape_in_62_shape_0[] = { 1, 16, 16, 56 };
  static const LL_LIB_TensorShape_TypeDef Transpose_52_tensor_shape_in_62[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 172032,
      .offset_limit = 172096,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_52_tensor_shape_in_62_shape_0,
      .batch = 56,
    }
  };

  static const uint32_t Transpose_52_tensor_axes_offsets_in_62_0[] = { 57344, 3584, 224, 4 };
  static const uint32_t* Transpose_52_tensor_axes_offsets_in_62[] = {
    Transpose_52_tensor_axes_offsets_in_62_0
  };

  static const uint32_t Transpose_52_tensor_shape_out_62_shape_0[] = { 1, 56, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_52_tensor_shape_out_62[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 172032,
      .offset_end = 229376,
      .offset_limit = 229440,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_52_tensor_shape_out_62_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_52_tensor_axes_offsets_out_62_0[] = { 57344, 1024, 64, 4 };
  static const uint32_t* Transpose_52_tensor_axes_offsets_out_62[] = {
    Transpose_52_tensor_axes_offsets_out_62_0
  };

  static const uint8_t Transpose_52_perm_to_use_array_in_62[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_52_target_pos_array_in_62[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_52_tensor_shape_in_62[0], Transpose_52_tensor_axes_offsets_in_62[0], &Transpose_52_tensor_shape_out_62[0], Transpose_52_tensor_axes_offsets_out_62[0], Transpose_52_target_pos_array_in_62, Transpose_52_perm_to_use_array_in_62, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 172032))) /* Equivalent hex address = 0x3430a000UL */, 57344);

}


/* scheduling epoch=63   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_63(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_55 */
  Conv_sw_info conv43_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 56,
    .general.input.dim.num_elem = 14336,
    .general.input.stride.b = 57344,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 224,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 56,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1008,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 399552))) /* Equivalent hex address = 0x710618c0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 56,
    .general.output.dim.num_elem = 14336,
    .general.output.stride.b = 57344,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 224,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 28,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_55 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv43_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 286720))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */, 57344);

}


/* scheduling epoch=64   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_64(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 65536);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_53 */
  unsigned char* Pad_53_input_start_addr_64 = ((unsigned char *)((0x342e0000UL + 172032))) /* Equivalent hex address = 0x3430a000UL */;
  unsigned char* Pad_53_output_start_addr_64 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_53_input_addr_limit_64 = ((unsigned char *)((0x342e0000UL + 229440))) /* Equivalent hex address = 0x34318040UL */;
  unsigned char* Pad_53_output_addr_limit_64 = ((unsigned char *)((0x342e0000UL + 65600))) /* Equivalent hex address = 0x342f0040UL */;

  static const uint32_t Pad_53_min_shape_64[] = { 1, 56, 16, 16 };

  static const int32_t Pad_53_pad_in_offsets_start_64[] = { 0, 0, 0, 0 };
  static const int32_t Pad_53_pad_in_offsets_end_64[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_53_pad_out_offsets_start_64[] = { 0, 0, 0, 0 };
  static const int32_t Pad_53_pad_out_offsets_end_64[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_53_out_shape_64[] = { 1, 64, 16, 16 };
  static const int32_t Pad_53_out_offsets_64[] = { 65536, 1024, 64, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_53_input_start_addr_64, Pad_53_output_start_addr_64, Pad_53_input_addr_limit_64, Pad_53_output_addr_limit_64, Pad_53_min_shape_64, 0, 4, 16384, *((int32_t*)&pad_const_value_native), 1, 14336, Pad_53_pad_in_offsets_start_64, Pad_53_pad_in_offsets_end_64, Pad_53_pad_out_offsets_start_64, Pad_53_pad_out_offsets_end_64, Pad_53_out_shape_64, Pad_53_out_offsets_64, 4, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 65536);

}


/* scheduling epoch=65   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_65(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_55_addbias20 */
  Arith_sw_info arith44_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 56,
    .general.input.dim.num_elem = 14336,
    .general.input.stride.b = 57344,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 224,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 56,
    .operand.dim.num_elem = 56,
    .operand.stride.b = 224,
    .operand.stride.h = 224,
    .operand.stride.w = 224,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 438512))) /* Equivalent hex address = 0x7106b0f0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 56,
    .general.output.dim.num_elem = 14336,
    .general.output.stride.b = 57344,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 224,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_55_addbias20 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith44_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 188416))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */, 57344);

}


/* scheduling epoch=66   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_66(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */, 65536);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_54 */
  static const uint32_t Transpose_54_tensor_shape_in_66_shape_0[] = { 1, 64, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_54_tensor_shape_in_66[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 65536,
      .offset_limit = 65600,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_54_tensor_shape_in_66_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_54_tensor_axes_offsets_in_66_0[] = { 65536, 1024, 64, 4 };
  static const uint32_t* Transpose_54_tensor_axes_offsets_in_66[] = {
    Transpose_54_tensor_axes_offsets_in_66_0
  };

  static const uint32_t Transpose_54_tensor_shape_out_66_shape_0[] = { 1, 16, 16, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_54_tensor_shape_out_66[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 65536,
      .offset_end = 131072,
      .offset_limit = 131136,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_54_tensor_shape_out_66_shape_0,
      .batch = 64,
    }
  };

  static const uint32_t Transpose_54_tensor_axes_offsets_out_66_0[] = { 65536, 4096, 256, 4 };
  static const uint32_t* Transpose_54_tensor_axes_offsets_out_66[] = {
    Transpose_54_tensor_axes_offsets_out_66_0
  };

  static const uint8_t Transpose_54_perm_to_use_array_in_66[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_54_target_pos_array_in_66[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_54_tensor_shape_in_66[0], Transpose_54_tensor_axes_offsets_in_66[0], &Transpose_54_tensor_shape_out_66[0], Transpose_54_tensor_axes_offsets_out_66[0], Transpose_54_target_pos_array_in_66, Transpose_54_perm_to_use_array_in_66, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */, 65536);

}


/* scheduling epoch=67   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_67(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_56 */
  Conv_sw_info conv45_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 56,
    .general.input.dim.num_elem = 14336,
    .general.input.stride.b = 57344,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 224,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 56,
    .weights.dim.num_elem = 3584,
    .weights.stride.b = 224,
    .weights.stride.h = 224,
    .weights.stride.w = 224,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 287744))) /* Equivalent hex address = 0x71046400UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 437776))) /* Equivalent hex address = 0x7106ae10UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 16384,
    .general.output.stride.b = 65536,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_56 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv45_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 65536);

}


/* scheduling epoch=68   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_68(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_57 */
  Arith_sw_info arith46_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 16384,
    .general.input.stride.b = 65536,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 16,
    .operand.dim.tensor_w = 16,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 16384,
    .operand.stride.b = 65536,
    .operand.stride.h = 4096,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 16384,
    .general.output.stride.b = 65536,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_57 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith46_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */, 65536);

}


/* scheduling epoch=69   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_69(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_58 */
  Activ_sw_info activ47_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 16384,
    .general.input.stride.b = 65536,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 16384,
    .general.output.stride.b = 65536,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_58 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ47_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */, 65536);

}


/* scheduling epoch=70   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_70(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) /* Equivalent hex address = 0x34310000UL */, 65536);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_59 */
  static const uint32_t Transpose_59_tensor_shape_in_70_shape_0[] = { 1, 16, 16, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_59_tensor_shape_in_70[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 196608,
      .offset_limit = 196672,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_59_tensor_shape_in_70_shape_0,
      .batch = 64,
    }
  };

  static const uint32_t Transpose_59_tensor_axes_offsets_in_70_0[] = { 65536, 4096, 256, 4 };
  static const uint32_t* Transpose_59_tensor_axes_offsets_in_70[] = {
    Transpose_59_tensor_axes_offsets_in_70_0
  };

  static const uint32_t Transpose_59_tensor_shape_out_70_shape_0[] = { 1, 64, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_59_tensor_shape_out_70[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 196608,
      .offset_end = 262144,
      .offset_limit = 262208,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_59_tensor_shape_out_70_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_59_tensor_axes_offsets_out_70_0[] = { 65536, 1024, 64, 4 };
  static const uint32_t* Transpose_59_tensor_axes_offsets_out_70[] = {
    Transpose_59_tensor_axes_offsets_out_70_0
  };

  static const uint8_t Transpose_59_perm_to_use_array_in_70[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_59_target_pos_array_in_70[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_59_tensor_shape_in_70[0], Transpose_59_tensor_axes_offsets_in_70[0], &Transpose_59_tensor_shape_out_70[0], Transpose_59_tensor_axes_offsets_out_70[0], Transpose_59_target_pos_array_in_70, Transpose_59_perm_to_use_array_in_70, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) /* Equivalent hex address = 0x34310000UL */, 65536);

}


/* scheduling epoch=71   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_71(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_62 */
  Conv_sw_info conv48_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 16384,
    .general.input.stride.b = 65536,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 131072))) /* Equivalent hex address = 0x34300000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1152,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 394944))) /* Equivalent hex address = 0x710606c0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 16384,
    .general.output.stride.b = 65536,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 32,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_62 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv48_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */, 65536);

}


/* scheduling epoch=72   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_72(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 73728);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_60 */
  unsigned char* Pad_60_input_start_addr_72 = ((unsigned char *)((0x342e0000UL + 196608))) /* Equivalent hex address = 0x34310000UL */;
  unsigned char* Pad_60_output_start_addr_72 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_60_input_addr_limit_72 = ((unsigned char *)((0x342e0000UL + 262208))) /* Equivalent hex address = 0x34320040UL */;
  unsigned char* Pad_60_output_addr_limit_72 = ((unsigned char *)((0x342e0000UL + 73792))) /* Equivalent hex address = 0x342f2040UL */;

  static const uint32_t Pad_60_min_shape_72[] = { 1, 64, 16, 16 };

  static const int32_t Pad_60_pad_in_offsets_start_72[] = { 0, 0, 0, 0 };
  static const int32_t Pad_60_pad_in_offsets_end_72[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_60_pad_out_offsets_start_72[] = { 0, 0, 0, 0 };
  static const int32_t Pad_60_pad_out_offsets_end_72[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_60_out_shape_72[] = { 1, 72, 16, 16 };
  static const int32_t Pad_60_out_offsets_72[] = { 73728, 1024, 64, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_60_input_start_addr_72, Pad_60_output_start_addr_72, Pad_60_input_addr_limit_72, Pad_60_output_addr_limit_72, Pad_60_min_shape_72, 0, 4, 18432, *((int32_t*)&pad_const_value_native), 1, 16384, Pad_60_pad_in_offsets_start_72, Pad_60_pad_in_offsets_end_72, Pad_60_pad_out_offsets_start_72, Pad_60_pad_out_offsets_end_72, Pad_60_out_shape_72, Pad_60_out_offsets_72, 4, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 73728);

}


/* scheduling epoch=73   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_73(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_62_addbias22 */
  Arith_sw_info arith49_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 16384,
    .general.input.stride.b = 65536,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) /* Equivalent hex address = 0x34320000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 64,
    .operand.stride.b = 256,
    .operand.stride.h = 256,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 438032))) /* Equivalent hex address = 0x7106af10UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 16384,
    .general.output.stride.b = 65536,
    .general.output.stride.h = 4096,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_62_addbias22 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith49_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 212992))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 65536);

}


/* scheduling epoch=74   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_74(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) /* Equivalent hex address = 0x342f2000UL */, 73728);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_61 */
  static const uint32_t Transpose_61_tensor_shape_in_74_shape_0[] = { 1, 72, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_61_tensor_shape_in_74[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 73728,
      .offset_limit = 73792,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_61_tensor_shape_in_74_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_61_tensor_axes_offsets_in_74_0[] = { 73728, 1024, 64, 4 };
  static const uint32_t* Transpose_61_tensor_axes_offsets_in_74[] = {
    Transpose_61_tensor_axes_offsets_in_74_0
  };

  static const uint32_t Transpose_61_tensor_shape_out_74_shape_0[] = { 1, 16, 16, 72 };
  static const LL_LIB_TensorShape_TypeDef Transpose_61_tensor_shape_out_74[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 73728,
      .offset_end = 147456,
      .offset_limit = 147520,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_61_tensor_shape_out_74_shape_0,
      .batch = 72,
    }
  };

  static const uint32_t Transpose_61_tensor_axes_offsets_out_74_0[] = { 73728, 4608, 288, 4 };
  static const uint32_t* Transpose_61_tensor_axes_offsets_out_74[] = {
    Transpose_61_tensor_axes_offsets_out_74_0
  };

  static const uint8_t Transpose_61_perm_to_use_array_in_74[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_61_target_pos_array_in_74[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_61_tensor_shape_in_74[0], Transpose_61_tensor_axes_offsets_in_74[0], &Transpose_61_tensor_shape_out_74[0], Transpose_61_tensor_axes_offsets_out_74[0], Transpose_61_target_pos_array_in_74, Transpose_61_perm_to_use_array_in_74, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) /* Equivalent hex address = 0x342f2000UL */, 73728);

}


/* scheduling epoch=75   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_75(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_63 */
  Conv_sw_info conv50_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 16384,
    .general.input.stride.b = 65536,
    .general.input.stride.h = 4096,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 72,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 4608,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 269312))) /* Equivalent hex address = 0x71041c00UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 72,
    .bias.dim.num_elem = 72,
    .bias.stride.b = 288,
    .bias.stride.h = 288,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 437200))) /* Equivalent hex address = 0x7106abd0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 72,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 288,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_63 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv50_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 73728);

}


/* scheduling epoch=76   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_76(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_64 */
  Arith_sw_info arith51_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 72,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 288,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) /* Equivalent hex address = 0x342f2000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 16,
    .operand.dim.tensor_w = 16,
    .operand.dim.tensor_c = 72,
    .operand.dim.num_elem = 18432,
    .operand.stride.b = 73728,
    .operand.stride.h = 4608,
    .operand.stride.w = 288,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 72,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 288,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_64 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith51_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 221184))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 73728);

}


/* scheduling epoch=77   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_77(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_65 */
  Activ_sw_info activ52_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 72,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 288,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 72,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 288,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_65 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ52_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 221184))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 73728);

}


/* scheduling epoch=78   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_78(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 221184))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 221184))) /* Equivalent hex address = 0x34316000UL */, 73728);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_66 */
  static const uint32_t Transpose_66_tensor_shape_in_78_shape_0[] = { 1, 16, 16, 72 };
  static const LL_LIB_TensorShape_TypeDef Transpose_66_tensor_shape_in_78[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 221184,
      .offset_limit = 221248,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_66_tensor_shape_in_78_shape_0,
      .batch = 72,
    }
  };

  static const uint32_t Transpose_66_tensor_axes_offsets_in_78_0[] = { 73728, 4608, 288, 4 };
  static const uint32_t* Transpose_66_tensor_axes_offsets_in_78[] = {
    Transpose_66_tensor_axes_offsets_in_78_0
  };

  static const uint32_t Transpose_66_tensor_shape_out_78_shape_0[] = { 1, 72, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_66_tensor_shape_out_78[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 221184,
      .offset_end = 294912,
      .offset_limit = 294976,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_66_tensor_shape_out_78_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_66_tensor_axes_offsets_out_78_0[] = { 73728, 1024, 64, 4 };
  static const uint32_t* Transpose_66_tensor_axes_offsets_out_78[] = {
    Transpose_66_tensor_axes_offsets_out_78_0
  };

  static const uint8_t Transpose_66_perm_to_use_array_in_78[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_66_target_pos_array_in_78[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_66_tensor_shape_in_78[0], Transpose_66_tensor_axes_offsets_in_78[0], &Transpose_66_tensor_shape_out_78[0], Transpose_66_tensor_axes_offsets_out_78[0], Transpose_66_target_pos_array_in_78, Transpose_66_perm_to_use_array_in_78, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 221184))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 221184))) /* Equivalent hex address = 0x34316000UL */, 73728);

}


/* scheduling epoch=79   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_79(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_69 */
  Conv_sw_info conv53_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 72,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 288,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 72,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1296,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 385152))) /* Equivalent hex address = 0x7105e080UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 72,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 288,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 36,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_69 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv53_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 368640))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */, 73728);

}


/* scheduling epoch=80   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_80(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 81920);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_67 */
  unsigned char* Pad_67_input_start_addr_80 = ((unsigned char *)((0x342e0000UL + 221184))) /* Equivalent hex address = 0x34316000UL */;
  unsigned char* Pad_67_output_start_addr_80 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_67_input_addr_limit_80 = ((unsigned char *)((0x342e0000UL + 294976))) /* Equivalent hex address = 0x34328040UL */;
  unsigned char* Pad_67_output_addr_limit_80 = ((unsigned char *)((0x342e0000UL + 81984))) /* Equivalent hex address = 0x342f4040UL */;

  static const uint32_t Pad_67_min_shape_80[] = { 1, 72, 16, 16 };

  static const int32_t Pad_67_pad_in_offsets_start_80[] = { 0, 0, 0, 0 };
  static const int32_t Pad_67_pad_in_offsets_end_80[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_67_pad_out_offsets_start_80[] = { 0, 0, 0, 0 };
  static const int32_t Pad_67_pad_out_offsets_end_80[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_67_out_shape_80[] = { 1, 80, 16, 16 };
  static const int32_t Pad_67_out_offsets_80[] = { 81920, 1024, 64, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_67_input_start_addr_80, Pad_67_output_start_addr_80, Pad_67_input_addr_limit_80, Pad_67_output_addr_limit_80, Pad_67_min_shape_80, 0, 4, 20480, *((int32_t*)&pad_const_value_native), 1, 18432, Pad_67_pad_in_offsets_start_80, Pad_67_pad_in_offsets_end_80, Pad_67_pad_out_offsets_start_80, Pad_67_pad_out_offsets_end_80, Pad_67_out_shape_80, Pad_67_out_offsets_80, 4, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 81920);

}


/* scheduling epoch=81   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_81(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_69_addbias24 */
  Arith_sw_info arith54_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 72,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 288,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) /* Equivalent hex address = 0x34328000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 72,
    .operand.dim.num_elem = 72,
    .operand.stride.b = 288,
    .operand.stride.h = 288,
    .operand.stride.w = 288,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 437488))) /* Equivalent hex address = 0x7106acf0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 72,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 4608,
    .general.output.stride.w = 288,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 368640))) /* Equivalent hex address = 0x3433a000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_69_addbias24 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith54_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 368640))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 442368))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 368640))) /* Equivalent hex address = 0x3433a000UL */, 73728);

}


/* scheduling epoch=82   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_82(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) /* Equivalent hex address = 0x342f4000UL */, 81920);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_68 */
  static const uint32_t Transpose_68_tensor_shape_in_82_shape_0[] = { 1, 80, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_68_tensor_shape_in_82[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 81920,
      .offset_limit = 81984,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_68_tensor_shape_in_82_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_68_tensor_axes_offsets_in_82_0[] = { 81920, 1024, 64, 4 };
  static const uint32_t* Transpose_68_tensor_axes_offsets_in_82[] = {
    Transpose_68_tensor_axes_offsets_in_82_0
  };

  static const uint32_t Transpose_68_tensor_shape_out_82_shape_0[] = { 1, 16, 16, 80 };
  static const LL_LIB_TensorShape_TypeDef Transpose_68_tensor_shape_out_82[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 81920,
      .offset_end = 163840,
      .offset_limit = 163904,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_68_tensor_shape_out_82_shape_0,
      .batch = 80,
    }
  };

  static const uint32_t Transpose_68_tensor_axes_offsets_out_82_0[] = { 81920, 5120, 320, 4 };
  static const uint32_t* Transpose_68_tensor_axes_offsets_out_82[] = {
    Transpose_68_tensor_axes_offsets_out_82_0
  };

  static const uint8_t Transpose_68_perm_to_use_array_in_82[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_68_target_pos_array_in_82[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_68_tensor_shape_in_82[0], Transpose_68_tensor_axes_offsets_in_82[0], &Transpose_68_tensor_shape_out_82[0], Transpose_68_tensor_axes_offsets_out_82[0], Transpose_68_target_pos_array_in_82, Transpose_68_perm_to_use_array_in_82, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) /* Equivalent hex address = 0x342f4000UL */, 81920);

}


/* scheduling epoch=83   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_83(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_70 */
  Conv_sw_info conv55_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 72,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 4608,
    .general.input.stride.w = 288,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 368640))) /* Equivalent hex address = 0x3433a000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 80,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 72,
    .weights.dim.num_elem = 5760,
    .weights.stride.b = 288,
    .weights.stride.h = 288,
    .weights.stride.w = 288,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 246272))) /* Equivalent hex address = 0x7103c200UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 80,
    .bias.dim.num_elem = 80,
    .bias.stride.b = 320,
    .bias.stride.h = 320,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 436560))) /* Equivalent hex address = 0x7106a950UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 80,
    .general.output.dim.num_elem = 20480,
    .general.output.stride.b = 81920,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 320,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_70 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv55_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 81920);

}


/* scheduling epoch=84   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_84(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_71 */
  Arith_sw_info arith56_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 80,
    .general.input.dim.num_elem = 20480,
    .general.input.stride.b = 81920,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 320,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) /* Equivalent hex address = 0x342f4000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 16,
    .operand.dim.tensor_w = 16,
    .operand.dim.tensor_c = 80,
    .operand.dim.num_elem = 20480,
    .operand.stride.b = 81920,
    .operand.stride.h = 5120,
    .operand.stride.w = 320,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 80,
    .general.output.dim.num_elem = 20480,
    .general.output.stride.b = 81920,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 320,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) /* Equivalent hex address = 0x34308000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_71 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith56_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) /* Equivalent hex address = 0x34308000UL */, 81920);

}


/* scheduling epoch=85   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_85(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_72 */
  Activ_sw_info activ57_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 80,
    .general.input.dim.num_elem = 20480,
    .general.input.stride.b = 81920,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 320,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) /* Equivalent hex address = 0x34308000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 80,
    .general.output.dim.num_elem = 20480,
    .general.output.stride.b = 81920,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 320,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) /* Equivalent hex address = 0x34308000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_72 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ57_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) /* Equivalent hex address = 0x34308000UL */, 81920);

}


/* scheduling epoch=86   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_86(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) /* Equivalent hex address = 0x3431c000UL */, 81920);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_73 */
  static const uint32_t Transpose_73_tensor_shape_in_86_shape_0[] = { 1, 16, 16, 80 };
  static const LL_LIB_TensorShape_TypeDef Transpose_73_tensor_shape_in_86[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 163840,
      .offset_end = 245760,
      .offset_limit = 245824,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_73_tensor_shape_in_86_shape_0,
      .batch = 80,
    }
  };

  static const uint32_t Transpose_73_tensor_axes_offsets_in_86_0[] = { 81920, 5120, 320, 4 };
  static const uint32_t* Transpose_73_tensor_axes_offsets_in_86[] = {
    Transpose_73_tensor_axes_offsets_in_86_0
  };

  static const uint32_t Transpose_73_tensor_shape_out_86_shape_0[] = { 1, 80, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_73_tensor_shape_out_86[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 245760,
      .offset_end = 327680,
      .offset_limit = 327744,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_73_tensor_shape_out_86_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_73_tensor_axes_offsets_out_86_0[] = { 81920, 1024, 64, 4 };
  static const uint32_t* Transpose_73_tensor_axes_offsets_out_86[] = {
    Transpose_73_tensor_axes_offsets_out_86_0
  };

  static const uint8_t Transpose_73_perm_to_use_array_in_86[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_73_target_pos_array_in_86[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_73_tensor_shape_in_86[0], Transpose_73_tensor_axes_offsets_in_86[0], &Transpose_73_tensor_shape_out_86[0], Transpose_73_tensor_axes_offsets_out_86[0], Transpose_73_target_pos_array_in_86, Transpose_73_perm_to_use_array_in_86, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) /* Equivalent hex address = 0x3431c000UL */, 81920);

}


/* scheduling epoch=87   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_87(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_76 */
  Conv_sw_info conv58_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 80,
    .general.input.dim.num_elem = 20480,
    .general.input.stride.b = 81920,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 320,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 163840))) /* Equivalent hex address = 0x34308000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 80,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1440,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 379392))) /* Equivalent hex address = 0x7105ca00UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 80,
    .general.output.dim.num_elem = 20480,
    .general.output.stride.b = 81920,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 320,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 40,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_76 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv58_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 81920);

}


/* scheduling epoch=88   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_88(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 90112);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_74 */
  unsigned char* Pad_74_input_start_addr_88 = ((unsigned char *)((0x342e0000UL + 245760))) /* Equivalent hex address = 0x3431c000UL */;
  unsigned char* Pad_74_output_start_addr_88 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* Pad_74_input_addr_limit_88 = ((unsigned char *)((0x342e0000UL + 327744))) /* Equivalent hex address = 0x34330040UL */;
  unsigned char* Pad_74_output_addr_limit_88 = ((unsigned char *)((0x342e0000UL + 90176))) /* Equivalent hex address = 0x342f6040UL */;

  static const uint32_t Pad_74_min_shape_88[] = { 1, 80, 16, 16 };

  static const int32_t Pad_74_pad_in_offsets_start_88[] = { 0, 0, 0, 0 };
  static const int32_t Pad_74_pad_in_offsets_end_88[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_74_pad_out_offsets_start_88[] = { 0, 0, 0, 0 };
  static const int32_t Pad_74_pad_out_offsets_end_88[] = { 0, 8192, 0, 0 };

  static const int32_t Pad_74_out_shape_88[] = { 1, 88, 16, 16 };
  static const int32_t Pad_74_out_offsets_88[] = { 90112, 1024, 64, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_74_input_start_addr_88, Pad_74_output_start_addr_88, Pad_74_input_addr_limit_88, Pad_74_output_addr_limit_88, Pad_74_min_shape_88, 0, 4, 22528, *((int32_t*)&pad_const_value_native), 1, 20480, Pad_74_pad_in_offsets_start_88, Pad_74_pad_in_offsets_end_88, Pad_74_pad_out_offsets_start_88, Pad_74_pad_out_offsets_end_88, Pad_74_out_shape_88, Pad_74_out_offsets_88, 4, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 90112);

}


/* scheduling epoch=89   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_89(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_76_addbias26 */
  Arith_sw_info arith59_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 80,
    .general.input.dim.num_elem = 20480,
    .general.input.stride.b = 81920,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 320,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 80,
    .operand.dim.num_elem = 80,
    .operand.stride.b = 320,
    .operand.stride.h = 320,
    .operand.stride.w = 320,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 436880))) /* Equivalent hex address = 0x7106aa90UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 80,
    .general.output.dim.num_elem = 20480,
    .general.output.stride.b = 81920,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 320,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_76_addbias26 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith59_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 262144))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */, 81920);

}


/* scheduling epoch=90   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_90(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) /* Equivalent hex address = 0x342f6000UL */, 90112);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_75 */
  static const uint32_t Transpose_75_tensor_shape_in_90_shape_0[] = { 1, 88, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_75_tensor_shape_in_90[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 90112,
      .offset_limit = 90176,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_75_tensor_shape_in_90_shape_0,
      .batch = 16,
    }
  };

  static const uint32_t Transpose_75_tensor_axes_offsets_in_90_0[] = { 90112, 1024, 64, 4 };
  static const uint32_t* Transpose_75_tensor_axes_offsets_in_90[] = {
    Transpose_75_tensor_axes_offsets_in_90_0
  };

  static const uint32_t Transpose_75_tensor_shape_out_90_shape_0[] = { 1, 16, 16, 88 };
  static const LL_LIB_TensorShape_TypeDef Transpose_75_tensor_shape_out_90[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 90112,
      .offset_end = 180224,
      .offset_limit = 180288,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_75_tensor_shape_out_90_shape_0,
      .batch = 88,
    }
  };

  static const uint32_t Transpose_75_tensor_axes_offsets_out_90_0[] = { 90112, 5632, 352, 4 };
  static const uint32_t* Transpose_75_tensor_axes_offsets_out_90[] = {
    Transpose_75_tensor_axes_offsets_out_90_0
  };

  static const uint8_t Transpose_75_perm_to_use_array_in_90[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_75_target_pos_array_in_90[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_75_tensor_shape_in_90[0], Transpose_75_tensor_axes_offsets_in_90[0], &Transpose_75_tensor_shape_out_90[0], Transpose_75_tensor_axes_offsets_out_90[0], Transpose_75_target_pos_array_in_90, Transpose_75_perm_to_use_array_in_90, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) /* Equivalent hex address = 0x342f6000UL */, 90112);

}


/* scheduling epoch=91   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_91(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_77 */
  Conv_sw_info conv60_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 80,
    .general.input.dim.num_elem = 20480,
    .general.input.stride.b = 81920,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 320,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 88,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 80,
    .weights.dim.num_elem = 7040,
    .weights.stride.b = 320,
    .weights.stride.h = 320,
    .weights.stride.w = 320,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 218112))) /* Equivalent hex address = 0x71035400UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 88,
    .bias.dim.num_elem = 88,
    .bias.stride.b = 352,
    .bias.stride.h = 352,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 435856))) /* Equivalent hex address = 0x7106a690UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 88,
    .general.output.dim.num_elem = 22528,
    .general.output.stride.b = 90112,
    .general.output.stride.h = 5632,
    .general.output.stride.w = 352,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_77 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv60_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 90112);

}


/* scheduling epoch=92   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_92(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_78 */
  Arith_sw_info arith61_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 22528,
    .general.input.stride.b = 90112,
    .general.input.stride.h = 5632,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 90112))) /* Equivalent hex address = 0x342f6000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 16,
    .operand.dim.tensor_w = 16,
    .operand.dim.tensor_c = 88,
    .operand.dim.num_elem = 22528,
    .operand.stride.b = 90112,
    .operand.stride.h = 5632,
    .operand.stride.w = 352,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 88,
    .general.output.dim.num_elem = 22528,
    .general.output.stride.b = 90112,
    .general.output.stride.h = 5632,
    .general.output.stride.w = 352,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_78 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith61_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 270336))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */, 90112);

}


/* scheduling epoch=93   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_93(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_79 */
  Activ_sw_info activ62_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 22528,
    .general.input.stride.b = 90112,
    .general.input.stride.h = 5632,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 88,
    .general.output.dim.num_elem = 22528,
    .general.output.stride.b = 90112,
    .general.output.stride.h = 5632,
    .general.output.stride.w = 352,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_79 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ62_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 270336))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */, 90112);

}


/* scheduling epoch=94   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_94(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_84 */
  Pool_sw_info pool63_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 22528,
    .general.input.stride.b = 90112,
    .general.input.stride.h = 5632,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 88,
    .general.output.dim.num_elem = 5632,
    .general.output.stride.b = 22528,
    .general.output.stride.h = 2816,
    .general.output.stride.w = 352,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 1, 1},
    .strides = {2, 2},
    .k_shape = {2, 2},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_84 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool63_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 325632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 22528);

}


/* scheduling epoch=95   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_95(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_118 */
  Conv_sw_info conv64_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 22528,
    .general.input.stride.b = 90112,
    .general.input.stride.h = 5632,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 2,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 88,
    .weights.dim.num_elem = 176,
    .weights.stride.b = 352,
    .weights.stride.h = 352,
    .weights.stride.w = 352,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 431312))) /* Equivalent hex address = 0x710694d0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 2,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 128,
    .general.output.stride.w = 8,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) /* Equivalent hex address = 0x3433a800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_118 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv64_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 372736))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) /* Equivalent hex address = 0x3433a800UL */, 2048);

}


/* scheduling epoch=96   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_96(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_88 */
  Conv_sw_info conv65_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 22528,
    .general.input.stride.b = 90112,
    .general.input.stride.h = 5632,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 88,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1584,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 367008))) /* Equivalent hex address = 0x710599a0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 88,
    .general.output.dim.num_elem = 5632,
    .general.output.stride.b = 22528,
    .general.output.stride.h = 2816,
    .general.output.stride.w = 352,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 325632))) /* Equivalent hex address = 0x3432f800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 44,
    .pads = {0, 0, 2, 2},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_88 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv65_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 325632))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 348160))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 325632))) /* Equivalent hex address = 0x3432f800UL */, 22528);

}


/* scheduling epoch=97   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_97(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_80 */
  Conv_sw_info conv66_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 22528,
    .general.input.stride.b = 90112,
    .general.input.stride.h = 5632,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 180224))) /* Equivalent hex address = 0x3430c000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 88,
    .weights.dim.num_elem = 2816,
    .weights.stride.b = 352,
    .weights.stride.h = 352,
    .weights.stride.w = 352,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 302080))) /* Equivalent hex address = 0x71049c00UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 440016))) /* Equivalent hex address = 0x7106b6d0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 8192,
    .general.output.stride.b = 32768,
    .general.output.stride.h = 2048,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 270336))) /* Equivalent hex address = 0x34322000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_80 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv66_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 270336))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 270336))) /* Equivalent hex address = 0x34322000UL */, 32768);

}


/* scheduling epoch=98   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_98(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 348160))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 348160))) /* Equivalent hex address = 0x34335000UL */, 22528);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_85 */
  static const uint32_t Transpose_85_tensor_shape_in_98_shape_0[] = { 1, 8, 8, 88 };
  static const LL_LIB_TensorShape_TypeDef Transpose_85_tensor_shape_in_98[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 325632,
      .offset_limit = 325696,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_85_tensor_shape_in_98_shape_0,
      .batch = 88,
    }
  };

  static const uint32_t Transpose_85_tensor_axes_offsets_in_98_0[] = { 22528, 2816, 352, 4 };
  static const uint32_t* Transpose_85_tensor_axes_offsets_in_98[] = {
    Transpose_85_tensor_axes_offsets_in_98_0
  };

  static const uint32_t Transpose_85_tensor_shape_out_98_shape_0[] = { 1, 88, 8, 8 };
  static const LL_LIB_TensorShape_TypeDef Transpose_85_tensor_shape_out_98[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 348160,
      .offset_end = 370688,
      .offset_limit = 370752,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_85_tensor_shape_out_98_shape_0,
      .batch = 8,
    }
  };

  static const uint32_t Transpose_85_tensor_axes_offsets_out_98_0[] = { 22528, 256, 32, 4 };
  static const uint32_t* Transpose_85_tensor_axes_offsets_out_98[] = {
    Transpose_85_tensor_axes_offsets_out_98_0
  };

  static const uint8_t Transpose_85_perm_to_use_array_in_98[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_85_target_pos_array_in_98[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_85_tensor_shape_in_98[0], Transpose_85_tensor_axes_offsets_in_98[0], &Transpose_85_tensor_shape_out_98[0], Transpose_85_tensor_axes_offsets_out_98[0], Transpose_85_target_pos_array_in_98, Transpose_85_perm_to_use_array_in_98, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 348160))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 348160))) /* Equivalent hex address = 0x34335000UL */, 22528);

}


/* scheduling epoch=99   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_99(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_118_addbias40 */
  Arith_sw_info arith67_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 16,
    .general.input.dim.tensor_c = 2,
    .general.input.dim.num_elem = 512,
    .general.input.stride.b = 2048,
    .general.input.stride.h = 128,
    .general.input.stride.w = 8,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) /* Equivalent hex address = 0x3433a800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 2,
    .operand.dim.num_elem = 2,
    .operand.stride.b = 8,
    .operand.stride.h = 8,
    .operand.stride.w = 8,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 441488))) /* Equivalent hex address = 0x7106bc90UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 16,
    .general.output.dim.tensor_c = 2,
    .general.output.dim.num_elem = 512,
    .general.output.stride.b = 2048,
    .general.output.stride.h = 128,
    .general.output.stride.w = 8,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) /* Equivalent hex address = 0x34342000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_118_addbias40 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith67_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 403456))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) /* Equivalent hex address = 0x34342000UL */, 2048);

}


/* scheduling epoch=100  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_100(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_88_addbias28 */
  Arith_sw_info arith68_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 5632,
    .general.input.stride.b = 22528,
    .general.input.stride.h = 2816,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 325632))) /* Equivalent hex address = 0x3432f800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 88,
    .operand.dim.num_elem = 88,
    .operand.stride.b = 352,
    .operand.stride.h = 352,
    .operand.stride.w = 352,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 436208))) /* Equivalent hex address = 0x7106a7f0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 88,
    .general.output.dim.num_elem = 5632,
    .general.output.stride.b = 22528,
    .general.output.stride.h = 2816,
    .general.output.stride.w = 352,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) /* Equivalent hex address = 0x3433a800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_88_addbias28 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith68_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 393216))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) /* Equivalent hex address = 0x3433a800UL */, 22528);

}


/* scheduling epoch=101  nodes=3   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_101(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=Pad_86 */
  unsigned char* Pad_86_input_start_addr_101 = ((unsigned char *)((0x342e0000UL + 348160))) /* Equivalent hex address = 0x34335000UL */;
  unsigned char* Pad_86_output_start_addr_101 = ((unsigned char *)((0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */;
  unsigned char* Pad_86_input_addr_limit_101 = ((unsigned char *)((0x342e0000UL + 370752))) /* Equivalent hex address = 0x3433a840UL */;
  unsigned char* Pad_86_output_addr_limit_101 = ((unsigned char *)((0x342e0000UL + 327744))) /* Equivalent hex address = 0x34330040UL */;

  static const uint32_t Pad_86_min_shape_101[] = { 1, 88, 8, 8 };

  static const int32_t Pad_86_pad_in_offsets_start_101[] = { 0, 0, 0, 0 };
  static const int32_t Pad_86_pad_in_offsets_end_101[] = { 0, 2048, 0, 0 };

  static const int32_t Pad_86_pad_out_offsets_start_101[] = { 0, 0, 0, 0 };
  static const int32_t Pad_86_pad_out_offsets_end_101[] = { 0, 2048, 0, 0 };

  static const int32_t Pad_86_out_shape_101[] = { 1, 96, 8, 8 };
  static const int32_t Pad_86_out_offsets_101[] = { 24576, 256, 32, 4 };

  const float pad_const_value_native = 0;
  assert(sizeof(float) == 4);

  LL_ATON_LIB_Pad(Pad_86_input_start_addr_101, Pad_86_output_start_addr_101, Pad_86_input_addr_limit_101, Pad_86_output_addr_limit_101, Pad_86_min_shape_101, 0, 4, 6144, *((int32_t*)&pad_const_value_native), 1, 5632, Pad_86_pad_in_offsets_start_101, Pad_86_pad_in_offsets_end_101, Pad_86_pad_out_offsets_start_101, Pad_86_pad_out_offsets_end_101, Pad_86_out_shape_101, Pad_86_out_offsets_101, 4, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=102  nodes=3   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_102(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_89 */
  Conv_sw_info conv69_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 88,
    .general.input.dim.num_elem = 5632,
    .general.input.stride.b = 22528,
    .general.input.stride.h = 2816,
    .general.input.stride.w = 352,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 370688))) /* Equivalent hex address = 0x3433a800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 88,
    .weights.dim.num_elem = 8448,
    .weights.stride.b = 352,
    .weights.stride.h = 352,
    .weights.stride.w = 352,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 184320))) /* Equivalent hex address = 0x7102d000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 96,
    .bias.dim.num_elem = 96,
    .bias.stride.b = 384,
    .bias.stride.h = 384,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 433936))) /* Equivalent hex address = 0x71069f10UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_89 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv69_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=103  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_103(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_87 */
  static const uint32_t Transpose_87_tensor_shape_in_103_shape_0[] = { 1, 96, 8, 8 };
  static const LL_LIB_TensorShape_TypeDef Transpose_87_tensor_shape_in_103[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_87_tensor_shape_in_103_shape_0,
      .batch = 8,
    }
  };

  static const uint32_t Transpose_87_tensor_axes_offsets_in_103_0[] = { 24576, 256, 32, 4 };
  static const uint32_t* Transpose_87_tensor_axes_offsets_in_103[] = {
    Transpose_87_tensor_axes_offsets_in_103_0
  };

  static const uint32_t Transpose_87_tensor_shape_out_103_shape_0[] = { 1, 8, 8, 96 };
  static const LL_LIB_TensorShape_TypeDef Transpose_87_tensor_shape_out_103[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_87_tensor_shape_out_103_shape_0,
      .batch = 96,
    }
  };

  static const uint32_t Transpose_87_tensor_axes_offsets_out_103_0[] = { 24576, 3072, 384, 4 };
  static const uint32_t* Transpose_87_tensor_axes_offsets_out_103[] = {
    Transpose_87_tensor_axes_offsets_out_103_0
  };

  static const uint8_t Transpose_87_perm_to_use_array_in_103[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_87_target_pos_array_in_103[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_87_tensor_shape_in_103[0], Transpose_87_tensor_axes_offsets_in_103[0], &Transpose_87_tensor_shape_out_103[0], Transpose_87_tensor_axes_offsets_out_103[0], Transpose_87_target_pos_array_in_103, Transpose_87_perm_to_use_array_in_103, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=104  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_104(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_90 */
  Arith_sw_info arith70_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 8,
    .operand.dim.tensor_w = 8,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 6144,
    .operand.stride.b = 24576,
    .operand.stride.h = 3072,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_90 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith70_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=105  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_105(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_91 */
  Activ_sw_info activ71_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_91 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ71_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=106  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_106(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_92 */
  Conv_sw_info conv72_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1728,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 339360))) /* Equivalent hex address = 0x71052da0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 48,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_92 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv72_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=107  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_107(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_92_addbias30 */
  Arith_sw_info arith73_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 96,
    .operand.stride.b = 384,
    .operand.stride.h = 384,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 434320))) /* Equivalent hex address = 0x7106a090UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_92_addbias30 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith73_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=108  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_108(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_93 */
  Conv_sw_info conv74_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 0))) /* Equivalent hex address = 0x71000000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 96,
    .bias.dim.num_elem = 96,
    .bias.stride.b = 384,
    .bias.stride.h = 384,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 432016))) /* Equivalent hex address = 0x71069790UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_93 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv74_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=109  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_109(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_94 */
  Arith_sw_info arith75_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 8,
    .operand.dim.tensor_w = 8,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 6144,
    .operand.stride.b = 24576,
    .operand.stride.h = 3072,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_94 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith75_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=110  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_110(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_95 */
  Activ_sw_info activ76_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_95 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ76_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=111  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_111(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_96 */
  Conv_sw_info conv77_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1728,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 346272))) /* Equivalent hex address = 0x710548a0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 48,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_96 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv77_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=112  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_112(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_96_addbias32 */
  Arith_sw_info arith78_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 96,
    .operand.stride.b = 384,
    .operand.stride.h = 384,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 434704))) /* Equivalent hex address = 0x7106a210UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_96_addbias32 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith78_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=113  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_113(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_97 */
  Conv_sw_info conv79_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 36864))) /* Equivalent hex address = 0x71009000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 96,
    .bias.dim.num_elem = 96,
    .bias.stride.b = 384,
    .bias.stride.h = 384,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 432400))) /* Equivalent hex address = 0x71069910UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_97 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv79_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=114  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_114(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_98 */
  Arith_sw_info arith80_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 8,
    .operand.dim.tensor_w = 8,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 6144,
    .operand.stride.b = 24576,
    .operand.stride.h = 3072,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_98 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith80_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=115  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_115(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_99 */
  Activ_sw_info activ81_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_99 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ81_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=116  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_116(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_100 */
  Conv_sw_info conv82_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1728,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 353184))) /* Equivalent hex address = 0x710563a0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 48,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_100 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv82_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=117  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_117(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_100_addbias34 */
  Arith_sw_info arith83_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 96,
    .operand.stride.b = 384,
    .operand.stride.h = 384,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 435088))) /* Equivalent hex address = 0x7106a390UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_100_addbias34 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith83_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=118  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_118(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_101 */
  Conv_sw_info conv84_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 73728))) /* Equivalent hex address = 0x71012000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 96,
    .bias.dim.num_elem = 96,
    .bias.stride.b = 384,
    .bias.stride.h = 384,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 432784))) /* Equivalent hex address = 0x71069a90UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_101 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv84_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=119  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_119(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_102 */
  Arith_sw_info arith85_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 8,
    .operand.dim.tensor_w = 8,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 6144,
    .operand.stride.b = 24576,
    .operand.stride.h = 3072,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_102 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith85_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=120  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_120(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_103 */
  Activ_sw_info activ86_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_103 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ86_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */, 24576);

}


/* scheduling epoch=121  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_121(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_104 */
  Conv_sw_info conv87_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 2,
    .weights.dim.num_elem = 1728,
    .weights.stride.b = 72,
    .weights.stride.h = 24,
    .weights.stride.w = 8,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 360096))) /* Equivalent hex address = 0x71057ea0UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 48,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_104 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv87_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=122  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_122(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_104_addbias36 */
  Arith_sw_info arith88_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 96,
    .operand.stride.b = 384,
    .operand.stride.h = 384,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 435472))) /* Equivalent hex address = 0x7106a510UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_104_addbias36 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith88_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */, 24576);

}


/* scheduling epoch=123  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_123(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_105 */
  Conv_sw_info conv89_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) /* Equivalent hex address = 0x34330000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 110592))) /* Equivalent hex address = 0x7101b000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 96,
    .bias.dim.num_elem = 96,
    .bias.stride.b = 384,
    .bias.stride.h = 384,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 433168))) /* Equivalent hex address = 0x71069c10UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_105 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv89_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 327680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 24576);

}


/* scheduling epoch=124  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_124(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_106 */
  Arith_sw_info arith90_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 352256))) /* Equivalent hex address = 0x34336000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 8,
    .operand.dim.tensor_w = 8,
    .operand.dim.tensor_c = 96,
    .operand.dim.num_elem = 6144,
    .operand.stride.b = 24576,
    .operand.stride.h = 3072,
    .operand.stride.w = 384,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_106 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith90_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */, 24576);

}


/* scheduling epoch=125  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_125(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Relu node=Relu_107 */
  Activ_sw_info activ91_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_RELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Relu_107 mapped on EmbedNets (FLOAT) as Relu | Category: Computational */
  ll_sw_forward_activ(&activ91_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */, 24576);

}


/* scheduling epoch=126  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_126(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_114 */
  Conv_sw_info conv92_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 6,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 576,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 423536))) /* Equivalent hex address = 0x71067670UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 6,
    .general.output.dim.num_elem = 384,
    .general.output.stride.b = 1536,
    .general.output.stride.h = 192,
    .general.output.stride.w = 24,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) /* Equivalent hex address = 0x34338000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_114 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv92_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 361984))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) /* Equivalent hex address = 0x34338000UL */, 1536);

}


/* scheduling epoch=127  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_127(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_108 */
  Conv_sw_info conv93_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 6144,
    .general.input.stride.b = 24576,
    .general.input.stride.h = 3072,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 376832))) /* Equivalent hex address = 0x3433c000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 96,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 147456))) /* Equivalent hex address = 0x71024000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 96,
    .bias.dim.num_elem = 96,
    .bias.stride.b = 384,
    .bias.stride.h = 384,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 433552))) /* Equivalent hex address = 0x71069d90UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 96,
    .general.output.dim.num_elem = 6144,
    .general.output.stride.b = 24576,
    .general.output.stride.h = 3072,
    .general.output.stride.w = 384,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 335872))) /* Equivalent hex address = 0x34332000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_108 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv93_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 335872))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 335872))) /* Equivalent hex address = 0x34332000UL */, 24576);

}


/* scheduling epoch=128  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_128(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Conv2D_114_addbias38 */
  Arith_sw_info arith94_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 8,
    .general.input.dim.tensor_w = 8,
    .general.input.dim.tensor_c = 6,
    .general.input.dim.num_elem = 384,
    .general.input.stride.b = 1536,
    .general.input.stride.h = 192,
    .general.input.stride.w = 24,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) /* Equivalent hex address = 0x34338000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 6,
    .operand.dim.num_elem = 6,
    .operand.stride.b = 24,
    .operand.stride.h = 24,
    .operand.stride.w = 24,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 441456))) /* Equivalent hex address = 0x7106bc70UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 8,
    .general.output.dim.tensor_w = 8,
    .general.output.dim.tensor_c = 6,
    .general.output.dim.num_elem = 384,
    .general.output.stride.b = 1536,
    .general.output.stride.h = 192,
    .general.output.stride.w = 24,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 403456))) /* Equivalent hex address = 0x34342800UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_114_addbias38 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith94_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 403456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 404992))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 403456))) /* Equivalent hex address = 0x34342800UL */, 1536);

}


// Epoch Controller Blob (name='_ec_blob_129') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_129') start function
static void _ec_blob_cache_start_func_129(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 335872))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 303104))) /* Equivalent hex address = 0x3432a000UL */, 32768);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 388608))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360448))) /* Equivalent hex address = 0x34338000UL */, 28160);

};


/* scheduling epoch=131  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_131(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 57344);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_112 */
  static const uint32_t Concat_112_tensor_info_in_131__shape_1_16_512[] = { 1, 16, 512, 1 };
  static const uint32_t Concat_112_tensor_info_in_131__mem_shape_F_1_16_512[] = { 1, 16, 512 };
  static const uint32_t Concat_112_tensor_info_in_131__shape_1_16_384[] = { 1, 16, 384, 1 };
  static const uint32_t Concat_112_tensor_info_in_131__mem_shape_F_1_16_384[] = { 1, 16, 384 };
  static const LL_Buffer_InfoTypeDef Concat_112_tensor_info_in_131[] = {
    {
      .name = "Transpose_83_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 335872,
      .offset_limit = 335936,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 131,
      .batch = 1,
      .mem_shape = Concat_112_tensor_info_in_131__mem_shape_F_1_16_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_112_tensor_info_in_131__shape_1_16_512,
    },
    {
      .name = "Transpose_111_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 360448,
      .offset_end = 385024,
      .offset_limit = 385088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 131,
      .batch = 1,
      .mem_shape = Concat_112_tensor_info_in_131__mem_shape_F_1_16_384,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_112_tensor_info_in_131__shape_1_16_384,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_112_tensor_info_out_131__shape_1_16_896[] = { 1, 16, 896, 1 };
  static const uint32_t Concat_112_tensor_info_out_131__mem_shape_F_1_16_896[] = { 1, 16, 896 };
  static const LL_Buffer_InfoTypeDef Concat_112_tensor_info_out_131[] = {
    {
      .name = "Concat_112_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 131,
      .batch = 1,
      .mem_shape = Concat_112_tensor_info_out_131__mem_shape_F_1_16_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_112_tensor_info_out_131__shape_1_16_896,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_112_tensor_info_in_131, 2, Concat_112_tensor_info_out_131, 3, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 57344);

}


// Epoch Controller Blob (name='_ec_blob_132') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_132') start function
static void _ec_blob_cache_start_func_132(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 57344);

};


/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_Default(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = NULL,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_1),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 1,
      .last_epoch_num = 1,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_2,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_3,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 3,
      .last_epoch_num = 3,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_4,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 4,
      .last_epoch_num = 4,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_5,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 5,
      .last_epoch_num = 5,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_6,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 6,
      .last_epoch_num = 6,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_7,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 7,
      .last_epoch_num = 7,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_8,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 8,
      .last_epoch_num = 8,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_9,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 9,
      .last_epoch_num = 9,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_10,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 10,
      .last_epoch_num = 10,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_11,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 11,
      .last_epoch_num = 11,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_12,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 12,
      .last_epoch_num = 12,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_13,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 13,
      .last_epoch_num = 13,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_14,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 14,
      .last_epoch_num = 14,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_15,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 15,
      .last_epoch_num = 15,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_16,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 16,
      .last_epoch_num = 16,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_17,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 17,
      .last_epoch_num = 17,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_18,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 18,
      .last_epoch_num = 18,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_19,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 19,
      .last_epoch_num = 19,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_20,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 20,
      .last_epoch_num = 20,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_21,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 21,
      .last_epoch_num = 21,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_22,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 22,
      .last_epoch_num = 22,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_23,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 23,
      .last_epoch_num = 23,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_24,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 24,
      .last_epoch_num = 24,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_25,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 25,
      .last_epoch_num = 25,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_26,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 26,
      .last_epoch_num = 26,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_27,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 27,
      .last_epoch_num = 27,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_28,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 28,
      .last_epoch_num = 28,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_29,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 29,
      .last_epoch_num = 29,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_30,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 30,
      .last_epoch_num = 30,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_31,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 31,
      .last_epoch_num = 31,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_32,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 32,
      .last_epoch_num = 32,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_33,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 33,
      .last_epoch_num = 33,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_34,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 34,
      .last_epoch_num = 34,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_35,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 35,
      .last_epoch_num = 35,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_36,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 36,
      .last_epoch_num = 36,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_37,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 37,
      .last_epoch_num = 37,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_38,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 38,
      .last_epoch_num = 38,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_39,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 39,
      .last_epoch_num = 39,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_40,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 40,
      .last_epoch_num = 40,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_41,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 41,
      .last_epoch_num = 41,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_42,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 42,
      .last_epoch_num = 42,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_43,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 43,
      .last_epoch_num = 43,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_44,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 44,
      .last_epoch_num = 44,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_45,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 45,
      .last_epoch_num = 45,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_46,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 46,
      .last_epoch_num = 46,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_47,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 47,
      .last_epoch_num = 47,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_48,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 48,
      .last_epoch_num = 48,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_49,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 49,
      .last_epoch_num = 49,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_50,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 50,
      .last_epoch_num = 50,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_51,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 51,
      .last_epoch_num = 51,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_52,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 52,
      .last_epoch_num = 52,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_53,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 53,
      .last_epoch_num = 53,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_54,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 54,
      .last_epoch_num = 54,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_55,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 55,
      .last_epoch_num = 55,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_56,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 56,
      .last_epoch_num = 56,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_57,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 57,
      .last_epoch_num = 57,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_58,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 58,
      .last_epoch_num = 58,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_59,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 59,
      .last_epoch_num = 59,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_60,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 60,
      .last_epoch_num = 60,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_61,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 61,
      .last_epoch_num = 61,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_62,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 62,
      .last_epoch_num = 62,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_63,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 63,
      .last_epoch_num = 63,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_64,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 64,
      .last_epoch_num = 64,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_65,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 65,
      .last_epoch_num = 65,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_66,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 66,
      .last_epoch_num = 66,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_67,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 67,
      .last_epoch_num = 67,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_68,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 68,
      .last_epoch_num = 68,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_69,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 69,
      .last_epoch_num = 69,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_70,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 70,
      .last_epoch_num = 70,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_71,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 71,
      .last_epoch_num = 71,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_72,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 72,
      .last_epoch_num = 72,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_73,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 73,
      .last_epoch_num = 73,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_74,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 74,
      .last_epoch_num = 74,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_75,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 75,
      .last_epoch_num = 75,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_76,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 76,
      .last_epoch_num = 76,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_77,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 77,
      .last_epoch_num = 77,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_78,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 78,
      .last_epoch_num = 78,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_79,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 79,
      .last_epoch_num = 79,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_80,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 80,
      .last_epoch_num = 80,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_81,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 81,
      .last_epoch_num = 81,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_82,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 82,
      .last_epoch_num = 82,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_83,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 83,
      .last_epoch_num = 83,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_84,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 84,
      .last_epoch_num = 84,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_85,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 85,
      .last_epoch_num = 85,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_86,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 86,
      .last_epoch_num = 86,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_87,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 87,
      .last_epoch_num = 87,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_88,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 88,
      .last_epoch_num = 88,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_89,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 89,
      .last_epoch_num = 89,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_90,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 90,
      .last_epoch_num = 90,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_91,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 91,
      .last_epoch_num = 91,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_92,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 92,
      .last_epoch_num = 92,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_93,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 93,
      .last_epoch_num = 93,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_94,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 94,
      .last_epoch_num = 94,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_95,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 95,
      .last_epoch_num = 95,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_96,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 96,
      .last_epoch_num = 96,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_97,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 97,
      .last_epoch_num = 97,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_98,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 98,
      .last_epoch_num = 98,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_99,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 99,
      .last_epoch_num = 99,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_100,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 100,
      .last_epoch_num = 100,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_101,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 101,
      .last_epoch_num = 101,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_102,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 102,
      .last_epoch_num = 102,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_103,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 103,
      .last_epoch_num = 103,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_104,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 104,
      .last_epoch_num = 104,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_105,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 105,
      .last_epoch_num = 105,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_106,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 106,
      .last_epoch_num = 106,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_107,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 107,
      .last_epoch_num = 107,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_108,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 108,
      .last_epoch_num = 108,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_109,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 109,
      .last_epoch_num = 109,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_110,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 110,
      .last_epoch_num = 110,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_111,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 111,
      .last_epoch_num = 111,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_112,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 112,
      .last_epoch_num = 112,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_113,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 113,
      .last_epoch_num = 113,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_114,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 114,
      .last_epoch_num = 114,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_115,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 115,
      .last_epoch_num = 115,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_116,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 116,
      .last_epoch_num = 116,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_117,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 117,
      .last_epoch_num = 117,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_118,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 118,
      .last_epoch_num = 118,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_119,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 119,
      .last_epoch_num = 119,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_120,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 120,
      .last_epoch_num = 120,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_121,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 121,
      .last_epoch_num = 121,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_122,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 122,
      .last_epoch_num = 122,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_123,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 123,
      .last_epoch_num = 123,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_124,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 124,
      .last_epoch_num = 124,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_125,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 125,
      .last_epoch_num = 125,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_126,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 126,
      .last_epoch_num = 126,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_127,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 127,
      .last_epoch_num = 127,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_128,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 128,
      .last_epoch_num = 128,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_129,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_129),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 129,
      .last_epoch_num = 130,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_131,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 131,
      .last_epoch_num = 131,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_132,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_132),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 132,
      .last_epoch_num = 134,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_128_128_3[] = { 1, 128, 3, 128 };
  static const uint32_t buff_info__mem_shape_F_1_128_128_3[] = { 1, 128, 128, 3 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_24_3_5_5[] = { 24, 5, 5, 3 };
  static const uint32_t buff_info__mem_shape_L_24_3_5_5[] = { 24, 5, 5, 3 };
  static const uint32_t buff_info__shape_24[] = { 1, 1, 24, 1 };
  static const uint32_t buff_info__mem_shape_U_24[] = { 24 };
  static const uint32_t buff_info__shape_24_24_1_1[] = { 24, 1, 1, 24 };
  static const uint32_t buff_info__mem_shape_F_24_24_1_1[] = { 24, 24, 1, 1 };
  static const uint32_t buff_info__shape_8[] = { 1, 1, 8, 1 };
  static const uint32_t buff_info__mem_shape_U_8[] = { 8 };
  static const uint32_t buff_info__shape_1[] = { 1, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_U_1[] = { 1 };
  static const uint32_t buff_info__shape_28_24_1_1[] = { 28, 1, 1, 24 };
  static const uint32_t buff_info__mem_shape_F_28_24_1_1[] = { 28, 24, 1, 1 };
  static const uint32_t buff_info__shape_32_28_1_1[] = { 32, 1, 1, 28 };
  static const uint32_t buff_info__mem_shape_F_32_28_1_1[] = { 32, 28, 1, 1 };
  static const uint32_t buff_info__shape_36_32_1_1[] = { 36, 1, 1, 32 };
  static const uint32_t buff_info__mem_shape_F_36_32_1_1[] = { 36, 32, 1, 1 };
  static const uint32_t buff_info__shape_36[] = { 1, 1, 36, 1 };
  static const uint32_t buff_info__mem_shape_U_36[] = { 36 };
  static const uint32_t buff_info__shape_42_36_1_1[] = { 42, 1, 1, 36 };
  static const uint32_t buff_info__mem_shape_F_42_36_1_1[] = { 42, 36, 1, 1 };
  static const uint32_t buff_info__shape_42[] = { 1, 1, 42, 1 };
  static const uint32_t buff_info__mem_shape_U_42[] = { 42 };
  static const uint32_t buff_info__shape_48_42_1_1[] = { 48, 1, 1, 42 };
  static const uint32_t buff_info__mem_shape_F_48_42_1_1[] = { 48, 42, 1, 1 };
  static const uint32_t buff_info__shape_48[] = { 1, 1, 48, 1 };
  static const uint32_t buff_info__mem_shape_U_48[] = { 48 };
  static const uint32_t buff_info__shape_56_48_1_1[] = { 56, 1, 1, 48 };
  static const uint32_t buff_info__mem_shape_F_56_48_1_1[] = { 56, 48, 1, 1 };
  static const uint32_t buff_info__shape_56[] = { 1, 1, 56, 1 };
  static const uint32_t buff_info__mem_shape_U_56[] = { 56 };
  static const uint32_t buff_info__shape_64_56_1_1[] = { 64, 1, 1, 56 };
  static const uint32_t buff_info__mem_shape_F_64_56_1_1[] = { 64, 56, 1, 1 };
  static const uint32_t buff_info__shape_64[] = { 1, 1, 64, 1 };
  static const uint32_t buff_info__mem_shape_U_64[] = { 64 };
  static const uint32_t buff_info__shape_72_64_1_1[] = { 72, 1, 1, 64 };
  static const uint32_t buff_info__mem_shape_F_72_64_1_1[] = { 72, 64, 1, 1 };
  static const uint32_t buff_info__shape_72[] = { 1, 1, 72, 1 };
  static const uint32_t buff_info__mem_shape_U_72[] = { 72 };
  static const uint32_t buff_info__shape_80_72_1_1[] = { 80, 1, 1, 72 };
  static const uint32_t buff_info__mem_shape_F_80_72_1_1[] = { 80, 72, 1, 1 };
  static const uint32_t buff_info__shape_80[] = { 1, 1, 80, 1 };
  static const uint32_t buff_info__mem_shape_U_80[] = { 80 };
  static const uint32_t buff_info__shape_88_80_1_1[] = { 88, 1, 1, 80 };
  static const uint32_t buff_info__mem_shape_F_88_80_1_1[] = { 88, 80, 1, 1 };
  static const uint32_t buff_info__shape_88[] = { 1, 1, 88, 1 };
  static const uint32_t buff_info__mem_shape_U_88[] = { 88 };
  static const uint32_t buff_info__shape_32_88_1_1[] = { 32, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_F_32_88_1_1[] = { 32, 88, 1, 1 };
  static const uint32_t buff_info__shape_32[] = { 1, 1, 32, 1 };
  static const uint32_t buff_info__mem_shape_U_32[] = { 32 };
  static const uint32_t buff_info__shape_96_88_1_1[] = { 96, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_F_96_88_1_1[] = { 96, 88, 1, 1 };
  static const uint32_t buff_info__shape_96[] = { 1, 1, 96, 1 };
  static const uint32_t buff_info__mem_shape_U_96[] = { 96 };
  static const uint32_t buff_info__shape_96_96_1_1[] = { 96, 1, 1, 96 };
  static const uint32_t buff_info__mem_shape_F_96_96_1_1[] = { 96, 96, 1, 1 };
  static const uint32_t buff_info__shape_6_96_1_1[] = { 6, 1, 1, 96 };
  static const uint32_t buff_info__mem_shape_F_6_96_1_1[] = { 6, 96, 1, 1 };
  static const uint32_t buff_info__shape_2_88_1_1[] = { 2, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_F_2_88_1_1[] = { 2, 88, 1, 1 };
  static const uint32_t buff_info__shape_24_1_1[] = { 1, 1, 1, 24 };
  static const uint32_t buff_info__mem_shape_F_24_1_1[] = { 24, 1, 1 };
  static const uint32_t buff_info__shape_28_1_1[] = { 1, 1, 1, 28 };
  static const uint32_t buff_info__mem_shape_F_28_1_1[] = { 28, 1, 1 };
  static const uint32_t buff_info__shape_32_1_1[] = { 1, 1, 1, 32 };
  static const uint32_t buff_info__mem_shape_F_32_1_1[] = { 32, 1, 1 };
  static const uint32_t buff_info__shape_36_1_1[] = { 1, 1, 1, 36 };
  static const uint32_t buff_info__mem_shape_F_36_1_1[] = { 36, 1, 1 };
  static const uint32_t buff_info__shape_42_1_1[] = { 1, 1, 1, 42 };
  static const uint32_t buff_info__mem_shape_F_42_1_1[] = { 42, 1, 1 };
  static const uint32_t buff_info__shape_48_1_1[] = { 1, 1, 1, 48 };
  static const uint32_t buff_info__mem_shape_F_48_1_1[] = { 48, 1, 1 };
  static const uint32_t buff_info__shape_56_1_1[] = { 1, 1, 1, 56 };
  static const uint32_t buff_info__mem_shape_F_56_1_1[] = { 56, 1, 1 };
  static const uint32_t buff_info__shape_64_1_1[] = { 1, 1, 1, 64 };
  static const uint32_t buff_info__mem_shape_F_64_1_1[] = { 64, 1, 1 };
  static const uint32_t buff_info__shape_72_1_1[] = { 1, 1, 1, 72 };
  static const uint32_t buff_info__mem_shape_F_72_1_1[] = { 72, 1, 1 };
  static const uint32_t buff_info__shape_80_1_1[] = { 1, 1, 1, 80 };
  static const uint32_t buff_info__mem_shape_F_80_1_1[] = { 80, 1, 1 };
  static const uint32_t buff_info__shape_88_1_1[] = { 1, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_F_88_1_1[] = { 88, 1, 1 };
  static const uint32_t buff_info__shape_96_1_1[] = { 1, 1, 1, 96 };
  static const uint32_t buff_info__mem_shape_F_96_1_1[] = { 96, 1, 1 };
  static const uint32_t buff_info__shape_6_1_1[] = { 1, 1, 1, 6 };
  static const uint32_t buff_info__mem_shape_F_6_1_1[] = { 6, 1, 1 };
  static const uint32_t buff_info__shape_2_1_1[] = { 1, 1, 1, 2 };
  static const uint32_t buff_info__mem_shape_F_2_1_1[] = { 2, 1, 1 };
  static const uint32_t buff_info__shape_24_2_3_3[] = { 24, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_24_2_3_3[] = { 24, 3, 3, 2 };
  static const uint32_t buff_info__shape_28_2_3_3[] = { 28, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_28_2_3_3[] = { 28, 3, 3, 2 };
  static const uint32_t buff_info__shape_32_2_3_3[] = { 32, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_32_2_3_3[] = { 32, 3, 3, 2 };
  static const uint32_t buff_info__shape_36_2_3_3[] = { 36, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_36_2_3_3[] = { 36, 3, 3, 2 };
  static const uint32_t buff_info__shape_42_2_3_3[] = { 42, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_42_2_3_3[] = { 42, 3, 3, 2 };
  static const uint32_t buff_info__shape_48_2_3_3[] = { 48, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_48_2_3_3[] = { 48, 3, 3, 2 };
  static const uint32_t buff_info__shape_56_2_3_3[] = { 56, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_56_2_3_3[] = { 56, 3, 3, 2 };
  static const uint32_t buff_info__shape_64_2_3_3[] = { 64, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_64_2_3_3[] = { 64, 3, 3, 2 };
  static const uint32_t buff_info__shape_72_2_3_3[] = { 72, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_72_2_3_3[] = { 72, 3, 3, 2 };
  static const uint32_t buff_info__shape_80_2_3_3[] = { 80, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_80_2_3_3[] = { 80, 3, 3, 2 };
  static const uint32_t buff_info__shape_88_2_3_3[] = { 88, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_88_2_3_3[] = { 88, 3, 3, 2 };
  static const uint32_t buff_info__shape_96_2_3_3[] = { 96, 3, 3, 2 };
  static const uint32_t buff_info__mem_shape_L_96_2_3_3[] = { 96, 3, 3, 2 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_0_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_128_3,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_128_3,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Conv2D_2_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 332160,
      .offset_end = 339360,
      .offset_limit = 339424,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_24_3_5_5,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_3_5_5,
    },
    {
      .name = "Conv2D_2_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440560,
      .offset_end = 440656,
      .offset_limit = 440720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_24,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24,
    },
    {
      .name = "Conv2D_5_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 418928,
      .offset_end = 421232,
      .offset_limit = 421296,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_24_24_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_24_1_1,
    },
    {
      .name = "Pad_9_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440752,
      .offset_end = 440816,
      .offset_limit = 440880,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_9_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441504,
      .offset_end = 441508,
      .offset_limit = 441576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_12_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 413648,
      .offset_end = 416336,
      .offset_limit = 416400,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_28_24_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_28_24_1_1,
    },
    {
      .name = "Pad_17_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440944,
      .offset_end = 441008,
      .offset_limit = 441072,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_17_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441552,
      .offset_end = 441556,
      .offset_limit = 441624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_20_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 403584,
      .offset_end = 407168,
      .offset_limit = 407232,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_F_32_28_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_28_1_1,
    },
    {
      .name = "Pad_24_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440880,
      .offset_end = 440944,
      .offset_limit = 441008,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_24_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441536,
      .offset_end = 441540,
      .offset_limit = 441608,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_27_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 390336,
      .offset_end = 394944,
      .offset_limit = 395008,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_F_36_32_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_36_32_1_1,
    },
    {
      .name = "Conv2D_27_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 439472,
      .offset_end = 439616,
      .offset_limit = 439680,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_36,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_36,
    },
    {
      .name = "Pad_31_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440816,
      .offset_end = 440880,
      .offset_limit = 440944,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_31_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441520,
      .offset_end = 441524,
      .offset_limit = 441592,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_34_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 373344,
      .offset_end = 379392,
      .offset_limit = 379456,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_F_42_36_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_42_36_1_1,
    },
    {
      .name = "Conv2D_34_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 439120,
      .offset_end = 439288,
      .offset_limit = 439352,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_42,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_42,
    },
    {
      .name = "Pad_39_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441328,
      .offset_end = 441392,
      .offset_limit = 441456,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_39_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441648,
      .offset_end = 441652,
      .offset_limit = 441720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_42_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 324096,
      .offset_end = 332160,
      .offset_limit = 332224,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_F_48_42_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_48_42_1_1,
    },
    {
      .name = "Conv2D_42_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 438736,
      .offset_end = 438928,
      .offset_limit = 438992,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_48,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_48,
    },
    {
      .name = "Pad_46_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441264,
      .offset_end = 441328,
      .offset_limit = 441392,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_46_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441632,
      .offset_end = 441636,
      .offset_limit = 441704,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_49_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 313344,
      .offset_end = 324096,
      .offset_limit = 324160,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_F_56_48_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_56_48_1_1,
    },
    {
      .name = "Conv2D_49_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 438288,
      .offset_end = 438512,
      .offset_limit = 438576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_56,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_56,
    },
    {
      .name = "Pad_53_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441200,
      .offset_end = 441264,
      .offset_limit = 441328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_53_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441616,
      .offset_end = 441620,
      .offset_limit = 441688,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_56_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 287744,
      .offset_end = 302080,
      .offset_limit = 302144,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_F_64_56_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_56_1_1,
    },
    {
      .name = "Conv2D_56_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 437776,
      .offset_end = 438032,
      .offset_limit = 438096,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Pad_60_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441136,
      .offset_end = 441200,
      .offset_limit = 441264,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_60_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441600,
      .offset_end = 441604,
      .offset_limit = 441672,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_63_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 269312,
      .offset_end = 287744,
      .offset_limit = 287808,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_72_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_72_64_1_1,
    },
    {
      .name = "Conv2D_63_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 437200,
      .offset_end = 437488,
      .offset_limit = 437552,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_72,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_72,
    },
    {
      .name = "Pad_67_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441072,
      .offset_end = 441136,
      .offset_limit = 441200,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_67_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441584,
      .offset_end = 441588,
      .offset_limit = 441656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_70_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 246272,
      .offset_end = 269312,
      .offset_limit = 269376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_F_80_72_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_80_72_1_1,
    },
    {
      .name = "Conv2D_70_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 436560,
      .offset_end = 436880,
      .offset_limit = 436944,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_80,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_80,
    },
    {
      .name = "Pad_74_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441008,
      .offset_end = 441072,
      .offset_limit = 441136,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_74_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441568,
      .offset_end = 441572,
      .offset_limit = 441640,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_77_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 218112,
      .offset_end = 246272,
      .offset_limit = 246336,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_F_88_80_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_88_80_1_1,
    },
    {
      .name = "Conv2D_77_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 435856,
      .offset_end = 436208,
      .offset_limit = 436272,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_88,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_88,
    },
    {
      .name = "Conv2D_80_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 302080,
      .offset_end = 313344,
      .offset_limit = 313408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_F_32_88_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_88_1_1,
    },
    {
      .name = "Conv2D_80_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440016,
      .offset_end = 440144,
      .offset_limit = 440208,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Pad_86_pads",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441392,
      .offset_end = 441456,
      .offset_limit = 441520,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "Pad_86_constant_value",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441664,
      .offset_end = 441668,
      .offset_limit = 441736,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_89_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 184320,
      .offset_end = 218112,
      .offset_limit = 218176,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_F_96_88_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_88_1_1,
    },
    {
      .name = "Conv2D_89_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 433936,
      .offset_end = 434320,
      .offset_limit = 434384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_96,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96,
    },
    {
      .name = "Conv2D_93_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 0,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
    },
    {
      .name = "Conv2D_93_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 432016,
      .offset_end = 432400,
      .offset_limit = 432464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_96,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96,
    },
    {
      .name = "Conv2D_97_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 36864,
      .offset_end = 73728,
      .offset_limit = 73792,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
    },
    {
      .name = "Conv2D_97_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 432400,
      .offset_end = 432784,
      .offset_limit = 432848,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_96,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96,
    },
    {
      .name = "Conv2D_101_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 73728,
      .offset_end = 110592,
      .offset_limit = 110656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
    },
    {
      .name = "Conv2D_101_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 432784,
      .offset_end = 433168,
      .offset_limit = 433232,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_96,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96,
    },
    {
      .name = "Conv2D_105_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 110592,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
    },
    {
      .name = "Conv2D_105_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 433168,
      .offset_end = 433552,
      .offset_limit = 433616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_96,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96,
    },
    {
      .name = "Conv2D_108_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 147456,
      .offset_end = 184320,
      .offset_limit = 184384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
    },
    {
      .name = "Conv2D_108_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 433552,
      .offset_end = 433936,
      .offset_limit = 434000,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_96,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96,
    },
    {
      .name = "Conv2D_114_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 423536,
      .offset_end = 425840,
      .offset_limit = 425904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_6_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_6_96_1_1,
    },
    {
      .name = "Conv2D_118_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 431312,
      .offset_end = 432016,
      .offset_limit = 432080,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_F_2_88_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_2_88_1_1,
    },
    {
      .name = "Conv2D_4_bias_copy1",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440368,
      .offset_end = 440464,
      .offset_limit = 440528,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_24_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_1_1,
    },
    {
      .name = "Conv2D_5_bias_copy3",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440464,
      .offset_end = 440560,
      .offset_limit = 440624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_24_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_1_1,
    },
    {
      .name = "Conv2D_11_bias_copy5",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440656,
      .offset_end = 440752,
      .offset_limit = 440816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_24_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_1_1,
    },
    {
      .name = "Conv2D_12_bias_copy7",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440144,
      .offset_end = 440256,
      .offset_limit = 440320,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_F_28_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_28_1_1,
    },
    {
      .name = "Conv2D_19_bias_copy9",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 440256,
      .offset_end = 440368,
      .offset_limit = 440432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_F_28_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_28_1_1,
    },
    {
      .name = "Conv2D_20_bias_copy11",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 439760,
      .offset_end = 439888,
      .offset_limit = 439952,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_F_32_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_1_1,
    },
    {
      .name = "Conv2D_26_bias_copy13",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 439888,
      .offset_end = 440016,
      .offset_limit = 440080,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_F_32_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_1_1,
    },
    {
      .name = "Conv2D_33_bias_copy15",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 439616,
      .offset_end = 439760,
      .offset_limit = 439824,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_F_36_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_36_1_1,
    },
    {
      .name = "Conv2D_41_bias_copy17",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 439296,
      .offset_end = 439464,
      .offset_limit = 439528,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_F_42_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_42_1_1,
    },
    {
      .name = "Conv2D_48_bias_copy19",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 438928,
      .offset_end = 439120,
      .offset_limit = 439184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_F_48_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_48_1_1,
    },
    {
      .name = "Conv2D_55_bias_copy21",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 438512,
      .offset_end = 438736,
      .offset_limit = 438800,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_F_56_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_56_1_1,
    },
    {
      .name = "Conv2D_62_bias_copy23",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 438032,
      .offset_end = 438288,
      .offset_limit = 438352,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_64_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_1_1,
    },
    {
      .name = "Conv2D_69_bias_copy25",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 437488,
      .offset_end = 437776,
      .offset_limit = 437840,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_F_72_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_72_1_1,
    },
    {
      .name = "Conv2D_76_bias_copy27",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 436880,
      .offset_end = 437200,
      .offset_limit = 437264,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_F_80_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_80_1_1,
    },
    {
      .name = "Conv2D_88_bias_copy29",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 436208,
      .offset_end = 436560,
      .offset_limit = 436624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_F_88_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_88_1_1,
    },
    {
      .name = "Conv2D_92_bias_copy31",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 434320,
      .offset_end = 434704,
      .offset_limit = 434768,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_1_1,
    },
    {
      .name = "Conv2D_96_bias_copy33",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 434704,
      .offset_end = 435088,
      .offset_limit = 435152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_1_1,
    },
    {
      .name = "Conv2D_100_bias_copy35",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 435088,
      .offset_end = 435472,
      .offset_limit = 435536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_1_1,
    },
    {
      .name = "Conv2D_104_bias_copy37",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 435472,
      .offset_end = 435856,
      .offset_limit = 435920,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_1_1,
    },
    {
      .name = "Conv2D_114_bias_copy39",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441456,
      .offset_end = 441480,
      .offset_limit = 441544,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_F_6_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_6_1_1,
    },
    {
      .name = "Conv2D_118_bias_copy41",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 441488,
      .offset_end = 441496,
      .offset_limit = 441560,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_F_2_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_2_1_1,
    },
    {
      .name = "Conv2D_4_weights_inflated_43",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 427856,
      .offset_end = 429584,
      .offset_limit = 429648,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_24_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_2_3_3,
    },
    {
      .name = "Conv2D_11_weights_inflated_45",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 429584,
      .offset_end = 431312,
      .offset_limit = 431376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_24_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_24_2_3_3,
    },
    {
      .name = "Conv2D_19_weights_inflated_47",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 425840,
      .offset_end = 427856,
      .offset_limit = 427920,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_28_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_28_2_3_3,
    },
    {
      .name = "Conv2D_26_weights_inflated_49",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 421232,
      .offset_end = 423536,
      .offset_limit = 423600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_32_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_2_3_3,
    },
    {
      .name = "Conv2D_33_weights_inflated_51",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 416336,
      .offset_end = 418928,
      .offset_limit = 418992,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_36_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_36_2_3_3,
    },
    {
      .name = "Conv2D_41_weights_inflated_53",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 410624,
      .offset_end = 413648,
      .offset_limit = 413712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_42_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_42_2_3_3,
    },
    {
      .name = "Conv2D_48_weights_inflated_55",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 407168,
      .offset_end = 410624,
      .offset_limit = 410688,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_48_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_48_2_3_3,
    },
    {
      .name = "Conv2D_55_weights_inflated_57",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 399552,
      .offset_end = 403584,
      .offset_limit = 403648,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_56_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_56_2_3_3,
    },
    {
      .name = "Conv2D_62_weights_inflated_59",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 394944,
      .offset_end = 399552,
      .offset_limit = 399616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_64_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_2_3_3,
    },
    {
      .name = "Conv2D_69_weights_inflated_61",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 385152,
      .offset_end = 390336,
      .offset_limit = 390400,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_72_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_72_2_3_3,
    },
    {
      .name = "Conv2D_76_weights_inflated_63",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 379392,
      .offset_end = 385152,
      .offset_limit = 385216,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_80_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_80_2_3_3,
    },
    {
      .name = "Conv2D_88_weights_inflated_65",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 367008,
      .offset_end = 373344,
      .offset_limit = 373408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_88_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_88_2_3_3,
    },
    {
      .name = "Conv2D_92_weights_inflated_67",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 339360,
      .offset_end = 346272,
      .offset_limit = 346336,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_96_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_2_3_3,
    },
    {
      .name = "Conv2D_96_weights_inflated_69",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 346272,
      .offset_end = 353184,
      .offset_limit = 353248,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_96_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_2_3_3,
    },
    {
      .name = "Conv2D_100_weights_inflated_71",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 353184,
      .offset_end = 360096,
      .offset_limit = 360160,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_96_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_2_3_3,
    },
    {
      .name = "Conv2D_104_weights_inflated_73",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 360096,
      .offset_end = 367008,
      .offset_limit = 367072,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_96_2_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_96_2_3_3,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_896_1[] = { 1, 896, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_1_896_1[] = { 1, 896, 1 };
  static const uint32_t buff_info__shape_1_896_16[] = { 1, 896, 16, 1 };
  static const uint32_t buff_info__mem_shape_F_1_896_16[] = { 1, 896, 16 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Transpose_123_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 385024,
      .offset_end = 388608,
      .offset_limit = 388672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_896_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_896_1,
    },
    {
      .name = "Transpose_113_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 133,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_896_16,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_896_16,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_Default(void)
{
  static const uint32_t buff_info__shape_1_3_128_128[] = { 1, 128, 128, 3 };
  static const uint32_t buff_info__mem_shape_L_1_3_128_128[] = { 1, 128, 128, 3 };
  static const uint32_t buff_info__shape_1_24_64_64[] = { 1, 64, 64, 24 };
  static const uint32_t buff_info__mem_shape_L_1_24_64_64[] = { 1, 64, 64, 24 };
  static const uint32_t buff_info__shape_1_64_24_64[] = { 1, 24, 64, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_24_64[] = { 1, 24, 64, 64 };
  static const uint32_t buff_info__shape_1_64_28_64[] = { 1, 28, 64, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_28_64[] = { 1, 28, 64, 64 };
  static const uint32_t buff_info__shape_1_28_64_64[] = { 1, 64, 64, 28 };
  static const uint32_t buff_info__mem_shape_L_1_28_64_64[] = { 1, 64, 64, 28 };
  static const uint32_t buff_info__shape_1_28_32_32[] = { 1, 32, 32, 28 };
  static const uint32_t buff_info__mem_shape_L_1_28_32_32[] = { 1, 32, 32, 28 };
  static const uint32_t buff_info__shape_1_32_28_32[] = { 1, 28, 32, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_28_32[] = { 1, 28, 32, 32 };
  static const uint32_t buff_info__shape_1_32_32_32[] = { 1, 32, 32, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_32_32[] = { 1, 32, 32, 32 };
  static const uint32_t buff_info__shape_1_32_36_32[] = { 1, 36, 32, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_36_32[] = { 1, 36, 32, 32 };
  static const uint32_t buff_info__shape_1_36_32_32[] = { 1, 32, 32, 36 };
  static const uint32_t buff_info__mem_shape_L_1_36_32_32[] = { 1, 32, 32, 36 };
  static const uint32_t buff_info__shape_1_32_42_32[] = { 1, 42, 32, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_42_32[] = { 1, 42, 32, 32 };
  static const uint32_t buff_info__shape_1_42_32_32[] = { 1, 32, 32, 42 };
  static const uint32_t buff_info__mem_shape_L_1_42_32_32[] = { 1, 32, 32, 42 };
  static const uint32_t buff_info__shape_1_42_16_16[] = { 1, 16, 16, 42 };
  static const uint32_t buff_info__mem_shape_L_1_42_16_16[] = { 1, 16, 16, 42 };
  static const uint32_t buff_info__shape_1_16_42_16[] = { 1, 42, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_42_16[] = { 1, 42, 16, 16 };
  static const uint32_t buff_info__shape_1_16_48_16[] = { 1, 48, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_48_16[] = { 1, 48, 16, 16 };
  static const uint32_t buff_info__shape_1_48_16_16[] = { 1, 16, 16, 48 };
  static const uint32_t buff_info__mem_shape_L_1_48_16_16[] = { 1, 16, 16, 48 };
  static const uint32_t buff_info__shape_1_16_56_16[] = { 1, 56, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_56_16[] = { 1, 56, 16, 16 };
  static const uint32_t buff_info__shape_1_56_16_16[] = { 1, 16, 16, 56 };
  static const uint32_t buff_info__mem_shape_L_1_56_16_16[] = { 1, 16, 16, 56 };
  static const uint32_t buff_info__shape_1_16_64_16[] = { 1, 64, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_64_16[] = { 1, 64, 16, 16 };
  static const uint32_t buff_info__shape_1_64_16_16[] = { 1, 16, 16, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_16_16[] = { 1, 16, 16, 64 };
  static const uint32_t buff_info__shape_1_16_72_16[] = { 1, 72, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_72_16[] = { 1, 72, 16, 16 };
  static const uint32_t buff_info__shape_1_72_16_16[] = { 1, 16, 16, 72 };
  static const uint32_t buff_info__mem_shape_L_1_72_16_16[] = { 1, 16, 16, 72 };
  static const uint32_t buff_info__shape_1_16_80_16[] = { 1, 80, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_80_16[] = { 1, 80, 16, 16 };
  static const uint32_t buff_info__shape_1_80_16_16[] = { 1, 16, 16, 80 };
  static const uint32_t buff_info__mem_shape_L_1_80_16_16[] = { 1, 16, 16, 80 };
  static const uint32_t buff_info__shape_1_16_88_16[] = { 1, 88, 16, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_88_16[] = { 1, 88, 16, 16 };
  static const uint32_t buff_info__shape_1_88_16_16[] = { 1, 16, 16, 88 };
  static const uint32_t buff_info__mem_shape_L_1_88_16_16[] = { 1, 16, 16, 88 };
  static const uint32_t buff_info__shape_1_88_8_8[] = { 1, 8, 8, 88 };
  static const uint32_t buff_info__mem_shape_L_1_88_8_8[] = { 1, 8, 8, 88 };
  static const uint32_t buff_info__shape_1_2_16_16[] = { 1, 16, 16, 2 };
  static const uint32_t buff_info__mem_shape_L_1_2_16_16[] = { 1, 16, 16, 2 };
  static const uint32_t buff_info__shape_1_32_16_16[] = { 1, 16, 16, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_16_16[] = { 1, 16, 16, 32 };
  static const uint32_t buff_info__shape_1_8_88_8[] = { 1, 88, 8, 8 };
  static const uint32_t buff_info__mem_shape_L_1_8_88_8[] = { 1, 88, 8, 8 };
  static const uint32_t buff_info__shape_1_16_16_32[] = { 1, 16, 32, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_16_32[] = { 1, 16, 16, 32 };
  static const uint32_t buff_info__shape_1_512_16_1[] = { 1, 16, 1, 512 };
  static const uint32_t buff_info__mem_shape_F_1_512_16_1[] = { 1, 512, 16, 1 };
  static const uint32_t buff_info__shape_1_8_96_8[] = { 1, 96, 8, 8 };
  static const uint32_t buff_info__mem_shape_L_1_8_96_8[] = { 1, 96, 8, 8 };
  static const uint32_t buff_info__shape_1_16_16_2[] = { 1, 16, 2, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_16_2[] = { 1, 16, 16, 2 };
  static const uint32_t buff_info__shape_1_1_512[] = { 1, 1, 512, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_512[] = { 1, 1, 512 };
  static const uint32_t buff_info__shape_1_96_8_8[] = { 1, 8, 8, 96 };
  static const uint32_t buff_info__mem_shape_L_1_96_8_8[] = { 1, 8, 8, 96 };
  static const uint32_t buff_info__shape_1_6_8_8[] = { 1, 8, 8, 6 };
  static const uint32_t buff_info__mem_shape_L_1_6_8_8[] = { 1, 8, 8, 6 };
  static const uint32_t buff_info__shape_1_8_8_96[] = { 1, 8, 96, 8 };
  static const uint32_t buff_info__mem_shape_F_1_8_8_96[] = { 1, 8, 8, 96 };
  static const uint32_t buff_info__shape_1_384_16_1[] = { 1, 16, 1, 384 };
  static const uint32_t buff_info__mem_shape_F_1_384_16_1[] = { 1, 384, 16, 1 };
  static const uint32_t buff_info__shape_1_8_8_6[] = { 1, 8, 6, 8 };
  static const uint32_t buff_info__mem_shape_F_1_8_8_6[] = { 1, 8, 8, 6 };
  static const uint32_t buff_info__shape_1_1_384[] = { 1, 1, 384, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_384[] = { 1, 1, 384 };
  static const uint32_t buff_info__mem_shape_L_1_512_16_1[] = { 1, 16, 1, 512 };
  static const uint32_t buff_info__shape_1_16_1_512[] = { 1, 1, 512, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_1_512[] = { 1, 16, 1, 512 };
  static const uint32_t buff_info__shape_1_16_512[] = { 1, 16, 512, 1 };
  static const uint32_t buff_info__mem_shape_F_1_16_512[] = { 1, 16, 512 };
  static const uint32_t buff_info__mem_shape_L_1_384_16_1[] = { 1, 16, 1, 384 };
  static const uint32_t buff_info__shape_1_16_1_384[] = { 1, 1, 384, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_1_384[] = { 1, 16, 1, 384 };
  static const uint32_t buff_info__shape_1_16_384[] = { 1, 16, 384, 1 };
  static const uint32_t buff_info__mem_shape_F_1_16_384[] = { 1, 16, 384 };
  static const uint32_t buff_info__shape_1_1_896[] = { 1, 1, 896, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_896[] = { 1, 1, 896 };
  static const uint32_t buff_info__shape_1_16_896[] = { 1, 16, 896, 1 };
  static const uint32_t buff_info__mem_shape_F_1_16_896[] = { 1, 16, 896 };
  static const uint32_t buff_info__shape_1_16_896_1[] = { 1, 896, 1, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_896_1[] = { 1, 16, 896, 1 };
  static const uint32_t buff_info__mem_shape_L_1_16_896_1[] = { 1, 896, 1, 16 };
  static const uint32_t buff_info__shape_1_896_1_16[] = { 1, 1, 16, 896 };
  static const uint32_t buff_info__mem_shape_F_1_896_1_16[] = { 1, 896, 1, 16 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Transpose_1_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_128_128,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_128_128,
    },
    {
      .name = "Conv2D_2_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Relu_3_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Conv2D_4_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 4,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Conv2D_4_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 5,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Conv2D_5_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Conv2D_5_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 7,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Add_6_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Relu_7_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Transpose_8_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 10,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_24_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_24_64,
    },
    {
      .name = "Conv2D_11_out_0_in",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Pad_9_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 458752,
      .offset_limit = 458816,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_28_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_28_64,
    },
    {
      .name = "Conv2D_11_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
    },
    {
      .name = "Transpose_10_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 458752,
      .offset_end = 917504,
      .offset_limit = 917568,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
    },
    {
      .name = "Conv2D_12_out_0_in",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 458752,
      .offset_limit = 458816,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 15,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
    },
    {
      .name = "Conv2D_12_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 917504,
      .offset_end = 1376256,
      .offset_limit = 1376320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
    },
    {
      .name = "Add_13_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 458752,
      .offset_limit = 458816,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 17,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
    },
    {
      .name = "Relu_14_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 458752,
      .offset_limit = 458816,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 18,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
    },
    {
      .name = "MaxPool_15_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_32_32,
    },
    {
      .name = "Conv2D_19_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 229376,
      .offset_limit = 229440,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 20,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_32_32,
    },
    {
      .name = "Transpose_16_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 229376,
      .offset_end = 344064,
      .offset_limit = 344128,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_28_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_28_32,
    },
    {
      .name = "Conv2D_19_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_28_32_32,
    },
    {
      .name = "Pad_17_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 131072,
      .offset_limit = 131136,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Conv2D_20_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 262144,
      .offset_limit = 262208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 24,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Transpose_18_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 262144,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 25,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Conv2D_20_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 131072,
      .offset_limit = 131136,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 26,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Add_21_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 262144,
      .offset_limit = 262208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 27,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Relu_22_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 262144,
      .offset_limit = 262208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 28,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Transpose_23_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 262144,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 29,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Conv2D_26_out_0_in",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 131072,
      .offset_limit = 131136,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 30,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Pad_24_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 31,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_36_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_36_32,
    },
    {
      .name = "Conv2D_26_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 294912,
      .offset_end = 425984,
      .offset_limit = 426048,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 32,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Transpose_25_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 294912,
      .offset_limit = 294976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
    },
    {
      .name = "Conv2D_27_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 34,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
    },
    {
      .name = "Add_28_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 294912,
      .offset_end = 442368,
      .offset_limit = 442432,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 35,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
    },
    {
      .name = "Relu_29_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 294912,
      .offset_end = 442368,
      .offset_limit = 442432,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 36,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
    },
    {
      .name = "Transpose_30_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 37,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_36_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_36_32,
    },
    {
      .name = "Conv2D_33_out_0_in",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 147456,
      .offset_end = 294912,
      .offset_limit = 294976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 38,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
    },
    {
      .name = "Pad_31_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 39,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_42_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_42_32,
    },
    {
      .name = "Conv2D_33_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 40,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
    },
    {
      .name = "Transpose_32_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 172032,
      .offset_end = 344064,
      .offset_limit = 344128,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 41,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_32_32,
    },
    {
      .name = "Conv2D_34_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 42,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_32_32,
    },
    {
      .name = "Add_35_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 43,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_32_32,
    },
    {
      .name = "Relu_36_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 44,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_32_32,
    },
    {
      .name = "MaxPool_37_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 43008,
      .offset_limit = 43072,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 45,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_16_16,
    },
    {
      .name = "Conv2D_41_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 43008,
      .offset_end = 86016,
      .offset_limit = 86080,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_16_16,
    },
    {
      .name = "Transpose_38_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 86016,
      .offset_end = 129024,
      .offset_limit = 129088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 47,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_42_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_42_16,
    },
    {
      .name = "Conv2D_41_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 129024,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 48,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_42_16_16,
    },
    {
      .name = "Pad_39_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 49152,
      .offset_limit = 49216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 49,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_48_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_48_16,
    },
    {
      .name = "Conv2D_42_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 49152,
      .offset_end = 98304,
      .offset_limit = 98368,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 50,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
    },
    {
      .name = "Transpose_40_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 98304,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
    },
    {
      .name = "Add_43_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 49152,
      .offset_limit = 49216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
    },
    {
      .name = "Relu_44_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 49152,
      .offset_limit = 49216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
    },
    {
      .name = "Transpose_45_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 106496,
      .offset_limit = 106560,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 54,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_48_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_48_16,
    },
    {
      .name = "Conv2D_48_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 106496,
      .offset_end = 155648,
      .offset_limit = 155712,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 55,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
    },
    {
      .name = "Pad_46_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 56,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_56_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_56_16,
    },
    {
      .name = "Conv2D_48_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 155648,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 57,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
    },
    {
      .name = "Transpose_47_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 58,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
    },
    {
      .name = "Conv2D_49_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 59,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
    },
    {
      .name = "Add_50_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 60,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
    },
    {
      .name = "Relu_51_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 172032,
      .offset_limit = 172096,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 61,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
    },
    {
      .name = "Transpose_52_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 172032,
      .offset_end = 229376,
      .offset_limit = 229440,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 62,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_56_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_56_16,
    },
    {
      .name = "Conv2D_55_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 229376,
      .offset_end = 286720,
      .offset_limit = 286784,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 63,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
    },
    {
      .name = "Pad_53_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 65536,
      .offset_limit = 65600,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 64,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_64_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_64_16,
    },
    {
      .name = "Conv2D_55_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 188416,
      .offset_limit = 188480,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 65,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
    },
    {
      .name = "Transpose_54_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 65536,
      .offset_end = 131072,
      .offset_limit = 131136,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 66,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
    },
    {
      .name = "Conv2D_56_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 65536,
      .offset_limit = 65600,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 67,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
    },
    {
      .name = "Add_57_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 68,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
    },
    {
      .name = "Relu_58_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
    },
    {
      .name = "Transpose_59_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 196608,
      .offset_end = 262144,
      .offset_limit = 262208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_64_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_64_16,
    },
    {
      .name = "Conv2D_62_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 262144,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 71,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
    },
    {
      .name = "Pad_60_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 73728,
      .offset_limit = 73792,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_72_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_72_16,
    },
    {
      .name = "Conv2D_62_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 212992,
      .offset_limit = 213056,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 73,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
    },
    {
      .name = "Transpose_61_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 73728,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 74,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
    },
    {
      .name = "Conv2D_63_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 73728,
      .offset_limit = 73792,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 75,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
    },
    {
      .name = "Add_64_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 221184,
      .offset_limit = 221248,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 76,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
    },
    {
      .name = "Relu_65_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 221184,
      .offset_limit = 221248,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 77,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
    },
    {
      .name = "Transpose_66_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 221184,
      .offset_end = 294912,
      .offset_limit = 294976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 78,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_72_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_72_16,
    },
    {
      .name = "Conv2D_69_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 294912,
      .offset_end = 368640,
      .offset_limit = 368704,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 79,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
    },
    {
      .name = "Pad_67_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 81920,
      .offset_limit = 81984,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 80,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_16,
    },
    {
      .name = "Conv2D_69_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 368640,
      .offset_end = 442368,
      .offset_limit = 442432,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 81,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
    },
    {
      .name = "Transpose_68_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 81920,
      .offset_end = 163840,
      .offset_limit = 163904,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 82,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
    },
    {
      .name = "Conv2D_70_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 81920,
      .offset_limit = 81984,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 83,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
    },
    {
      .name = "Add_71_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 163840,
      .offset_end = 245760,
      .offset_limit = 245824,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 84,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
    },
    {
      .name = "Relu_72_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 163840,
      .offset_end = 245760,
      .offset_limit = 245824,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 85,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
    },
    {
      .name = "Transpose_73_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 245760,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 86,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_16,
    },
    {
      .name = "Conv2D_76_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 87,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
    },
    {
      .name = "Pad_74_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 90112,
      .offset_limit = 90176,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 88,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_88_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_88_16,
    },
    {
      .name = "Conv2D_76_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 180224,
      .offset_end = 262144,
      .offset_limit = 262208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 89,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
    },
    {
      .name = "Transpose_75_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 90112,
      .offset_end = 180224,
      .offset_limit = 180288,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 90,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
    },
    {
      .name = "Conv2D_77_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 90112,
      .offset_limit = 90176,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 91,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
    },
    {
      .name = "Add_78_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 180224,
      .offset_end = 270336,
      .offset_limit = 270400,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 92,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
    },
    {
      .name = "Relu_79_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 180224,
      .offset_end = 270336,
      .offset_limit = 270400,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 93,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
    },
    {
      .name = "MaxPool_84_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 325632,
      .offset_limit = 325696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 94,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_8_8,
    },
    {
      .name = "Conv2D_118_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 370688,
      .offset_end = 372736,
      .offset_limit = 372800,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 95,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_1_2_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_16_16,
    },
    {
      .name = "Conv2D_88_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 325632,
      .offset_end = 348160,
      .offset_limit = 348224,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 96,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_8_8,
    },
    {
      .name = "Conv2D_80_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 270336,
      .offset_end = 303104,
      .offset_limit = 303168,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 97,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_16_16,
    },
    {
      .name = "Transpose_85_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 348160,
      .offset_end = 370688,
      .offset_limit = 370752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 98,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_1_8_88_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_8_88_8,
    },
    {
      .name = "Conv2D_118_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 403456,
      .offset_limit = 403520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 99,
      .batch = 2,
      .mem_shape = buff_info__mem_shape_L_1_2_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_16_16,
    },
    {
      .name = "Conv2D_88_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 370688,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 100,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_88_8_8,
    },
    {
      .name = "Transpose_81_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 270336,
      .offset_end = 303104,
      .offset_limit = 303168,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 101,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_16_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_16_32,
    },
    {
      .name = "Reshape_82_out_0_inserted_out247",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 270336,
      .offset_end = 303104,
      .offset_limit = 303168,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 101,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_16_1,
    },
    {
      .name = "Pad_86_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 101,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_1_8_96_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_8_96_8,
    },
    {
      .name = "Transpose_119_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 403456,
      .offset_limit = 403520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 102,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_16_2,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_16_2,
    },
    {
      .name = "Transpose_121_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 403456,
      .offset_limit = 403520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 102,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_512,
    },
    {
      .name = "Conv2D_89_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 102,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Transpose_87_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 103,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Add_90_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 104,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Relu_91_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 105,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_92_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 106,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_92_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 107,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_93_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 108,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Add_94_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 109,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Relu_95_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 110,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_96_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 111,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_96_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 112,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_97_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 113,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Add_98_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 114,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Relu_99_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 115,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_100_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 116,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_100_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 117,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_101_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 118,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Add_102_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 119,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Relu_103_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 352256,
      .offset_end = 376832,
      .offset_limit = 376896,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 120,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_104_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 121,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_104_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 327680,
      .offset_end = 352256,
      .offset_limit = 352320,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 122,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_105_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 327680,
      .offset_limit = 327744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 123,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Add_106_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 376832,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 124,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Relu_107_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 376832,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 125,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_114_out_0_in",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 360448,
      .offset_end = 361984,
      .offset_limit = 362048,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 126,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_1_6_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_6_8_8,
    },
    {
      .name = "Conv2D_108_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 335872,
      .offset_end = 360448,
      .offset_limit = 360512,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 127,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
    },
    {
      .name = "Conv2D_114_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 403456,
      .offset_end = 404992,
      .offset_limit = 405056,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 128,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_1_6_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_6_8_8,
    },
    {
      .name = "Transpose_109_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 335872,
      .offset_end = 360448,
      .offset_limit = 360512,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_8_8_96,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_8_8_96,
    },
    {
      .name = "Reshape_110_out_0_inserted_out250",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 335872,
      .offset_end = 360448,
      .offset_limit = 360512,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_384_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_384_16_1,
    },
    {
      .name = "Transpose_115_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 403456,
      .offset_end = 404992,
      .offset_limit = 405056,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_8_8_6,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_8_8_6,
    },
    {
      .name = "Transpose_117_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 403456,
      .offset_end = 404992,
      .offset_limit = 405056,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_384,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_384,
    },
    {
      .name = "Reshape_82_out_0_inserted_out247_inserted_out249",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 335872,
      .offset_limit = 335936,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_16_1,
    },
    {
      .name = "Transpose_83_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 335872,
      .offset_limit = 335936,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_1_512,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_512,
    },
    {
      .name = "Transpose_83_out_0_cp_in_7",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 303104,
      .offset_end = 335872,
      .offset_limit = 335936,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_512,
    },
    {
      .name = "Reshape_110_out_0_inserted_out250_inserted_out252",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 360448,
      .offset_end = 385024,
      .offset_limit = 385088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_L_1_384_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_384_16_1,
    },
    {
      .name = "Transpose_111_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 360448,
      .offset_end = 385024,
      .offset_limit = 385088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_1_384,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_384,
    },
    {
      .name = "Transpose_111_out_0_cp_in_8",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 360448,
      .offset_end = 385024,
      .offset_limit = 385088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_384,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_384,
    },
    {
      .name = "Concat_122_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 385024,
      .offset_end = 388608,
      .offset_limit = 388672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_896,
    },
    {
      .name = "Concat_112_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 131,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896,
    },
    {
      .name = "Concat_112_out_0_inserted_out253",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 132,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_896_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896_1,
    },
    {
      .name = "Concat_112_out_0_inserted_out253_inserted_out255",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 133,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_896_1,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896_1,
    },
    {
      .name = "Transpose_113_out_0_cp_in_9",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 133,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_896_1_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_896_1_16,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

