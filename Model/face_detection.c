/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    face_detection.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "27f5d5bcb9ca9522b73a54d7aec841664ee448cd"
 * GIT_BRANCH      "STAI-2.1"
 * GIT_DESCRIPTION "atonn-v1.1.0-31-g27f5d5bcb"
 *
 * Command Line options:
 * --onnx-input = "C:/Users/pele/.stm32cubemx/network_output/face_detection_front_128_integer_quant_OE_3_2_0.onnx"
 * --out-dir-prefix = "C:/Users/pele/AppData/Local/Temp/mxAI_workspace7415836699225001002309535702380999/neural_art__face_detection/"
 * --network-name = "face_detection"
 * --all-buffers-info = true
 * --mvei = true
 * --load-mdesc-file = "C:/ST/STEdgeAI/2.1/Utilities/configs/stm32n6"
 * --load-mpool-file = "C:/ST/STEdgeAI/2.1/scripts/N6_scripts/my_mpools/stm32n6_n6-allmems-O3"
 * --cache-maintenance = true
 * --enable-virtual-mem-pools = true
 * --native-float = true
 * --json-quant-file = "C:/Users/pele/.stm32cubemx/network_output/face_detection_front_128_integer_quant_OE_3_2_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --enable-epoch-controller = true
 * --output-info-file = "c_info"
 * --Oauto-sched = true
 *
 * auto* option expanded into:
 *   alt-scheduler = false
 */

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"
#include "ecloader.h"

#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 0 || LL_ATON_VERSION_DEV != 31
#  warning "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 7 is ? */
/* index=7 file postfix=xSPI1 name=hyperRAM offset=0x90000000  absolute_mode size=33554424 READ_WRITE THROUGHPUT=MID LATENCY=HIGH byte width=2 freq ratio=5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=380 write_power=340 use4initializers=YES score=82  */
/* global pool 8 is 150.92 KB */
/* index=8 file postfix=xSPI2 name=octoFlash offset=0x72000000  absolute_mode size=117440504 READ_ONLY THROUGHPUT=MID LATENCY=HIGH byte width=1 freq ratio=6 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=110 write_power=400 use4initializers=YES score=50  */
/* global pool 1 is 384.00 KB */
/* index=1 file postfix=AXISRAM5 name=npuRAM5 offset=0x342e0000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 2 is 192.00 KB */
/* index=2 file postfix=AXISRAM4 name=npuRAM4 offset=0x34270000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 3 is ? */
/* index=3 file postfix=AXISRAM3 name=npuRAM3 offset=0x34200000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 0 is ? */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458744 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */
/* global pool 11 is 2.25 MB */
/* index=11 file postfix=AXISRAM2_AXISRAM3_AXISRAM4_AXISRAM5_AXISRAM6 name=cpuRAM2_npuRAM3_npuRAM4_npuRAM5_npuRAM6 offset=0x34100000  absolute_mode size=2883576 vpool READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=16.201 use4initializers=NO score=85  */
/* global pool 4 is ? */
/* index=4 file postfix=AXISRAM2 name=cpuRAM2 offset=0x34100000  absolute_mode size=1048576 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=17.324 write_power=15.321 use4initializers=NO score=84  */
/* global pool 5 is ? */
/* index=5 file postfix=AXISRAM1 name=cpuRAM1 offset=0x34064000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=16.616 write_power=14.522 use4initializers=NO score=84  */
/* global pool 6 is ? */
/* index=6 file postfix=AXIFLEXMEM name=flexMEM offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=9.381 write_power=8.569 use4initializers=NO score=84  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_face_detection(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_face_detection(uint32_t num)
{
  { 
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_face_detection(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_face_detection(uint32_t num)
{
  { 
    return NULL;
  }
}

#include "face_detection_ecblobs.h"

/* scheduling epoch=0    nodes=152 ------------------------------------------------------------------- */

// Epoch Controller Blob (name='_ec_blob_1') micro instructions needed


/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_2(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=Quantize_2 */
  Quantizelinear_sw_info quantizelinear1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 128,
    .general.input.dim.tensor_w = 128,
    .general.input.dim.tensor_c = 3,
    .general.input.dim.num_elem = 49152,
    .general.input.stride.b = 196608,
    .general.input.stride.h = 1536,
    .general.input.stride.w = 12,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x72000000UL + 154480))) /* Equivalent hex address = 0x72025b70UL */,
    .os.format.is_signed = 1,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x72000000UL + 154528))) /* Equivalent hex address = 0x72025ba0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 128,
    .general.output.dim.tensor_w = 128,
    .general.output.dim.tensor_c = 3,
    .general.output.dim.num_elem = 49152,
    .general.output.stride.b = 49152,
    .general.output.stride.h = 384,
    .general.output.stride.w = 3,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) /* Equivalent hex address = 0x34310000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Quantize_2 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 245760))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) /* Equivalent hex address = 0x34310000UL */, 49152);

}


// Epoch Controller Blob (name='_ec_blob_3') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_3') start function
static void _ec_blob_cache_start_func_3(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 294912))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 196608))) /* Equivalent hex address = 0x34310000UL */, 98304);

};


/* scheduling epoch=7    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_7(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 98304);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_19 */
  static const uint32_t Transpose_19_tensor_shape_in_7_shape_0[] = { 1, 64, 64, 24 };
  static const LL_LIB_TensorShape_TypeDef Transpose_19_tensor_shape_in_7[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 196608,
      .offset_end = 294912,
      .offset_limit = 294976,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_19_tensor_shape_in_7_shape_0,
      .batch = 24,
    }
  };

  static const uint32_t Transpose_19_tensor_axes_offsets_in_7_0[] = { 98304, 1536, 24, 1 };
  static const uint32_t* Transpose_19_tensor_axes_offsets_in_7[] = {
    Transpose_19_tensor_axes_offsets_in_7_0
  };

  static const uint32_t Transpose_19_tensor_shape_out_7_shape_0[] = { 1, 24, 64, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_19_tensor_shape_out_7[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 98304,
      .offset_limit = 98368,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_19_tensor_shape_out_7_shape_0,
      .batch = 64,
    }
  };

  static const uint32_t Transpose_19_tensor_axes_offsets_out_7_0[] = { 98304, 4096, 64, 1 };
  static const uint32_t* Transpose_19_tensor_axes_offsets_out_7[] = {
    Transpose_19_tensor_axes_offsets_out_7_0
  };

  static const uint8_t Transpose_19_perm_to_use_array_in_7[] = { 0, 3, 2, 1 };
  static const uint8_t Transpose_19_target_pos_array_in_7[] = { 0, 3, 2, 1 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_19_tensor_shape_in_7[0], Transpose_19_tensor_axes_offsets_in_7[0], &Transpose_19_tensor_shape_out_7[0], Transpose_19_tensor_axes_offsets_out_7[0], Transpose_19_target_pos_array_in_7, Transpose_19_perm_to_use_array_in_7, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 98304);

}


// Epoch Controller Blob (name='_ec_blob_8') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_8') start function
static void _ec_blob_cache_start_func_8(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */, 114688);

};


/* scheduling epoch=10   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_10(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 344064))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */, 114688);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_21 */
  static const uint32_t Transpose_21_tensor_shape_in_10_shape_0[] = { 1, 64, 28, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_21_tensor_shape_in_10[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 229376,
      .offset_limit = 229440,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_21_tensor_shape_in_10_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_21_tensor_axes_offsets_in_10_0[] = { 114688, 1792, 64, 1 };
  static const uint32_t* Transpose_21_tensor_axes_offsets_in_10[] = {
    Transpose_21_tensor_axes_offsets_in_10_0
  };

  static const uint32_t Transpose_21_tensor_shape_out_10_shape_0[] = { 1, 28, 64, 64 };
  static const LL_LIB_TensorShape_TypeDef Transpose_21_tensor_shape_out_10[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 229376,
      .offset_end = 344064,
      .offset_limit = 344128,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_21_tensor_shape_out_10_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_21_tensor_axes_offsets_out_10_0[] = { 114688, 4096, 64, 1 };
  static const uint32_t* Transpose_21_tensor_axes_offsets_out_10[] = {
    Transpose_21_tensor_axes_offsets_out_10_0
  };

  static const uint8_t Transpose_21_perm_to_use_array_in_10[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_21_target_pos_array_in_10[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_21_tensor_shape_in_10[0], Transpose_21_tensor_axes_offsets_in_10[0], &Transpose_21_tensor_shape_out_10[0], Transpose_21_tensor_axes_offsets_out_10[0], Transpose_21_target_pos_array_in_10, Transpose_21_perm_to_use_array_in_10, 1, 2);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 344064))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 229376))) /* Equivalent hex address = 0x34318000UL */, 114688);

}


// Epoch Controller Blob (name='_ec_blob_11') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_11') start function
static void _ec_blob_cache_start_func_11(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) /* Equivalent hex address = 0x342e8000UL */, 32768);

};


/* scheduling epoch=13   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_13(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */, 32768);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_43 */
  static const uint32_t Transpose_43_tensor_shape_in_13_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_43_tensor_shape_in_13[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 32768,
      .offset_end = 65536,
      .offset_limit = 65600,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_43_tensor_shape_in_13_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_43_tensor_axes_offsets_in_13_0[] = { 32768, 1024, 32, 1 };
  static const uint32_t* Transpose_43_tensor_axes_offsets_in_13[] = {
    Transpose_43_tensor_axes_offsets_in_13_0
  };

  static const uint32_t Transpose_43_tensor_shape_out_13_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_43_tensor_shape_out_13[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 65536,
      .offset_end = 98304,
      .offset_limit = 98368,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_43_tensor_shape_out_13_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_43_tensor_axes_offsets_out_13_0[] = { 32768, 1024, 32, 1 };
  static const uint32_t* Transpose_43_tensor_axes_offsets_out_13[] = {
    Transpose_43_tensor_axes_offsets_out_13_0
  };

  static const uint8_t Transpose_43_perm_to_use_array_in_13[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_43_target_pos_array_in_13[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_43_tensor_shape_in_13[0], Transpose_43_tensor_axes_offsets_in_13[0], &Transpose_43_tensor_shape_out_13[0], Transpose_43_tensor_axes_offsets_out_13[0], Transpose_43_target_pos_array_in_13, Transpose_43_perm_to_use_array_in_13, 4, 8);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */, 32768);

}


// Epoch Controller Blob (name='_ec_blob_14') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_14') start function
static void _ec_blob_cache_start_func_14(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) /* Equivalent hex address = 0x342e8000UL */, 32768);

};


/* scheduling epoch=15   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_15(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */, 32768);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_48 */
  static const uint32_t Transpose_48_tensor_shape_in_15_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_48_tensor_shape_in_15[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 32768,
      .offset_end = 65536,
      .offset_limit = 65600,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_48_tensor_shape_in_15_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_48_tensor_axes_offsets_in_15_0[] = { 32768, 1024, 32, 1 };
  static const uint32_t* Transpose_48_tensor_axes_offsets_in_15[] = {
    Transpose_48_tensor_axes_offsets_in_15_0
  };

  static const uint32_t Transpose_48_tensor_shape_out_15_shape_0[] = { 1, 32, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_48_tensor_shape_out_15[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 65536,
      .offset_end = 98304,
      .offset_limit = 98368,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_48_tensor_shape_out_15_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_48_tensor_axes_offsets_out_15_0[] = { 32768, 1024, 32, 1 };
  static const uint32_t* Transpose_48_tensor_axes_offsets_out_15[] = {
    Transpose_48_tensor_axes_offsets_out_15_0
  };

  static const uint8_t Transpose_48_perm_to_use_array_in_15[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_48_target_pos_array_in_15[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_48_tensor_shape_in_15[0], Transpose_48_tensor_axes_offsets_in_15[0], &Transpose_48_tensor_shape_out_15[0], Transpose_48_tensor_axes_offsets_out_15[0], Transpose_48_target_pos_array_in_15, Transpose_48_perm_to_use_array_in_15, 3, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 98304))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 65536))) /* Equivalent hex address = 0x342f0000UL */, 32768);

}


// Epoch Controller Blob (name='_ec_blob_16') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_16') start function
static void _ec_blob_cache_start_func_16(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 36864);

};


/* scheduling epoch=17   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_17(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) /* Equivalent hex address = 0x342e9000UL */, 36864);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_50 */
  static const uint32_t Transpose_50_tensor_shape_in_17_shape_0[] = { 1, 32, 36, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_50_tensor_shape_in_17[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 36864,
      .offset_limit = 36928,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_50_tensor_shape_in_17_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_50_tensor_axes_offsets_in_17_0[] = { 36864, 1152, 32, 1 };
  static const uint32_t* Transpose_50_tensor_axes_offsets_in_17[] = {
    Transpose_50_tensor_axes_offsets_in_17_0
  };

  static const uint32_t Transpose_50_tensor_shape_out_17_shape_0[] = { 1, 36, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_50_tensor_shape_out_17[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 36864,
      .offset_end = 73728,
      .offset_limit = 73792,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_50_tensor_shape_out_17_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_50_tensor_axes_offsets_out_17_0[] = { 36864, 1024, 32, 1 };
  static const uint32_t* Transpose_50_tensor_axes_offsets_out_17[] = {
    Transpose_50_tensor_axes_offsets_out_17_0
  };

  static const uint8_t Transpose_50_perm_to_use_array_in_17[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_50_target_pos_array_in_17[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_50_tensor_shape_in_17[0], Transpose_50_tensor_axes_offsets_in_17[0], &Transpose_50_tensor_shape_out_17[0], Transpose_50_tensor_axes_offsets_out_17[0], Transpose_50_target_pos_array_in_17, Transpose_50_perm_to_use_array_in_17, 2, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) /* Equivalent hex address = 0x342e9000UL */, 36864);

}


// Epoch Controller Blob (name='_ec_blob_18') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_18') start function
static void _ec_blob_cache_start_func_18(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 110592))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) /* Equivalent hex address = 0x342f2000UL */, 36864);

};


/* scheduling epoch=20   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_20(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 184320))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 36864);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_62 */
  static const uint32_t Transpose_62_tensor_shape_in_20_shape_0[] = { 1, 36, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_62_tensor_shape_in_20[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 73728,
      .offset_end = 110592,
      .offset_limit = 110656,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_62_tensor_shape_in_20_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_62_tensor_axes_offsets_in_20_0[] = { 36864, 1024, 32, 1 };
  static const uint32_t* Transpose_62_tensor_axes_offsets_in_20[] = {
    Transpose_62_tensor_axes_offsets_in_20_0
  };

  static const uint32_t Transpose_62_tensor_shape_out_20_shape_0[] = { 1, 32, 36, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_62_tensor_shape_out_20[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 184320,
      .offset_limit = 184384,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_62_tensor_shape_out_20_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_62_tensor_axes_offsets_out_20_0[] = { 36864, 1152, 32, 1 };
  static const uint32_t* Transpose_62_tensor_axes_offsets_out_20[] = {
    Transpose_62_tensor_axes_offsets_out_20_0
  };

  static const uint8_t Transpose_62_perm_to_use_array_in_20[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_62_target_pos_array_in_20[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_62_tensor_shape_in_20[0], Transpose_62_tensor_axes_offsets_in_20[0], &Transpose_62_tensor_shape_out_20[0], Transpose_62_tensor_axes_offsets_out_20[0], Transpose_62_target_pos_array_in_20, Transpose_62_perm_to_use_array_in_20, 9, 0);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 184320))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 147456))) /* Equivalent hex address = 0x34304000UL */, 36864);

}


// Epoch Controller Blob (name='_ec_blob_21') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_21') start function
static void _ec_blob_cache_start_func_21(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) /* Equivalent hex address = 0x342ea800UL */, 43008);

};


/* scheduling epoch=22   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_22(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) /* Equivalent hex address = 0x342f5000UL */, 43008);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_64 */
  static const uint32_t Transpose_64_tensor_shape_in_22_shape_0[] = { 1, 32, 42, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_64_tensor_shape_in_22[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 43008,
      .offset_end = 86016,
      .offset_limit = 86080,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_64_tensor_shape_in_22_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_64_tensor_axes_offsets_in_22_0[] = { 43008, 1344, 32, 1 };
  static const uint32_t* Transpose_64_tensor_axes_offsets_in_22[] = {
    Transpose_64_tensor_axes_offsets_in_22_0
  };

  static const uint32_t Transpose_64_tensor_shape_out_22_shape_0[] = { 1, 42, 32, 32 };
  static const LL_LIB_TensorShape_TypeDef Transpose_64_tensor_shape_out_22[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 86016,
      .offset_end = 129024,
      .offset_limit = 129088,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_64_tensor_shape_out_22_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_64_tensor_axes_offsets_out_22_0[] = { 43008, 1024, 32, 1 };
  static const uint32_t* Transpose_64_tensor_axes_offsets_out_22[] = {
    Transpose_64_tensor_axes_offsets_out_22_0
  };

  static const uint8_t Transpose_64_perm_to_use_array_in_22[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_64_target_pos_array_in_22[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_64_tensor_shape_in_22[0], Transpose_64_tensor_axes_offsets_in_22[0], &Transpose_64_tensor_shape_out_22[0], Transpose_64_tensor_axes_offsets_out_22[0], Transpose_64_target_pos_array_in_22, Transpose_64_perm_to_use_array_in_22, 8, 0);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 129024))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) /* Equivalent hex address = 0x342f5000UL */, 43008);

}


// Epoch Controller Blob (name='_ec_blob_23') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_23') start function
static void _ec_blob_cache_start_func_23(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 12288))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 12288))) /* Equivalent hex address = 0x342e3000UL */, 12288);

};


/* scheduling epoch=25   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_25(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) /* Equivalent hex address = 0x342e6000UL */, 12288);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_86 */
  static const uint32_t Transpose_86_tensor_shape_in_25_shape_0[] = { 1, 16, 48, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_86_tensor_shape_in_25[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 24576,
      .offset_limit = 24640,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_86_tensor_shape_in_25_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_86_tensor_axes_offsets_in_25_0[] = { 12288, 768, 16, 1 };
  static const uint32_t* Transpose_86_tensor_axes_offsets_in_25[] = {
    Transpose_86_tensor_axes_offsets_in_25_0
  };

  static const uint32_t Transpose_86_tensor_shape_out_25_shape_0[] = { 1, 48, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_86_tensor_shape_out_25[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 24576,
      .offset_end = 36864,
      .offset_limit = 36928,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_86_tensor_shape_out_25_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_86_tensor_axes_offsets_out_25_0[] = { 12288, 256, 16, 1 };
  static const uint32_t* Transpose_86_tensor_axes_offsets_out_25[] = {
    Transpose_86_tensor_axes_offsets_out_25_0
  };

  static const uint8_t Transpose_86_perm_to_use_array_in_25[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_86_target_pos_array_in_25[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_86_tensor_shape_in_25[0], Transpose_86_tensor_axes_offsets_in_25[0], &Transpose_86_tensor_shape_out_25[0], Transpose_86_tensor_axes_offsets_out_25[0], Transpose_86_target_pos_array_in_25, Transpose_86_perm_to_use_array_in_25, 3, 4);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) /* Equivalent hex address = 0x342e6000UL */, 12288);

}


// Epoch Controller Blob (name='_ec_blob_26') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_26') start function
static void _ec_blob_cache_start_func_26(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 12288))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 12288))) /* Equivalent hex address = 0x342e3000UL */, 12288);

};


/* scheduling epoch=27   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_27(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) /* Equivalent hex address = 0x342e6000UL */, 12288);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_91 */
  static const uint32_t Transpose_91_tensor_shape_in_27_shape_0[] = { 1, 48, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_91_tensor_shape_in_27[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 24576,
      .offset_limit = 24640,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_91_tensor_shape_in_27_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_91_tensor_axes_offsets_in_27_0[] = { 12288, 256, 16, 1 };
  static const uint32_t* Transpose_91_tensor_axes_offsets_in_27[] = {
    Transpose_91_tensor_axes_offsets_in_27_0
  };

  static const uint32_t Transpose_91_tensor_shape_out_27_shape_0[] = { 1, 16, 48, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_91_tensor_shape_out_27[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 24576,
      .offset_end = 36864,
      .offset_limit = 36928,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_91_tensor_shape_out_27_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_91_tensor_axes_offsets_out_27_0[] = { 12288, 768, 16, 1 };
  static const uint32_t* Transpose_91_tensor_axes_offsets_out_27[] = {
    Transpose_91_tensor_axes_offsets_out_27_0
  };

  static const uint8_t Transpose_91_perm_to_use_array_in_27[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_91_target_pos_array_in_27[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_91_tensor_shape_in_27[0], Transpose_91_tensor_axes_offsets_in_27[0], &Transpose_91_tensor_shape_out_27[0], Transpose_91_tensor_axes_offsets_out_27[0], Transpose_91_target_pos_array_in_27, Transpose_91_perm_to_use_array_in_27, 5, 6);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 24576))) /* Equivalent hex address = 0x342e6000UL */, 12288);

}


// Epoch Controller Blob (name='_ec_blob_28') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_28') start function
static void _ec_blob_cache_start_func_28(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 14336))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 14336);

};


/* scheduling epoch=29   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_29(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 14336))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 28672))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 14336))) /* Equivalent hex address = 0x342e3800UL */, 14336);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_93 */
  static const uint32_t Transpose_93_tensor_shape_in_29_shape_0[] = { 1, 16, 56, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_93_tensor_shape_in_29[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 14336,
      .offset_limit = 14400,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_93_tensor_shape_in_29_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_93_tensor_axes_offsets_in_29_0[] = { 14336, 896, 16, 1 };
  static const uint32_t* Transpose_93_tensor_axes_offsets_in_29[] = {
    Transpose_93_tensor_axes_offsets_in_29_0
  };

  static const uint32_t Transpose_93_tensor_shape_out_29_shape_0[] = { 1, 56, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_93_tensor_shape_out_29[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 14336,
      .offset_end = 28672,
      .offset_limit = 28736,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_93_tensor_shape_out_29_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_93_tensor_axes_offsets_out_29_0[] = { 14336, 256, 16, 1 };
  static const uint32_t* Transpose_93_tensor_axes_offsets_out_29[] = {
    Transpose_93_tensor_axes_offsets_out_29_0
  };

  static const uint8_t Transpose_93_perm_to_use_array_in_29[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_93_target_pos_array_in_29[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_93_tensor_shape_in_29[0], Transpose_93_tensor_axes_offsets_in_29[0], &Transpose_93_tensor_shape_out_29[0], Transpose_93_tensor_axes_offsets_out_29[0], Transpose_93_target_pos_array_in_29, Transpose_93_perm_to_use_array_in_29, 2, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 14336))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 28672))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 14336))) /* Equivalent hex address = 0x342e3800UL */, 14336);

}


// Epoch Controller Blob (name='_ec_blob_30') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_30') start function
static void _ec_blob_cache_start_func_30(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 28672))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 43008))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 28672))) /* Equivalent hex address = 0x342e7000UL */, 14336);

};


/* scheduling epoch=32   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_32(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 71680))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 14336);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_105 */
  static const uint32_t Transpose_105_tensor_shape_in_32_shape_0[] = { 1, 56, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_105_tensor_shape_in_32[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 28672,
      .offset_end = 43008,
      .offset_limit = 43072,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_105_tensor_shape_in_32_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_105_tensor_axes_offsets_in_32_0[] = { 14336, 256, 16, 1 };
  static const uint32_t* Transpose_105_tensor_axes_offsets_in_32[] = {
    Transpose_105_tensor_axes_offsets_in_32_0
  };

  static const uint32_t Transpose_105_tensor_shape_out_32_shape_0[] = { 1, 16, 56, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_105_tensor_shape_out_32[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 71680,
      .offset_limit = 71744,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_105_tensor_shape_out_32_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_105_tensor_axes_offsets_out_32_0[] = { 14336, 896, 16, 1 };
  static const uint32_t* Transpose_105_tensor_axes_offsets_out_32[] = {
    Transpose_105_tensor_axes_offsets_out_32_0
  };

  static const uint8_t Transpose_105_perm_to_use_array_in_32[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_105_target_pos_array_in_32[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_105_tensor_shape_in_32[0], Transpose_105_tensor_axes_offsets_in_32[0], &Transpose_105_tensor_shape_out_32[0], Transpose_105_tensor_axes_offsets_out_32[0], Transpose_105_target_pos_array_in_32, Transpose_105_perm_to_use_array_in_32, 2, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 71680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 14336);

}


// Epoch Controller Blob (name='_ec_blob_33') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_33') start function
static void _ec_blob_cache_start_func_33(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16384))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 16384);

};


/* scheduling epoch=34   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_34(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16384))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16384))) /* Equivalent hex address = 0x342e4000UL */, 16384);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_107 */
  static const uint32_t Transpose_107_tensor_shape_in_34_shape_0[] = { 1, 16, 64, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_107_tensor_shape_in_34[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 16384,
      .offset_limit = 16448,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_107_tensor_shape_in_34_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_107_tensor_axes_offsets_in_34_0[] = { 16384, 1024, 16, 1 };
  static const uint32_t* Transpose_107_tensor_axes_offsets_in_34[] = {
    Transpose_107_tensor_axes_offsets_in_34_0
  };

  static const uint32_t Transpose_107_tensor_shape_out_34_shape_0[] = { 1, 64, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_107_tensor_shape_out_34[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 16384,
      .offset_end = 32768,
      .offset_limit = 32832,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_107_tensor_shape_out_34_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_107_tensor_axes_offsets_out_34_0[] = { 16384, 256, 16, 1 };
  static const uint32_t* Transpose_107_tensor_axes_offsets_out_34[] = {
    Transpose_107_tensor_axes_offsets_out_34_0
  };

  static const uint8_t Transpose_107_perm_to_use_array_in_34[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_107_target_pos_array_in_34[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_107_tensor_shape_in_34[0], Transpose_107_tensor_axes_offsets_in_34[0], &Transpose_107_tensor_shape_out_34[0], Transpose_107_tensor_axes_offsets_out_34[0], Transpose_107_target_pos_array_in_34, Transpose_107_perm_to_use_array_in_34, 7, 8);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16384))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16384))) /* Equivalent hex address = 0x342e4000UL */, 16384);

}


// Epoch Controller Blob (name='_ec_blob_35') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_35') start function
static void _ec_blob_cache_start_func_35(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 49152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 32768))) /* Equivalent hex address = 0x342e8000UL */, 16384);

};


/* scheduling epoch=37   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_37(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 69632))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 69632))) /* Equivalent hex address = 0x342f1000UL */, 16384);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_119 */
  static const uint32_t Transpose_119_tensor_shape_in_37_shape_0[] = { 1, 64, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_119_tensor_shape_in_37[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 32768,
      .offset_end = 49152,
      .offset_limit = 49216,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_119_tensor_shape_in_37_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_119_tensor_axes_offsets_in_37_0[] = { 16384, 256, 16, 1 };
  static const uint32_t* Transpose_119_tensor_axes_offsets_in_37[] = {
    Transpose_119_tensor_axes_offsets_in_37_0
  };

  static const uint32_t Transpose_119_tensor_shape_out_37_shape_0[] = { 1, 16, 64, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_119_tensor_shape_out_37[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 69632,
      .offset_end = 86016,
      .offset_limit = 86080,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_119_tensor_shape_out_37_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_119_tensor_axes_offsets_out_37_0[] = { 16384, 1024, 16, 1 };
  static const uint32_t* Transpose_119_tensor_axes_offsets_out_37[] = {
    Transpose_119_tensor_axes_offsets_out_37_0
  };

  static const uint8_t Transpose_119_perm_to_use_array_in_37[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_119_target_pos_array_in_37[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_119_tensor_shape_in_37[0], Transpose_119_tensor_axes_offsets_in_37[0], &Transpose_119_tensor_shape_out_37[0], Transpose_119_tensor_axes_offsets_out_37[0], Transpose_119_target_pos_array_in_37, Transpose_119_perm_to_use_array_in_37, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 69632))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 86016))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 69632))) /* Equivalent hex address = 0x342f1000UL */, 16384);

}


// Epoch Controller Blob (name='_ec_blob_38') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_38') start function
static void _ec_blob_cache_start_func_38(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 18432))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 18432);

};


/* scheduling epoch=39   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_39(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 18432))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 18432))) /* Equivalent hex address = 0x342e4800UL */, 18432);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_121 */
  static const uint32_t Transpose_121_tensor_shape_in_39_shape_0[] = { 1, 16, 72, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_121_tensor_shape_in_39[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 18432,
      .offset_limit = 18496,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_121_tensor_shape_in_39_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_121_tensor_axes_offsets_in_39_0[] = { 18432, 1152, 16, 1 };
  static const uint32_t* Transpose_121_tensor_axes_offsets_in_39[] = {
    Transpose_121_tensor_axes_offsets_in_39_0
  };

  static const uint32_t Transpose_121_tensor_shape_out_39_shape_0[] = { 1, 72, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_121_tensor_shape_out_39[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 36864,
      .offset_limit = 36928,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_121_tensor_shape_out_39_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_121_tensor_axes_offsets_out_39_0[] = { 18432, 256, 16, 1 };
  static const uint32_t* Transpose_121_tensor_axes_offsets_out_39[] = {
    Transpose_121_tensor_axes_offsets_out_39_0
  };

  static const uint8_t Transpose_121_perm_to_use_array_in_39[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_121_target_pos_array_in_39[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_121_tensor_shape_in_39[0], Transpose_121_tensor_axes_offsets_in_39[0], &Transpose_121_tensor_shape_out_39[0], Transpose_121_tensor_axes_offsets_out_39[0], Transpose_121_target_pos_array_in_39, Transpose_121_perm_to_use_array_in_39, 7, 8);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 18432))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 18432))) /* Equivalent hex address = 0x342e4800UL */, 18432);

}


// Epoch Controller Blob (name='_ec_blob_40') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_40') start function
static void _ec_blob_cache_start_func_40(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 55296))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 36864))) /* Equivalent hex address = 0x342e9000UL */, 18432);

};


/* scheduling epoch=42   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_42(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 92160))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) /* Equivalent hex address = 0x342f2000UL */, 18432);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_133 */
  static const uint32_t Transpose_133_tensor_shape_in_42_shape_0[] = { 1, 72, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_133_tensor_shape_in_42[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 36864,
      .offset_end = 55296,
      .offset_limit = 55360,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_133_tensor_shape_in_42_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_133_tensor_axes_offsets_in_42_0[] = { 18432, 256, 16, 1 };
  static const uint32_t* Transpose_133_tensor_axes_offsets_in_42[] = {
    Transpose_133_tensor_axes_offsets_in_42_0
  };

  static const uint32_t Transpose_133_tensor_shape_out_42_shape_0[] = { 1, 16, 72, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_133_tensor_shape_out_42[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 73728,
      .offset_end = 92160,
      .offset_limit = 92224,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_133_tensor_shape_out_42_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_133_tensor_axes_offsets_out_42_0[] = { 18432, 1152, 16, 1 };
  static const uint32_t* Transpose_133_tensor_axes_offsets_out_42[] = {
    Transpose_133_tensor_axes_offsets_out_42_0
  };

  static const uint8_t Transpose_133_perm_to_use_array_in_42[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_133_target_pos_array_in_42[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_133_tensor_shape_in_42[0], Transpose_133_tensor_axes_offsets_in_42[0], &Transpose_133_tensor_shape_out_42[0], Transpose_133_tensor_axes_offsets_out_42[0], Transpose_133_target_pos_array_in_42, Transpose_133_perm_to_use_array_in_42, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 92160))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 73728))) /* Equivalent hex address = 0x342f2000UL */, 18432);

}


// Epoch Controller Blob (name='_ec_blob_43') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_43') start function
static void _ec_blob_cache_start_func_43(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 20480))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 20480);

};


/* scheduling epoch=44   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_44(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 20480))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 40960))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 20480))) /* Equivalent hex address = 0x342e5000UL */, 20480);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_135 */
  static const uint32_t Transpose_135_tensor_shape_in_44_shape_0[] = { 1, 16, 80, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_135_tensor_shape_in_44[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 20480,
      .offset_limit = 20544,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_135_tensor_shape_in_44_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_135_tensor_axes_offsets_in_44_0[] = { 20480, 1280, 16, 1 };
  static const uint32_t* Transpose_135_tensor_axes_offsets_in_44[] = {
    Transpose_135_tensor_axes_offsets_in_44_0
  };

  static const uint32_t Transpose_135_tensor_shape_out_44_shape_0[] = { 1, 80, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_135_tensor_shape_out_44[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 20480,
      .offset_end = 40960,
      .offset_limit = 41024,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_135_tensor_shape_out_44_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_135_tensor_axes_offsets_out_44_0[] = { 20480, 256, 16, 1 };
  static const uint32_t* Transpose_135_tensor_axes_offsets_out_44[] = {
    Transpose_135_tensor_axes_offsets_out_44_0
  };

  static const uint8_t Transpose_135_perm_to_use_array_in_44[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_135_target_pos_array_in_44[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_135_tensor_shape_in_44[0], Transpose_135_tensor_axes_offsets_in_44[0], &Transpose_135_tensor_shape_out_44[0], Transpose_135_tensor_axes_offsets_out_44[0], Transpose_135_target_pos_array_in_44, Transpose_135_perm_to_use_array_in_44, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 20480))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 40960))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 20480))) /* Equivalent hex address = 0x342e5000UL */, 20480);

}


// Epoch Controller Blob (name='_ec_blob_45') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_45') start function
static void _ec_blob_cache_start_func_45(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 40960))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 61440))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 40960))) /* Equivalent hex address = 0x342ea000UL */, 20480);

};


/* scheduling epoch=47   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_47(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) /* Equivalent hex address = 0x342f4000UL */, 20480);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_147 */
  static const uint32_t Transpose_147_tensor_shape_in_47_shape_0[] = { 1, 80, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_147_tensor_shape_in_47[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 40960,
      .offset_end = 61440,
      .offset_limit = 61504,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_147_tensor_shape_in_47_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_147_tensor_axes_offsets_in_47_0[] = { 20480, 256, 16, 1 };
  static const uint32_t* Transpose_147_tensor_axes_offsets_in_47[] = {
    Transpose_147_tensor_axes_offsets_in_47_0
  };

  static const uint32_t Transpose_147_tensor_shape_out_47_shape_0[] = { 1, 16, 80, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_147_tensor_shape_out_47[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 81920,
      .offset_end = 102400,
      .offset_limit = 102464,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_147_tensor_shape_out_47_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_147_tensor_axes_offsets_out_47_0[] = { 20480, 1280, 16, 1 };
  static const uint32_t* Transpose_147_tensor_axes_offsets_out_47[] = {
    Transpose_147_tensor_axes_offsets_out_47_0
  };

  static const uint8_t Transpose_147_perm_to_use_array_in_47[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_147_target_pos_array_in_47[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_147_tensor_shape_in_47[0], Transpose_147_tensor_axes_offsets_in_47[0], &Transpose_147_tensor_shape_out_47[0], Transpose_147_tensor_axes_offsets_out_47[0], Transpose_147_target_pos_array_in_47, Transpose_147_perm_to_use_array_in_47, 9, 0);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 81920))) /* Equivalent hex address = 0x342f4000UL */, 20480);

}


// Epoch Controller Blob (name='_ec_blob_48') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_48') start function
static void _ec_blob_cache_start_func_48(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 22528))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 22528);

};


/* scheduling epoch=49   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_49(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 22528))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 45056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 22528))) /* Equivalent hex address = 0x342e5800UL */, 22528);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_149 */
  static const uint32_t Transpose_149_tensor_shape_in_49_shape_0[] = { 1, 16, 88, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_149_tensor_shape_in_49[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 22528,
      .offset_limit = 22592,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_149_tensor_shape_in_49_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_149_tensor_axes_offsets_in_49_0[] = { 22528, 1408, 16, 1 };
  static const uint32_t* Transpose_149_tensor_axes_offsets_in_49[] = {
    Transpose_149_tensor_axes_offsets_in_49_0
  };

  static const uint32_t Transpose_149_tensor_shape_out_49_shape_0[] = { 1, 88, 16, 16 };
  static const LL_LIB_TensorShape_TypeDef Transpose_149_tensor_shape_out_49[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 22528,
      .offset_end = 45056,
      .offset_limit = 45120,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_149_tensor_shape_out_49_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_149_tensor_axes_offsets_out_49_0[] = { 22528, 256, 16, 1 };
  static const uint32_t* Transpose_149_tensor_axes_offsets_out_49[] = {
    Transpose_149_tensor_axes_offsets_out_49_0
  };

  static const uint8_t Transpose_149_perm_to_use_array_in_49[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_149_target_pos_array_in_49[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_149_tensor_shape_in_49[0], Transpose_149_tensor_axes_offsets_in_49[0], &Transpose_149_tensor_shape_out_49[0], Transpose_149_tensor_axes_offsets_out_49[0], Transpose_149_target_pos_array_in_49, Transpose_149_perm_to_use_array_in_49, 7, 8);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 22528))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 45056))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 22528))) /* Equivalent hex address = 0x342e5800UL */, 22528);

}


// Epoch Controller Blob (name='_ec_blob_50') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_50') start function
static void _ec_blob_cache_start_func_50(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 38912))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 45056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 38912))) /* Equivalent hex address = 0x342e9800UL */, 6144);

};


/* scheduling epoch=54   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_54(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 12288))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) /* Equivalent hex address = 0x342e1800UL */, 6144);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_181 */
  static const uint32_t Transpose_181_tensor_shape_in_54_shape_0[] = { 1, 8, 96, 8 };
  static const LL_LIB_TensorShape_TypeDef Transpose_181_tensor_shape_in_54[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 38912,
      .offset_end = 45056,
      .offset_limit = 45120,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_181_tensor_shape_in_54_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_181_tensor_axes_offsets_in_54_0[] = { 6144, 768, 8, 1 };
  static const uint32_t* Transpose_181_tensor_axes_offsets_in_54[] = {
    Transpose_181_tensor_axes_offsets_in_54_0
  };

  static const uint32_t Transpose_181_tensor_shape_out_54_shape_0[] = { 1, 96, 8, 8 };
  static const LL_LIB_TensorShape_TypeDef Transpose_181_tensor_shape_out_54[] = {
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 12288,
      .offset_limit = 12352,
      .ndims = 4,
      .nbits = 8,
      .shape = Transpose_181_tensor_shape_out_54_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_181_tensor_axes_offsets_out_54_0[] = { 6144, 64, 8, 1 };
  static const uint32_t* Transpose_181_tensor_axes_offsets_out_54[] = {
    Transpose_181_tensor_axes_offsets_out_54_0
  };

  static const uint8_t Transpose_181_perm_to_use_array_in_54[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_181_target_pos_array_in_54[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_181_tensor_shape_in_54[0], Transpose_181_tensor_axes_offsets_in_54[0], &Transpose_181_tensor_shape_out_54[0], Transpose_181_tensor_axes_offsets_out_54[0], Transpose_181_target_pos_array_in_54, Transpose_181_perm_to_use_array_in_54, 7, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 12288))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) /* Equivalent hex address = 0x342e1800UL */, 6144);

}


// Epoch Controller Blob (name='_ec_blob_55') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_55') start function
static void _ec_blob_cache_start_func_55(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 7040))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) /* Equivalent hex address = 0x342e1800UL */, 896);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 38912))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 53248))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 38912))) /* Equivalent hex address = 0x342e9800UL */, 14336);

};


/* scheduling epoch=71   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_71(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=Dequantize_258 */
  Dequantizelinear_sw_info dequantizelinear2_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 896,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 896,
    .general.input.stride.b = 896,
    .general.input.stride.h = 896,
    .general.input.stride.w = 1,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 6144))) /* Equivalent hex address = 0x342e1800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x72000000UL + 154496))) /* Equivalent hex address = 0x72025b80UL */,
    .is.format.is_signed = 1,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x72000000UL + 154544))) /* Equivalent hex address = 0x72025bb0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 896,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 896,
    .general.output.stride.b = 3584,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Dequantize_258 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear2_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 118272))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) /* Equivalent hex address = 0x342fc000UL */, 3584);

}


/* scheduling epoch=72   nodes=2   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_72(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 71680))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 14336);

  LL_ATON_LIB_UNUSED(epoch_block);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_236 */
  static const uint32_t Concat_236_tensor_info_in_72__shape_1_16_512[] = { 1, 16, 512, 1 };
  static const uint32_t Concat_236_tensor_info_in_72__mem_shape_F_1_16_512[] = { 1, 16, 512 };
  static const float Concat_236_tensor_info_in_72_Dequantize_177_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t Concat_236_tensor_info_in_72_Dequantize_177_out_0_quant_offset[] = { -57 };
  static const uint32_t Concat_236_tensor_info_in_72__shape_1_16_384[] = { 1, 16, 384, 1 };
  static const uint32_t Concat_236_tensor_info_in_72__mem_shape_F_1_16_384[] = { 1, 16, 384 };
  static const float Concat_236_tensor_info_in_72_Transpose_235_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t Concat_236_tensor_info_in_72_Transpose_235_out_0_quant_offset[] = { -57 };
  static const LL_Buffer_InfoTypeDef Concat_236_tensor_info_in_72[] = {
    {
      .name = "Dequantize_177_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 38912,
      .offset_end = 47104,
      .offset_limit = 47168,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 1,
      .mem_shape = Concat_236_tensor_info_in_72__mem_shape_F_1_16_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = Concat_236_tensor_info_in_72__shape_1_16_512,
      .per_channel = 0,
      .scale = Concat_236_tensor_info_in_72_Dequantize_177_out_0_quant_scale,
      .offset = Concat_236_tensor_info_in_72_Dequantize_177_out_0_quant_offset,
    },
    {
      .name = "Transpose_235_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 47104,
      .offset_end = 53248,
      .offset_limit = 53312,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 1,
      .mem_shape = Concat_236_tensor_info_in_72__mem_shape_F_1_16_384,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = Concat_236_tensor_info_in_72__shape_1_16_384,
      .per_channel = 0,
      .scale = Concat_236_tensor_info_in_72_Transpose_235_out_0_quant_scale,
      .offset = Concat_236_tensor_info_in_72_Transpose_235_out_0_quant_offset,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_236_tensor_info_out_72__shape_1_16_896[] = { 1, 16, 896, 1 };
  static const uint32_t Concat_236_tensor_info_out_72__mem_shape_F_1_16_896[] = { 1, 16, 896 };
  static const float Concat_236_tensor_info_out_72_Concat_236_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t Concat_236_tensor_info_out_72_Concat_236_out_0_quant_offset[] = { -57 };
  static const LL_Buffer_InfoTypeDef Concat_236_tensor_info_out_72[] = {
    {
      .name = "Concat_236_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 71680,
      .offset_limit = 71744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 1,
      .mem_shape = Concat_236_tensor_info_out_72__mem_shape_F_1_16_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = Concat_236_tensor_info_out_72__shape_1_16_896,
      .per_channel = 0,
      .scale = Concat_236_tensor_info_out_72_Concat_236_out_0_quant_scale,
      .offset = Concat_236_tensor_info_out_72_Concat_236_out_0_quant_offset,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_236_tensor_info_in_72, 2, Concat_236_tensor_info_out_72, 3, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 71680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 14336);

}


/* scheduling epoch=73   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_73(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=Dequantize_238 */
  Dequantizelinear_sw_info dequantizelinear3_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 16,
    .general.input.dim.tensor_w = 896,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 14336,
    .general.input.stride.b = 14336,
    .general.input.stride.h = 896,
    .general.input.stride.w = 1,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x72000000UL + 154464))) /* Equivalent hex address = 0x72025b60UL */,
    .is.format.is_signed = 1,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x72000000UL + 154512))) /* Equivalent hex address = 0x72025b90UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 16,
    .general.output.dim.tensor_w = 896,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 14336,
    .general.output.stride.b = 57344,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Dequantize_238 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear3_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 57344);

}


// Epoch Controller Blob (name='_ec_blob_74') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_74') start function
static void _ec_blob_cache_start_func_74(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 114688))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 57344))) /* Equivalent hex address = 0x342ee000UL */, 57344);

};


/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_face_detection(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = NULL,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_1),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 1,
      .last_epoch_num = 1,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_2,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_3,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_3),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 3,
      .last_epoch_num = 6,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_7,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 7,
      .last_epoch_num = 7,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_8,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_8),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 8,
      .last_epoch_num = 9,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_10,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 10,
      .last_epoch_num = 10,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_11,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_11),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 11,
      .last_epoch_num = 12,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_13,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 13,
      .last_epoch_num = 13,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_14,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_14),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 14,
      .last_epoch_num = 14,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_15,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 15,
      .last_epoch_num = 15,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_16,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_16),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 16,
      .last_epoch_num = 16,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_17,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 17,
      .last_epoch_num = 17,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_18,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_18),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 18,
      .last_epoch_num = 19,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_20,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 20,
      .last_epoch_num = 20,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_21,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_21),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 21,
      .last_epoch_num = 21,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_22,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 22,
      .last_epoch_num = 22,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_23,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_23),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 23,
      .last_epoch_num = 24,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_25,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 25,
      .last_epoch_num = 25,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_26,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_26),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 26,
      .last_epoch_num = 26,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_27,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 27,
      .last_epoch_num = 27,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_28,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_28),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 28,
      .last_epoch_num = 28,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_29,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 29,
      .last_epoch_num = 29,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_30,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_30),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 30,
      .last_epoch_num = 31,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_32,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 32,
      .last_epoch_num = 32,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_33,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_33),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 33,
      .last_epoch_num = 33,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_34,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 34,
      .last_epoch_num = 34,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_35,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_35),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 35,
      .last_epoch_num = 36,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_37,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 37,
      .last_epoch_num = 37,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_38,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_38),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 38,
      .last_epoch_num = 38,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_39,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 39,
      .last_epoch_num = 39,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_40,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_40),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 40,
      .last_epoch_num = 41,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_42,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 42,
      .last_epoch_num = 42,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_43,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_43),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 43,
      .last_epoch_num = 43,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_44,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 44,
      .last_epoch_num = 44,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_45,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_45),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 45,
      .last_epoch_num = 46,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_47,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 47,
      .last_epoch_num = 47,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_48,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_48),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 48,
      .last_epoch_num = 48,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_49,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 49,
      .last_epoch_num = 49,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_50,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_50),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 50,
      .last_epoch_num = 53,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_54,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 54,
      .last_epoch_num = 54,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_55,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_55),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 55,
      .last_epoch_num = 70,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_71,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 71,
      .last_epoch_num = 71,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_72,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 72,
      .last_epoch_num = 72,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_73,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 73,
      .last_epoch_num = 73,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_74,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_74),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 74,
      .last_epoch_num = 76,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_face_detection(void)
{
  static const uint32_t buff_info__shape_1_128_128_3[] = { 1, 128, 3, 128 };
  static const uint32_t buff_info__mem_shape_F_1_128_128_3[] = { 1, 128, 128, 3 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_1[] = { 1, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_U_1[] = { 1 };
  static const uint32_t buff_info__shape_24_24_1_1[] = { 24, 1, 1, 24 };
  static const uint32_t buff_info__mem_shape_F_24_24_1_1[] = { 24, 24, 1, 1 };
  static const float buff_info_Conv2D_12_weights_quant_scale[] = { 0.00904228538274765, 0.0114161232486367, 0.005754672922194, 0.00677212281152606, 0.0100361695513129, 0.00686325924471021, 0.00669686635956168, 0.00633745361119509, 0.0106507427990437, 0.00972703378647566, 0.00939628016203642, 0.00559821957722306, 0.00545299565419555, 0.00976435840129852, 0.0162336546927691, 0.00995632447302341, 0.0124349994584918, 0.0214011911302805, 0.0213031675666571, 0.00521175656467676, 0.00620874296873808, 0.0176218040287495, 0.00837681069970131, 0.0114746484905481 };
  static const int16_t buff_info_Conv2D_12_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_28_24_1_1[] = { 28, 1, 1, 24 };
  static const uint32_t buff_info__mem_shape_F_28_24_1_1[] = { 28, 24, 1, 1 };
  static const float buff_info_Conv2D_26_weights_quant_scale[] = { 0.00422372622415423, 0.00242803222499788, 0.00387830566614866, 0.00506196636706591, 0.00336782494559884, 0.00671580340713263, 0.00349321658723056, 0.00267423433251679, 0.00204029469750822, 0.0052708275616169, 0.012436012737453, 0.0102962618693709, 0.0025150291621685, 0.00616274727508426, 0.00576932681724429, 0.00556859048083425, 0.00375051400624216, 0.00353268557228148, 0.00701102195307612, 0.0105371158570051, 0.0046763950958848, 0.00409168098121881, 0.00243127369321883, 0.00517399935051799, 0.00305552873760462, 0.00244247424416244, 0.00214696093462408, 0.00148829817771912 };
  static const int16_t buff_info_Conv2D_26_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_28_1_3_3[] = { 28, 3, 3, 1 };
  static const uint32_t buff_info__mem_shape_F_28_1_3_3[] = { 28, 1, 3, 3 };
  static const float buff_info_Conv2D_34_weights_quant_scale[] = { 0.00452477252110839, 0.00352244102396071, 0.0122864712029696, 0.00527743296697736, 0.00869130715727806, 0.00641962699592113, 0.00816379766911268, 0.00686539616435766, 0.00807238277047873, 0.00417680572718382, 0.00252216728404164, 0.0056429011747241, 0.00719343638047576, 0.00500265508890152, 0.00458446005359292, 0.0103111360222101, 0.00549304485321045, 0.00581671856343746, 0.00302576646208763, 0.00448310468345881, 0.0065981512889266, 0.00719288177788258, 0.00590089056640863, 0.00503465626388788, 0.0112494435161352, 0.011131358332932, 0.0115682603791356, 0.00791849754750729 };
  static const int16_t buff_info_Conv2D_34_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_32_28_1_1[] = { 32, 1, 1, 28 };
  static const uint32_t buff_info__mem_shape_M_32_28_1_1[] = { 32, 2, 1, 1, 14 };
  static const float buff_info_Conv2D_37_weights_quant_scale[] = { 0.00461891666054726, 0.00279632164165378, 0.00451094517484307, 0.00550530897453427, 0.00550335086882114, 0.00529649062082171, 0.00606713630259037, 0.00240766815841198, 0.00358744524419308, 0.0032244878821075, 0.00630809832364321, 0.00504401139914989, 0.00216749263927341, 0.00310085527598858, 0.00523973349481821, 0.00202939170412719, 0.0112843094393611, 0.00362947350367904, 0.00423540268093348, 0.00383129180409014, 0.00311262835748494, 0.00274777505546808, 0.00404915073886514, 0.00287709315307438, 0.00675716903060675, 0.00665804324671626, 0.00448980601504445, 0.00206821411848068, 0.00201709754765034, 0.00197567720897496, 0.0017446530982852, 0.00302221300080419 };
  static const int16_t buff_info_Conv2D_37_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_36_32_1_1[] = { 36, 1, 1, 32 };
  static const uint32_t buff_info__mem_shape_M_36_32_1_1[] = { 36, 2, 1, 1, 16 };
  static const float buff_info_Conv2D_55_weights_quant_scale[] = { 0.00329319690354168, 0.00243039219640195, 0.00283216941170394, 0.00125334342010319, 0.00288937357254326, 0.00394571479409933, 0.00268874457105994, 0.00184251985047013, 0.00236369529739022, 0.00348264910280704, 0.002617331687361, 0.00562334526330233, 0.00113522610627115, 0.00203827349469066, 0.00452209822833538, 0.00331919733434916, 0.00204972736537457, 0.00149295467417687, 0.00277094985358417, 0.00416336813941598, 0.00153344694990665, 0.00287588150240481, 0.00178562244400382, 0.00172963214572519, 0.00254942709580064, 0.00371272768825293, 0.00462393974885345, 0.00229111895896494, 0.00186176027636975, 0.00137103872839361, 0.001722646295093, 0.00135691917967051, 0.00153564696665853, 0.00161255011335015, 0.00154509616550058, 0.00172874412965029 };
  static const int16_t buff_info_Conv2D_55_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_36_1_3_3[] = { 36, 3, 3, 1 };
  static const uint32_t buff_info__mem_shape_F_36_1_3_3[] = { 36, 1, 3, 3 };
  static const float buff_info_Conv2D_66_weights_quant_scale[] = { 0.00426155794411898, 0.00504121836274862, 0.00690061552450061, 0.00805486086755991, 0.00649490393698215, 0.00604269886389375, 0.0112585732713342, 0.00559803796932101, 0.00635279295966029, 0.00710161123424768, 0.00544502772390842, 0.0104174930602312, 0.00871135760098696, 0.0116828279569745, 0.00979751814156771, 0.0046976744197309, 0.00701416283845901, 0.00878756493330002, 0.00774909276515245, 0.00725954631343484, 0.00650930404663086, 0.0086864959448576, 0.0108613967895508, 0.00661327876150608, 0.00563916517421603, 0.00737257627770305, 0.00445715803653002, 0.00538036040961742, 0.0092308335006237, 0.0067189116962254, 0.00904726702719927, 0.00734152505174279, 0.00869574677199125, 0.00741700036451221, 0.0100174602121115, 0.0112427053973079 };
  static const int16_t buff_info_Conv2D_66_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_42_36_1_1[] = { 42, 1, 1, 36 };
  static const uint32_t buff_info__mem_shape_M_42_36_1_1[] = { 42, 2, 1, 1, 18 };
  static const float buff_info_Conv2D_69_weights_quant_scale[] = { 0.00277926656417549, 0.00203165179118514, 0.00461997557431459, 0.00213137385435402, 0.00242730090394616, 0.00340577773749828, 0.00446270313113928, 0.00385133782401681, 0.0028454961720854, 0.00260402378626168, 0.00411135843023658, 0.0052417335100472, 0.00265766540542245, 0.00365885091014206, 0.00415032962337136, 0.00404076976701617, 0.00280639622360468, 0.00232964870519936, 0.00357331219129264, 0.00418582232668996, 0.00210783840157092, 0.00397877767682076, 0.00386138004250824, 0.00163566798437387, 0.0031502100173384, 0.00196081004105508, 0.0042438474483788, 0.00266573228873312, 0.00146788929123431, 0.00112635991536081, 0.00159572612028569, 0.00266281282529235, 0.00161336676683277, 0.00176322588231415, 0.00173887645360082, 0.00179183238651603, 0.00123379414435476, 0.00126118597108871, 0.00198908429592848, 0.00305042485706508, 0.00124160386621952, 0.00168233085423708 };
  static const int16_t buff_info_Conv2D_69_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_42_1_3_3[] = { 42, 3, 3, 1 };
  static const uint32_t buff_info__mem_shape_F_42_1_3_3[] = { 42, 1, 3, 3 };
  static const float buff_info_Conv2D_77_weights_quant_scale[] = { 0.00671124178916216, 0.00615026708692312, 0.00543140014633536, 0.00502178119495511, 0.00654646987095475, 0.00669231871142983, 0.00556864310055971, 0.00500368513166904, 0.00568557484075427, 0.00527430698275566, 0.0114986430853605, 0.00516855390742421, 0.00590620934963226, 0.00432515889406204, 0.00720201851800084, 0.00589658319950104, 0.0056605925783515, 0.00742268888279796, 0.00521669769659638, 0.00366081460379064, 0.00398721126839519, 0.00785432104021311, 0.00758174667134881, 0.00659125065430999, 0.00764124374836683, 0.00568239390850067, 0.00491361878812313, 0.0053491173312068, 0.00699111772701144, 0.00877204444259405, 0.00594172580167651, 0.00445104250684381, 0.00780400959774852, 0.00965163856744766, 0.00577420042827725, 0.00923887360841036, 0.0139519544318318, 0.0128333978354931, 0.0105407042428851, 0.00648806430399418, 0.0120038669556379, 0.00882681831717491 };
  static const int16_t buff_info_Conv2D_77_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_48_42_1_1[] = { 48, 1, 1, 42 };
  static const uint32_t buff_info__mem_shape_M_48_42_1_1[] = { 48, 2, 1, 1, 21 };
  static const float buff_info_Conv2D_80_weights_quant_scale[] = { 0.00191014655865729, 0.00220318650826812, 0.00280245067551732, 0.00230297492817044, 0.00305761862546206, 0.0040523624047637, 0.0013406143989414, 0.00211260211654007, 0.00175485119689256, 0.00217359862290323, 0.00199811602942646, 0.00262096431106329, 0.00235754949972034, 0.00181550497654825, 0.00240212585777044, 0.00321130105294287, 0.00399585580453277, 0.00384815991856158, 0.00199696910567582, 0.00262451590970159, 0.00246370956301689, 0.002690703375265, 0.00366316363215446, 0.00197542412206531, 0.00275733135640621, 0.00224092160351574, 0.00210413034074008, 0.00215160055086017, 0.00249137845821679, 0.00148095411714166, 0.00187313067726791, 0.00216776505112648, 0.00146335666067898, 0.00200213119387627, 0.0019339588470757, 0.00155543652363122, 0.00172008527442813, 0.00175681209657341, 0.00147245964035392, 0.00174068612977862, 0.00200457288883626, 0.00209073140285909, 0.00173962931148708, 0.00148767081554979, 0.00087427330436185, 0.00115127780009061, 0.00166799582075328, 0.00124326127115637 };
  static const int16_t buff_info_Conv2D_80_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_56_48_1_1[] = { 56, 1, 1, 48 };
  static const uint32_t buff_info__mem_shape_M_56_48_1_1[] = { 56, 2, 1, 1, 24 };
  static const float buff_info_Conv2D_98_weights_quant_scale[] = { 0.00180372374597937, 0.00334798661060631, 0.00260258256457746, 0.0022068431135267, 0.00321015645749867, 0.00183201138861477, 0.00142934417817742, 0.00279981479980052, 0.00250820349901915, 0.00418411660939455, 0.00217152805998921, 0.00180131813976914, 0.00151625322178006, 0.00205552019178867, 0.00180015270598233, 0.00387412961572409, 0.00173686572816223, 0.0029063371475786, 0.00307601946406066, 0.00226158509030938, 0.00243060919456184, 0.00389263243414462, 0.00145195110235363, 0.00142267777118832, 0.00239107245579362, 0.00252389535307884, 0.00310956803150475, 0.00148460548371077, 0.00164847413543612, 0.00199556793086231, 0.00242550810799003, 0.00222927588038146, 0.00156218593474478, 0.00129335187375546, 0.00134695146698505, 0.00286296708509326, 0.00148288498166949, 0.00192056153900921, 0.00233393139205873, 0.00117322395090014, 0.00140082975849509, 0.00149991002399474, 0.00127798493485898, 0.000833773287013173, 0.00116312887985259, 0.00129975122399628, 0.0019401548197493, 0.00157340837176889, 0.00111592339817435, 0.000958480930421501, 0.00151606881991029, 0.00101506791543216, 0.00236533302813768, 0.00100903678685427, 0.000618274498265237, 0.00140920258127153 };
  static const int16_t buff_info_Conv2D_98_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_64_56_1_1[] = { 64, 1, 1, 56 };
  static const uint32_t buff_info__mem_shape_M_64_56_1_1[] = { 64, 4, 1, 1, 14 };
  static const float buff_info_Conv2D_112_weights_quant_scale[] = { 0.00262854248285294, 0.00281763845123351, 0.00373293017037213, 0.00247110403142869, 0.00367446546442807, 0.00224335445091128, 0.00208148313686252, 0.00291043170727789, 0.00248865387402475, 0.00393012864515185, 0.00358616164885461, 0.00331755797378719, 0.00257894163951278, 0.00274644582532346, 0.00316205248236656, 0.003843002486974, 0.00324061210267246, 0.00233107060194016, 0.00256950850598514, 0.00284537277184427, 0.00488984631374478, 0.00298444740474224, 0.00252672843635082, 0.00210993690416217, 0.00420093582943082, 0.00217989599332213, 0.00155561335850507, 0.00210445653647184, 0.00167830148711801, 0.00307929655537009, 0.00159719376824796, 0.00248062051832676, 0.00129035266581923, 0.00317235453985631, 0.00175368052441627, 0.00268170167692006, 0.00157833017874509, 0.00485456641763449, 0.0021071897353977, 0.00344795663841069, 0.00204091961495578, 0.00257351086474955, 0.00244151707738638, 0.00138830777723342, 0.00131048040930182, 0.00183260499034077, 0.00184407806955278, 0.00136272283270955, 0.00227750046178699, 0.00168670003768057, 0.00189482735004276, 0.00187290925532579, 0.0027391598559916, 0.00203045597299933, 0.00124834338203073, 0.00128096726257354, 0.00159401772543788, 0.00130888214334846, 0.000947190506849438, 0.00132692966144532, 0.00103150168433785, 0.00192308390978724, 0.000603288237471133, 0.000748478807508945 };
  static const int16_t buff_info_Conv2D_112_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_72_64_1_1[] = { 72, 1, 1, 64 };
  static const uint32_t buff_info__mem_shape_M_72_64_1_1[] = { 72, 4, 1, 1, 16 };
  static const float buff_info_Conv2D_126_weights_quant_scale[] = { 0.00280559249222279, 0.00425709830597043, 0.00298147974535823, 0.00462744478136301, 0.00377321057021618, 0.00247780210338533, 0.00364508060738444, 0.00731313787400723, 0.00247174012474716, 0.00165455904789269, 0.00394339999184012, 0.00537248095497489, 0.00193440855946392, 0.00291631813161075, 0.00321474694646895, 0.00319520756602287, 0.00297518749721348, 0.00272093154489994, 0.00252902531065047, 0.00202294904738665, 0.00280147977173328, 0.00356142176315188, 0.00280519668012857, 0.00227516167797148, 0.00306792533956468, 0.00341616873629391, 0.00234652427025139, 0.00369068188592792, 0.00221449253149331, 0.00207082647830248, 0.00282293418422341, 0.00216888217255473, 0.00171393528580666, 0.0028120104689151, 0.00328842806629837, 0.00338431750424206, 0.00255516823381186, 0.00281497184187174, 0.00237468699924648, 0.00227713491767645, 0.00317093799822032, 0.00254616443999112, 0.00202257535420358, 0.00188064866233617, 0.00223052641376853, 0.0020400530193001, 0.00223972997628152, 0.0024047049228102, 0.00170256604906172, 0.00189983029849827, 0.00153779308311641, 0.00184731523040682, 0.00253526330925524, 0.00238347798585892, 0.00322245433926582, 0.00249663484282792, 0.00171912997029722, 0.00220122141763568, 0.00164116837549955, 0.00219743372872472, 0.00184392801020294, 0.00153390131890774, 0.00179340341128409, 0.00203427532687783, 0.0010519081261009, 0.00179536850191653, 0.00154286052566022, 0.00143515411764383, 0.00101409538183361, 0.00101009558420628, 0.0016366537893191, 0.00123461848124862 };
  static const int16_t buff_info_Conv2D_126_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_80_72_1_1[] = { 80, 1, 1, 72 };
  static const uint32_t buff_info__mem_shape_M_80_72_1_1[] = { 80, 3, 1, 1, 24 };
  static const float buff_info_Conv2D_140_weights_quant_scale[] = { 0.00252426229417324, 0.00296953646466136, 0.00265519437380135, 0.00419789040461183, 0.00407707039266825, 0.00276292278431356, 0.00424081180244684, 0.00442224321886897, 0.00309380050748587, 0.00379670574329793, 0.00218739523552358, 0.00398351578041911, 0.00260761915706098, 0.00406857021152973, 0.00166981038637459, 0.00364931463263929, 0.00214892812073231, 0.00399369979277253, 0.00378396571613848, 0.00234560156241059, 0.00358460214920342, 0.00318751996383071, 0.00377101404592395, 0.00448904512450099, 0.00601543858647346, 0.00195737206377089, 0.00313007482327521, 0.00362012372352183, 0.00488970335572958, 0.00351381511427462, 0.00235002487897873, 0.00384371564723551, 0.00211883033625782, 0.00435305619612336, 0.00366244069300592, 0.00251732370816171, 0.00220188708044589, 0.00291346642188728, 0.00394507730379701, 0.00339220208115876, 0.00239241914823651, 0.00294702127575874, 0.00346303428523242, 0.00319862389005721, 0.00317410682328045, 0.00221317051909864, 0.00231635221280158, 0.00191766617354006, 0.00258483039215207, 0.00250197784043849, 0.00318089453503489, 0.00290693459101021, 0.00220197928138077, 0.00281146261841059, 0.00238473061472178, 0.00293485773727298, 0.00173316651489586, 0.00179360108450055, 0.00182986597064883, 0.00193251925520599, 0.00199161190539598, 0.00213177711702883, 0.00182720005977899, 0.00312327034771442, 0.00194247276522219, 0.00123210006859154, 0.00186394026968628, 0.00139268673956394, 0.00100752059370279, 0.00169664912391454, 0.0015392858767882, 0.00163084163796157, 0.00153983465861529, 0.00211364193819463, 0.00100510579068214, 0.00120705808512866, 0.00148477230686694, 0.00138309446629137, 0.00195375992916524, 0.001968152821064 };
  static const int16_t buff_info_Conv2D_140_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_88_80_1_1[] = { 88, 1, 1, 80 };
  static const uint32_t buff_info__mem_shape_M_88_80_1_1[] = { 88, 4, 1, 1, 20 };
  static const float buff_info_Conv2D_154_weights_quant_scale[] = { 0.00309963990002871, 0.00491076242178679, 0.00269324635155499, 0.00486936559900641, 0.00343081681057811, 0.00247184373438358, 0.00366210611537099, 0.00381264858879149, 0.00351341697387397, 0.00495993066579103, 0.00242503639310598, 0.00505288457497954, 0.00197825278155506, 0.00382113503292203, 0.00227507809177041, 0.00335512915626168, 0.00256468961015344, 0.00335759925656021, 0.0024655822198838, 0.00309778144583106, 0.00295502017252147, 0.00238985614851117, 0.00553020462393761, 0.00345063721761107, 0.00570340361446142, 0.00201120902784169, 0.0042490353807807, 0.00329142366535962, 0.00657914718613029, 0.00203074235469103, 0.00354809011332691, 0.0030371283646673, 0.00330858468078077, 0.00320140109397471, 0.00308468122966588, 0.00529837142676115, 0.00279899407178164, 0.00248730671592057, 0.00191757711581886, 0.00363701675087214, 0.002368493238464, 0.0024040078278631, 0.00384444324299693, 0.00309931510128081, 0.00212479615584016, 0.00301812519319355, 0.00220259907655418, 0.00365739455446601, 0.00305320182815194, 0.00204337039031088, 0.00242318538948894, 0.00180926139000803, 0.00333343097008765, 0.00240789283998311, 0.00335688050836325, 0.00368671980686486, 0.00308286002837121, 0.0038438190240413, 0.00165849213954061, 0.00242731417529285, 0.00174888875335455, 0.00209161755628884, 0.00231202505528927, 0.00281697628088295, 0.00193433894310147, 0.0018841001437977, 0.00241297855973244, 0.00348181370645761, 0.000954667921178043, 0.00199448294006288, 0.00164567702449858, 0.00165503099560738, 0.00139628676697612, 0.00171247380785644, 0.00155076233204454, 0.00203452212736011, 0.00165677594486624, 0.00240890053100884, 0.00260438700206578, 0.001620102673769, 0.00147625675890595, 0.000912846531718969, 0.00110489130020142, 0.0016972963931039, 0.00175139121711254, 0.00129422661848366, 0.0015312428586185, 0.00120018341112882 };
  static const int16_t buff_info_Conv2D_154_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_96_88_1_1[] = { 96, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_M_96_88_1_1[] = { 96, 4, 1, 1, 22 };
  static const float buff_info_Conv2D_165_weights_quant_scale[] = { 0.00159767782315612, 0.00178560160566121, 0.000997332856059074, 0.00134755787439644, 0.00222601625137031, 0.00129408214706928, 0.00106447737198323, 0.000925839005503803, 0.00144518818706274, 0.00159085891209543, 0.00168309081345797, 0.00156085484195501, 0.00159539887681603, 0.00280189537443221, 0.00168377219233662, 0.00313538080081344, 0.0023930543102324, 0.00114625063724816, 0.00106629997026175, 0.0020202815067023, 0.00120018748566508, 0.00196163123473525, 0.00267306761816144, 0.001659273984842, 0.00126563408412039, 0.00109217478893697, 0.0016681388951838, 0.00226346170529723, 0.00282403617165983, 0.00111803133040667, 0.00157003523781896, 0.00170066952705383, 0.00146589439827949, 0.00115869368892163, 0.00159590353723615, 0.00211898027919233, 0.00127986201550812, 0.0015895765973255, 0.00135851965751499, 0.00222379295155406, 0.00144190946593881, 0.00152456131763756, 0.00136264332104474, 0.00130897492635995, 0.000760398688726127, 0.00175274687353522, 0.00143535085953772, 0.00104921031743288, 0.00112935493234545, 0.00161272601690143, 0.00104499491862953, 0.00133389257825911, 0.00114685273729265, 0.00184252648614347, 0.00206220615655184, 0.0016066818498075, 0.00148472166620195, 0.00165706733241677, 0.00119688105769455, 0.00122579128947109, 0.00131166854407638, 0.00221547181718051, 0.00152787682600319, 0.00187698320951313, 0.000822047353722155, 0.0012393364449963, 0.00102677091490477, 0.00163226423319429, 0.000810958910733461, 0.00128198170568794, 0.000508456199895591, 0.00137880956754088, 0.00154507649131119, 0.00112307840026915, 0.00110382493585348, 0.00103805237449706, 0.000797023996710777, 0.00116806663572788, 0.00108336913399398, 0.000987495644949377, 0.0009161620400846, 0.00155614700634032, 0.001122219953686, 0.00112027430441231, 0.0011186373885721, 0.00108113838359714, 0.000846586772240698, 0.000731795211322606, 0.000821797293610871, 0.000968374079093337, 0.000788750126957893, 0.000676711380947381, 0.000810249359346926, 0.00108480011112988, 0.000585017492994666, 0.000833963102195412 };
  static const int16_t buff_info_Conv2D_165_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_32_88_1_1[] = { 32, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_F_32_88_1_1[] = { 32, 88, 1, 1 };
  static const float buff_info_Conv2D_168_weights_quant_scale[] = { 0.00268242950551212, 0.00368491047993302, 0.0101013332605362, 0.0101108970120549, 0.00705417152494192, 0.0100816925987601, 0.00816902425140142, 0.0104293469339609, 0.00903324969112873, 0.0124780684709549, 0.00802204385399818, 0.00880260206758976, 0.00906266365200281, 0.00764010520651937, 0.0147234136238694, 0.0113138193264604, 0.00440855044871569, 0.00584748107939959, 0.0218851286917925, 0.0218825675547123, 0.00754288258031011, 0.0145511459559202, 0.00838486105203629, 0.0143323270604014, 0.0107625247910619, 0.0184126980602741, 0.0096215819939971, 0.0119036752730608, 0.0125630078837276, 0.00891294982284307, 0.014986895956099, 0.0119003588333726 };
  static const int16_t buff_info_Conv2D_168_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_96_96_1_1[] = { 96, 1, 1, 96 };
  static const uint32_t buff_info__mem_shape_M_96_96_1_1[] = { 96, 4, 1, 1, 24 };
  static const float buff_info_Conv2D_190_weights_quant_scale[] = { 0.00198850478045642, 0.00138277572114021, 0.00399486254900694, 0.00108032952994108, 0.00314262206666172, 0.00249861809425056, 0.00302207190543413, 0.0016094712773338, 0.00320345791988075, 0.00165017519611865, 0.00296341767534614, 0.00371031975373626, 0.00172470649704337, 0.0017847097478807, 0.00166853750124574, 0.00319712609052658, 0.00286555429920554, 0.00178592244628817, 0.00265604606829584, 0.00297655910253525, 0.00130477279890329, 0.00268013845197856, 0.00294260354712605, 0.00232816790230572, 0.00140834366902709, 0.00284237693995237, 0.0019942345097661, 0.00234757456928492, 0.00381960277445614, 0.00336847710423172, 0.00258115888573229, 0.00213930942118168, 0.0011648575309664, 0.0027115736156702, 0.00346870929934084, 0.0029317915905267, 0.00210109166800976, 0.00237437803298235, 0.00139038288034499, 0.00268337014131248, 0.00259858905337751, 0.00185636954847723, 0.002362628467381, 0.00174792925827205, 0.0015814199578017, 0.0028118051122874, 0.00240029161795974, 0.00244280765764415, 0.00103199575096369, 0.00157259427942336, 0.00332497549243271, 0.00142444763332605, 0.00321069755591452, 0.00220434204675257, 0.00255871820263565, 0.001475854893215, 0.00331681058742106, 0.00242410064674914, 0.000989694381132722, 0.00177184375934303, 0.00239499984309077, 0.00272335857152939, 0.00198171031661332, 0.00301315146498382, 0.0014383188681677, 0.00210242997854948, 0.00193113903515041, 0.0031260009855032, 0.00110590143594891, 0.00144200876820832, 0.000955833937041461, 0.00181562046054751, 0.00147675292100757, 0.00157652492634952, 0.00210144952870905, 0.00133207160979509, 0.00179883907549083, 0.0019475023727864, 0.00144572765566409, 0.00166957371402532, 0.00166632933542132, 0.00163440126925707, 0.00161930511239916, 0.0020656653214246, 0.00217939401045442, 0.00179961114190519, 0.00111174117773771, 0.00142403633799404, 0.00109995994716883, 0.00165672367438674, 0.00157760491129011, 0.00110415217932314, 0.00166168855503201, 0.00141808204352856, 0.00104824255686253, 0.000947582127992064 };
  static const int16_t buff_info_Conv2D_190_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_201_weights_quant_scale[] = { 0.00280808773823082, 0.00321373273618519, 0.0035616394598037, 0.00130018847994506, 0.00262053334154189, 0.00482808286324143, 0.0029776319861412, 0.00153943931218237, 0.00444581406190991, 0.00217912928201258, 0.00274928356520832, 0.00304617267102003, 0.0035725471097976, 0.00310542923398316, 0.00262455735355616, 0.00237903115339577, 0.00263739563524723, 0.00285271718166769, 0.00201688660308719, 0.00237994221970439, 0.00141917320434004, 0.0042437519878149, 0.0043060458265245, 0.00251747365109622, 0.0018595460569486, 0.00402375916019082, 0.00261667626909912, 0.00169607566203922, 0.00476162089034915, 0.0019659916870296, 0.002020234009251, 0.00196348899044096, 0.00171315821353346, 0.00323968380689621, 0.00357314525172114, 0.00355431111529469, 0.00217234855517745, 0.00203347858041525, 0.00438635190948844, 0.00238299439661205, 0.00190991710405797, 0.00102395832072943, 0.00345487566664815, 0.00171793671324849, 0.00157797022257, 0.00188102945685387, 0.00132679706439376, 0.00227812025696039, 0.0021254641469568, 0.00151518557686359, 0.00223976001143456, 0.00293938885442913, 0.0015294998884201, 0.00154621037654579, 0.00257742218673229, 0.00166342977900058, 0.00299538136459887, 0.0019127904670313, 0.00130211189389229, 0.00239576911553741, 0.00222283462062478, 0.00203804741613567, 0.00179937272332609, 0.00258349161595106, 0.0016935981111601, 0.00445615826174617, 0.00170208502095193, 0.0023342645727098, 0.00231404136866331, 0.00312237301841378, 0.000718958326615393, 0.00194796384312212, 0.00214590970426798, 0.00168750202283263, 0.00163687998428941, 0.00183958024717867, 0.00179089594166726, 0.0011805152753368, 0.0013278229162097, 0.00226670270785689, 0.00243043433874846, 0.00278635579161346, 0.00130043062381446, 0.00228533893823624, 0.00234256987459958, 0.00196983898058534, 0.00143680337350816, 0.00171113340184093, 0.001592529239133, 0.00158663629554212, 0.00121069489978254, 0.00116305949632078, 0.00143389671575278, 0.00181175675243139, 0.001009187893942, 0.00145442795474082 };
  static const int16_t buff_info_Conv2D_201_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_212_weights_quant_scale[] = { 0.00441536959260702, 0.00302314735017717, 0.00217547710053623, 0.00178355653770268, 0.0032521013636142, 0.00305477809160948, 0.00292623834684491, 0.00301499711349607, 0.00276260334067047, 0.00283267023041844, 0.00157265516463667, 0.00242817797698081, 0.00171250046696514, 0.00354078807868063, 0.00260469852946699, 0.00212923903018236, 0.00280993757769465, 0.00292298966087401, 0.00380823225714266, 0.00246698968112469, 0.00264857476577163, 0.00391511572524905, 0.00214632600545883, 0.00303856283426285, 0.00531956739723682, 0.0019211150938645, 0.00187991373240948, 0.00265083904378116, 0.00336368475109339, 0.0024136807769537, 0.0045127016492188, 0.00230027991347015, 0.00181570579297841, 0.00405081454664469, 0.00351877557113767, 0.00242465804331005, 0.00372880254872143, 0.00511364080011845, 0.00267429067753255, 0.0052689965814352, 0.00365760689601302, 0.00180091033689678, 0.00352199911139905, 0.00441030226647854, 0.00180247612297535, 0.00242963642813265, 0.00152109237387776, 0.00262066116556525, 0.0019115440081805, 0.00236138282343745, 0.00278135389089584, 0.00237540993839502, 0.00225506233982742, 0.00245881616137922, 0.00260116485878825, 0.00152655621059239, 0.00414323853328824, 0.00141467689536512, 0.00190121540799737, 0.00191013934090734, 0.00124567560851574, 0.00130958948284388, 0.00165352621115744, 0.00213451823219657, 0.00156989705283195, 0.00291243591345847, 0.00332441669888794, 0.00246443273499608, 0.00168337265495211, 0.00157985079567879, 0.00134616228751838, 0.00294764619320631, 0.00260973419062793, 0.00205344008281827, 0.00424341438338161, 0.00241398578509688, 0.00313626579008996, 0.00284566404297948, 0.00227615307085216, 0.00145993963815272, 0.0026696128770709, 0.00134729908313602, 0.00198283861391246, 0.00256325025111437, 0.002429174259305, 0.00223817909136415, 0.00111783377360553, 0.00188286963384598, 0.00226288405247033, 0.00155926449224353, 0.00197951542213559, 0.00140008353628218, 0.00184546294622123, 0.00164737715385854, 0.00162418361287564, 0.00140142627060413 };
  static const int16_t buff_info_Conv2D_212_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_223_weights_quant_scale[] = { 0.00264944811351597, 0.00413368316367269, 0.00246650050394237, 0.000754917738959193, 0.00210130796767771, 0.00443182559683919, 0.00505635282024741, 0.00283072656020522, 0.00632022134959698, 0.00443717697635293, 0.000990280415862799, 0.00255326274782419, 0.00536670535802841, 0.00310090114362538, 0.00263740867376328, 0.00188930286094546, 0.00198821187950671, 0.00527532631531358, 0.00224747788161039, 0.00242050155065954, 0.00325497682206333, 0.00466346368193626, 0.00298325438052416, 0.00306189642287791, 0.00439344951882958, 0.000705685699358582, 0.00461767241358757, 0.00330732599832118, 0.00329748797230422, 0.00404379423707724, 0.00525121344253421, 0.00262691616080701, 0.0015388869214803, 0.00244139693677425, 0.00141839892603457, 0.00267799175344408, 0.0019229402532801, 0.00186972343362868, 0.00207045068964362, 0.00469144294038415, 0.00415333919227123, 0.00261285970918834, 0.00450625410303473, 0.00353537034243345, 0.001364930649288, 0.00300646829418838, 0.00138662429526448, 0.0021953119430691, 0.00492095807567239, 0.00174134876579046, 0.0019614459015429, 0.00335250096395612, 0.00146332930307835, 0.00141695397906005, 0.0035877067130059, 0.00265722791664302, 0.0030031856149435, 0.00342960166744888, 0.00159849913325161, 0.0012261678930372, 0.00192127039190382, 0.00281136808916926, 0.00132753991056234, 0.00251911603845656, 0.00340910418890417, 0.0043610674329102, 0.0020839108619839, 0.00124319340102375, 0.00175952108111233, 0.00319706555455923, 0.000993824447505176, 0.00290974299423397, 0.00183131906669587, 0.00202061282470822, 0.00190124486107379, 0.00235740258358419, 0.00255112000741065, 0.00287051615305245, 0.00173666805494577, 0.0030190993566066, 0.00412737019360065, 0.00221616751514375, 0.00142738420981914, 0.00332674873061478, 0.00131225015502423, 0.00159971171524376, 0.00112095801159739, 0.00193592847790569, 0.00359684135764837, 0.00177298591006547, 0.00113307486753911, 0.00146424619015306, 0.00292375963181257, 0.00203873752616346, 0.0018115978455171, 0.00160555634647608 };
  static const int16_t buff_info_Conv2D_223_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__mem_shape_F_96_96_1_1[] = { 96, 96, 1, 1 };
  static const float buff_info_Conv2D_230_weights_quant_scale[] = { 0.00505501125007868, 0.003402627306059, 0.0131552880629897, 0.0131682688370347, 0.00479889893904328, 0.00431806081905961, 0.0040913550183177, 0.00463304715231061, 0.00488016940653324, 0.00694095203652978, 0.00663624377921224, 0.00580189004540443, 0.00839808024466038, 0.00730753829702735, 0.00900369603186846, 0.0082684364169836, 0.00539602292701602, 0.00379783171229064, 0.0166535582393408, 0.0166569631546736, 0.00530602084472775, 0.0043090688996017, 0.00427095545455813, 0.0049187745898962, 0.00512485345825553, 0.00704842060804367, 0.00696427840739489, 0.00663970736786723, 0.00841502752155066, 0.00723107066005468, 0.00949737336486578, 0.00829139538109303, 0.00564066367223859, 0.00373126869089901, 0.0187707357108593, 0.0187707804143429, 0.0066446540877223, 0.00509116286411881, 0.00464149843901396, 0.00559424515813589, 0.00559499161317945, 0.00730976508930326, 0.00732935220003128, 0.00694427965208888, 0.00769974803552032, 0.00824891868978739, 0.0105401072651148, 0.00899170059710741, 0.0055667283013463, 0.00429845694452524, 0.0183694064617157, 0.0183508135378361, 0.00854803156107664, 0.00639255344867706, 0.00473078340291977, 0.00592787424102426, 0.00624428875744343, 0.00816117599606514, 0.00777887739241123, 0.00713723199442029, 0.00820426922291517, 0.00949953123927116, 0.0114193754270673, 0.00914765708148479, 0.00530964741483331, 0.00489671947434545, 0.0202070455998182, 0.0202151630073786, 0.0100542129948735, 0.00729807652533054, 0.00486630154773593, 0.00558415008708835, 0.00838552229106426, 0.00978096760809422, 0.00823540426790714, 0.00872145500034094, 0.00924224220216274, 0.0104863010346889, 0.00920832902193069, 0.0103378677740693, 0.0058109019882977, 0.00533244665712118, 0.026273638010025, 0.0262307096272707, 0.00869350880384445, 0.00750646414235234, 0.00705819809809327, 0.00620255433022976, 0.00986114516854286, 0.0105169164016843, 0.00828517787158489, 0.0100325383245945, 0.00959451403468847, 0.0107151446864009, 0.00975366495549679, 0.011951214633882 };
  static const int16_t buff_info_Conv2D_230_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_6_96_1_1[] = { 6, 1, 1, 96 };
  static const uint32_t buff_info__mem_shape_F_6_96_1_1[] = { 6, 96, 1, 1 };
  static const float buff_info_Conv2D_240_weights_quant_scale[] = { 0.0104650408029556, 0.0105041656643152, 0.00852485746145248, 0.007776471786201, 0.00681707728654146, 0.00544331828132272 };
  static const int16_t buff_info_Conv2D_240_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_2_88_1_1[] = { 2, 1, 1, 88 };
  static const uint32_t buff_info__mem_shape_M_2_88_1_1[] = { 2, 4, 1, 1, 22 };
  static const float buff_info_Conv2D_246_weights_quant_scale[] = { 0.0105239795520902, 0.0150612629950047 };
  static const int16_t buff_info_Conv2D_246_weights_quant_offset[] = { 0, 0 };
  static const uint32_t buff_info__shape_24_8_3_3[] = { 24, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_24_8_3_3[] = { 24, 3, 3, 8 };
  static const float buff_info_Conv2D_9_weights_inflated_334_quant_scale[] = { 0.00680512236431241, 0.0114423520863056, 0.00757048884406686, 0.0100872805342078, 0.0138478120788932, 0.00802811421453953, 0.00519136665388942, 0.00971368048340082, 0.00933831371366978, 0.00581877585500479, 0.00671513099223375, 0.0106185507029295, 0.00721940631046891, 0.00819133594632149, 0.0152060789987445, 0.00863141100853682, 0.0100087895989418, 0.00856069196015596, 0.00897479709237814, 0.00673859193921089, 0.00684364233165979, 0.0162201188504696, 0.00736107164993882, 0.00623702863231301 };
  static const int16_t buff_info_Conv2D_9_weights_inflated_334_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_23_weights_inflated_336_quant_scale[] = { 0.00635019363835454, 0.00758020766079426, 0.00985111854970455, 0.00567028811201453, 0.00542215956375003, 0.00984035991132259, 0.00683952867984772, 0.0100184036418796, 0.00546275731176138, 0.00760967563837767, 0.0069740004837513, 0.00970136653631926, 0.00743826059624553, 0.00347151211462915, 0.00529953185468912, 0.0117365121841431, 0.00771723547950387, 0.00670333625748754, 0.00790123455226421, 0.00866139959543943, 0.00748343905434012, 0.00604636920616031, 0.00957969203591347, 0.0106718875467777 };
  static const int16_t buff_info_Conv2D_23_weights_inflated_336_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_32_8_3_3[] = { 32, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_32_8_3_3[] = { 32, 3, 3, 8 };
  static const float buff_info_Conv2D_52_weights_inflated_339_quant_scale[] = { 0.0104222316294909, 0.00741918664425611, 0.00307227740995586, 0.0132620707154274, 0.00575776537880301, 0.0110945906490088, 0.00661476468667388, 0.00762712117284536, 0.0101782800629735, 0.00597857730463147, 0.00834378227591515, 0.00823637936264277, 0.00870076194405556, 0.00709559489041567, 0.00769096007570624, 0.00440743891522288, 0.00885327346622944, 0.0116432895883918, 0.0123578198254108, 0.0116684986278415, 0.00601955223828554, 0.00608508940786123, 0.00860645808279514, 0.00390690146014094, 0.00626623537391424, 0.0117344642058015, 0.00888892821967602, 0.0114125022664666, 0.012663665227592, 0.0110223283991218, 0.00798791274428368, 0.0110761765390635 };
  static const int16_t buff_info_Conv2D_52_weights_inflated_339_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_48_8_3_3[] = { 48, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_48_8_3_3[] = { 48, 3, 3, 8 };
  static const float buff_info_Conv2D_95_weights_inflated_343_quant_scale[] = { 0.00561858573928475, 0.00799133628606796, 0.00351266353391111, 0.00839909724891186, 0.00449483375996351, 0.00777226267382503, 0.00631761504337192, 0.00727720372378826, 0.00821614265441895, 0.00564394518733025, 0.00773984100669622, 0.0100492481142282, 0.00515239592641592, 0.00454411562532187, 0.00599417695775628, 0.00935149285942316, 0.00652888277545571, 0.00855379737913609, 0.00679390830919147, 0.00672223651781678, 0.00798783823847771, 0.0104219643399119, 0.00782344397157431, 0.00639198673889041, 0.00816398207098246, 0.00735526625066996, 0.0119203105568886, 0.00516673363745213, 0.00605582073330879, 0.00690346630290151, 0.00495626544579864, 0.00835746992379427, 0.00911918934434652, 0.0075725126080215, 0.00704194325953722, 0.00617738533765078, 0.0065485411323607, 0.00800527911633253, 0.00653584487736225, 0.00606053695082664, 0.00819467194378376, 0.0117989974096417, 0.0128588583320379, 0.0100977271795273, 0.0114388102665544, 0.0129442885518074, 0.014308906160295, 0.012420772574842 };
  static const int16_t buff_info_Conv2D_95_weights_inflated_343_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_56_8_3_3[] = { 56, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_56_8_3_3[] = { 56, 3, 3, 8 };
  static const float buff_info_Conv2D_109_weights_inflated_345_quant_scale[] = { 0.00781008927151561, 0.00722287641838193, 0.00800791569054127, 0.00672776298597455, 0.0041307806968689, 0.00425904290750623, 0.00744021125137806, 0.00507358647882938, 0.00606576818972826, 0.0062547018751502, 0.00535203609615564, 0.00559199508279562, 0.00731637049466372, 0.00951894372701645, 0.00749460887163877, 0.00841798726469278, 0.00664129666984081, 0.0069506922736764, 0.00761037925258279, 0.0046512633562088, 0.00923661049455404, 0.00691647827625275, 0.00562304304912686, 0.00630954513326287, 0.00606101471930742, 0.00499158957973123, 0.0107813645154238, 0.00688522635027766, 0.00541714299470186, 0.00738583784550428, 0.00572094740346074, 0.00751763628795743, 0.00856596697121859, 0.00584349920973182, 0.00638705072924495, 0.0108686182647943, 0.0060730604454875, 0.00913399364799261, 0.0102674616500735, 0.00824928469955921, 0.0087928269058466, 0.00855153426527977, 0.00805982481688261, 0.0121646290645003, 0.011140925809741, 0.00960018206387758, 0.00764823472127318, 0.00631123594939709, 0.0130378808826208, 0.00885188672691584, 0.0171316657215357, 0.011460842564702, 0.0132842287421227, 0.0100330328568816, 0.0133235864341259, 0.0117444433271885 };
  static const int16_t buff_info_Conv2D_109_weights_inflated_345_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_64_8_3_3[] = { 64, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_64_8_3_3[] = { 64, 3, 3, 8 };
  static const float buff_info_Conv2D_123_weights_inflated_347_quant_scale[] = { 0.0058443914167583, 0.00437547825276852, 0.00590158253908157, 0.00544151524081826, 0.00756693771108985, 0.00473214546218514, 0.00971068814396858, 0.00445864861831069, 0.00473837042227387, 0.00709594506770372, 0.00686843972653151, 0.00641378341242671, 0.0095847500488162, 0.0054344916716218, 0.00431463867425919, 0.00588603178039193, 0.00490135606378317, 0.00718634482473135, 0.00713483896106482, 0.0055301315151155, 0.00816543865948915, 0.00613025669008493, 0.00609459728002548, 0.00487926509231329, 0.00832813419401646, 0.00879729073494673, 0.00938974134624004, 0.00573477242141962, 0.00580901373177767, 0.00718938419595361, 0.00776280509307981, 0.00435777334496379, 0.00619382038712502, 0.00839111767709255, 0.010779014788568, 0.00827851705253124, 0.00839800294488668, 0.00705715827643871, 0.00782024674117565, 0.00643110368400812, 0.00412941724061966, 0.00754753220826387, 0.00748086674138904, 0.012376906350255, 0.00969662610441446, 0.00689454190433025, 0.00863559078425169, 0.013186096213758, 0.00924318935722113, 0.0103808315470815, 0.0103076649829745, 0.00793642085045576, 0.0115110920742154, 0.00729626230895519, 0.00978530384600163, 0.00735514843836427, 0.0117774596437812, 0.0134997218847275, 0.0120029235258698, 0.0120949884876609, 0.0154709089547396, 0.0117795215919614, 0.0139071494340897, 0.0129793221130967 };
  static const int16_t buff_info_Conv2D_123_weights_inflated_347_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_72_8_3_3[] = { 72, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_72_8_3_3[] = { 72, 3, 3, 8 };
  static const float buff_info_Conv2D_137_weights_inflated_349_quant_scale[] = { 0.00393123226240277, 0.00428581098094583, 0.00371323828585446, 0.00902634300291538, 0.00785480067133904, 0.00476965634152293, 0.0071792840026319, 0.00470777740702033, 0.00473495200276375, 0.00556454760953784, 0.00674895429983735, 0.00700958399102092, 0.00469778059050441, 0.00773237086832523, 0.00493572372943163, 0.0059802713803947, 0.00402260012924671, 0.00883071683347225, 0.00728658912703395, 0.00426194118335843, 0.0121349720284343, 0.00965119060128927, 0.00474293902516365, 0.00503887236118317, 0.00759618822485209, 0.00413802498951554, 0.00739053450524807, 0.0129074035212398, 0.00932918768376112, 0.00841079838573933, 0.00491866376250982, 0.0102452775463462, 0.0050035915337503, 0.00592512032017112, 0.00729077868163586, 0.00517085660248995, 0.00479909079149365, 0.0102782044559717, 0.0107625527307391, 0.00646158494055271, 0.00653845025226474, 0.00722227059304714, 0.00617700768634677, 0.00752165354788303, 0.0105505660176277, 0.00943822227418423, 0.0091615142300725, 0.00847782474011183, 0.00652853725478053, 0.00943163968622684, 0.0103198308497667, 0.010098684579134, 0.0106550073251128, 0.00761214550584555, 0.0101275248453021, 0.00605288194492459, 0.0102351037785411, 0.0128237567842007, 0.00775430211797357, 0.00944907311350107, 0.00620000436902046, 0.00939615163952112, 0.0102746319025755, 0.00794604420661926, 0.0113890981301665, 0.0114961992949247, 0.0133973183110356, 0.0109339291229844, 0.0128164775669575, 0.0113717261701822, 0.0130653167143464, 0.0139284068718553 };
  static const int16_t buff_info_Conv2D_137_weights_inflated_349_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_80_8_3_3[] = { 80, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_80_8_3_3[] = { 80, 3, 3, 8 };
  static const float buff_info_Conv2D_151_weights_inflated_351_quant_scale[] = { 0.00471307197585702, 0.010038168169558, 0.00387168559245765, 0.0113124186173081, 0.00534396851435304, 0.00267636845819652, 0.00937668606638908, 0.00333892111666501, 0.00325528951361775, 0.0101826833561063, 0.00488917622715235, 0.00875451788306236, 0.00484195072203875, 0.00815228652209044, 0.00624636700376868, 0.00560774048790336, 0.00506672775372863, 0.00509570119902492, 0.00682712160050869, 0.00310156610794365, 0.00607923418283463, 0.00493116909638047, 0.00504684681072831, 0.00464890152215958, 0.00913565792143345, 0.00560705550014973, 0.00814733188599348, 0.00896808505058289, 0.00784011092036963, 0.00695254886522889, 0.00695543736219406, 0.00632462231442332, 0.00456099817529321, 0.0044474103488028, 0.0060453750193119, 0.00442313496023417, 0.00566417351365089, 0.00686491560190916, 0.0075472597964108, 0.00488649262115359, 0.00497392239049077, 0.00499845808371902, 0.00410986365750432, 0.00523804873228073, 0.00790743622928858, 0.00611712364479899, 0.00748284393921494, 0.00723126158118248, 0.00494601344689727, 0.00418963097035885, 0.00650789821520448, 0.00739402836188674, 0.00576694868505001, 0.0080678379163146, 0.0095050660893321, 0.0076797534711659, 0.00671834917739034, 0.0102753601968288, 0.00985308364033699, 0.00547992577776313, 0.00660953111946583, 0.00931422971189022, 0.0131470644846559, 0.00801772624254227, 0.00962038431316614, 0.0099999038502574, 0.0133989145979285, 0.0098622739315033, 0.0159452185034752, 0.00841185264289379, 0.0142856519669294, 0.00896662101149559, 0.0159005764871836, 0.0199284050613642, 0.014071679674089, 0.0138563578948379, 0.0145966978743672, 0.0113422330468893, 0.0122322337701917, 0.0107135362923145 };
  static const int16_t buff_info_Conv2D_151_weights_inflated_351_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_88_8_3_3[] = { 88, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_88_8_3_3[] = { 88, 3, 3, 8 };
  static const float buff_info_Conv2D_162_weights_inflated_353_quant_scale[] = { 0.00376800424419343, 0.00416678981855512, 0.00708202784880996, 0.00483267568051815, 0.00652736239135265, 0.00688059581443667, 0.00429458636790514, 0.008240707218647, 0.00440671993419528, 0.00509201688691974, 0.00485421996563673, 0.00625696871429682, 0.00420195935294032, 0.00530301593244076, 0.00364599004387856, 0.00516794202849269, 0.00417021242901683, 0.00489931600168347, 0.00416270922869444, 0.00484640803188086, 0.00578008312731981, 0.0052173025906086, 0.00338998739607632, 0.00412026280537248, 0.00510233594104648, 0.00613736128434539, 0.00574382953345776, 0.00425587920472026, 0.00333374971523881, 0.00942954141646624, 0.00605283258482814, 0.00601727236062288, 0.00477447733283043, 0.00486468290910125, 0.00348315038718283, 0.00387703068554401, 0.010329719632864, 0.00691503239795566, 0.0068316487595439, 0.00479806819930673, 0.00633775582537055, 0.00705376453697681, 0.00577740324661136, 0.00534020364284515, 0.00851137470453978, 0.00510190380737185, 0.00750323245301843, 0.0049721198156476, 0.00699649658054113, 0.00560540426522493, 0.00466250209137797, 0.00859327614307404, 0.00526809180155396, 0.00589832104742527, 0.005760517437011, 0.00806872919201851, 0.00497840438038111, 0.00592334056273103, 0.0102360062301159, 0.010184052400291, 0.00799797661602497, 0.00556245911866426, 0.00668950285762548, 0.0070789665915072, 0.007612987421453, 0.0047891684807837, 0.00897151045501232, 0.00440940167754889, 0.00926035083830357, 0.00793280173093081, 0.0136203393340111, 0.00721943518146873, 0.00842182990163565, 0.0109413331374526, 0.00663923099637032, 0.00764858163893223, 0.0100674973800778, 0.00981470569968224, 0.0114594884216785, 0.00778340129181743, 0.0087927570566535, 0.00602606358006597, 0.0110258124768734, 0.00790611654520035, 0.0128569640219212, 0.0122625324875116, 0.0106388349086046, 0.0105311721563339 };
  static const int16_t buff_info_Conv2D_162_weights_inflated_353_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_96_8_3_3[] = { 96, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_96_8_3_3[] = { 96, 3, 3, 8 };
  static const float buff_info_Conv2D_187_weights_inflated_355_quant_scale[] = { 0.00355211761780083, 0.00375423720106483, 0.00696774944663048, 0.00503064179793, 0.00633459538221359, 0.00630133133381605, 0.00561518734320998, 0.00421960419043899, 0.00475886184722185, 0.00608462793752551, 0.00669573899358511, 0.00607077497988939, 0.00653065927326679, 0.00530661502853036, 0.0045219948515296, 0.00428700912743807, 0.00621008593589067, 0.00294153043068945, 0.0080312667414546, 0.00472827022895217, 0.00558255659416318, 0.00669268704950809, 0.00295505532994866, 0.00312930019572377, 0.00482503278180957, 0.00487261824309826, 0.00758143700659275, 0.00454671075567603, 0.00454632937908173, 0.00801105331629515, 0.0041200784035027, 0.00576235167682171, 0.0040434799157083, 0.00603929860517383, 0.00526033667847514, 0.00525308772921562, 0.00733200460672379, 0.00730602722615004, 0.00980034749954939, 0.00717993406578898, 0.00340983178466558, 0.00526605173945427, 0.00515433214604855, 0.00493943179026246, 0.00911800097674131, 0.0063529028557241, 0.00985042471438646, 0.00413737772032619, 0.00538164842873812, 0.00551000563427806, 0.00732085993513465, 0.00999352429062128, 0.0105335637927055, 0.00570889981463552, 0.00714362412691116, 0.00682646036148071, 0.00492603564634919, 0.0063340119086206, 0.00694707781076431, 0.00921722408384085, 0.00645218649879098, 0.00467364583164454, 0.00564505811780691, 0.00569724757224321, 0.0113382525742054, 0.0076185236684978, 0.00791730172932148, 0.0104361353442073, 0.0145074417814612, 0.00663277739658952, 0.0132081722840667, 0.00902118161320686, 0.00945602729916573, 0.00737614976242185, 0.00543039385229349, 0.00732876919209957, 0.00971646886318922, 0.00577223161235452, 0.0098321633413434, 0.0108695635572076, 0.0101936841383576, 0.00934275053441525, 0.0103337848559022, 0.00974873173981905, 0.0103737115859985, 0.00750767067074776, 0.0108795892447233, 0.0178030971437693, 0.00866925343871117, 0.0175525397062302, 0.00923164747655392, 0.010455409064889, 0.00796398520469666, 0.012159701436758, 0.0181858241558075, 0.0188865382224321 };
  static const int16_t buff_info_Conv2D_187_weights_inflated_355_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_198_weights_inflated_357_quant_scale[] = { 0.00339281791821122, 0.00750286364927888, 0.0101208733394742, 0.00992558337748051, 0.00656880531460047, 0.00649232231080532, 0.00445165345445275, 0.00550119671970606, 0.00417691981419921, 0.00628847535699606, 0.00887288432568312, 0.00792178139090538, 0.00591816194355488, 0.00567124458029866, 0.00706753972917795, 0.00336659187451005, 0.00375127629376948, 0.00447224313393235, 0.00430510798469186, 0.00414761947467923, 0.00472802948206663, 0.00566173112019897, 0.00756755890324712, 0.00351929757744074, 0.00363567564636469, 0.00380478310398757, 0.00643463851884007, 0.00632333662360907, 0.00716798892244697, 0.0056182611733675, 0.00456071272492409, 0.00600316748023033, 0.0050421329215169, 0.00478651840239763, 0.00471355160698295, 0.00504601746797562, 0.00844599492847919, 0.00871129799634218, 0.01094064489007, 0.00587220070883632, 0.00382214831188321, 0.00636706640943885, 0.00391928758472204, 0.00626358296722174, 0.00674889236688614, 0.00584238581359386, 0.00868534948676825, 0.00706244632601738, 0.00617973692715168, 0.00694907270371914, 0.00609091762453318, 0.0131082180887461, 0.00749832019209862, 0.0067828013561666, 0.00983947981148958, 0.0096374899148941, 0.00823177769780159, 0.00572413671761751, 0.00787383783608675, 0.00572626572102308, 0.00789123401045799, 0.00630204193294048, 0.0117298690602183, 0.00801499933004379, 0.00808297283947468, 0.00634619919583201, 0.00636756373569369, 0.0111988093703985, 0.0143516687676311, 0.00645739678293467, 0.01113395486027, 0.0107244206592441, 0.00620163232088089, 0.00832991395145655, 0.00590918771922588, 0.0104147829115391, 0.0116785680875182, 0.00567542295902967, 0.0127179743722081, 0.00936851371079683, 0.00665528234094381, 0.00823226664215326, 0.00797803327441216, 0.00917757768183947, 0.0096838278695941, 0.0151196951046586, 0.0125501053407788, 0.0179501008242369, 0.00805707089602947, 0.00717020081356168, 0.0116953030228615, 0.00745283300057054, 0.0159328114241362, 0.0119521114975214, 0.00843536667525768, 0.0111344428732991 };
  static const int16_t buff_info_Conv2D_198_weights_inflated_357_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_209_weights_inflated_359_quant_scale[] = { 0.00672419788315892, 0.00233286875300109, 0.00752473203465343, 0.00598831102252007, 0.0106617668643594, 0.00334468996152282, 0.00527379103004932, 0.00364689971320331, 0.00387303601019084, 0.00535504287108779, 0.00670383684337139, 0.00654679164290428, 0.00540239969268441, 0.00685212761163712, 0.00582458917051554, 0.00278432271443307, 0.00421120086684823, 0.00493856612592936, 0.0111332330852747, 0.00450556771829724, 0.00514622544869781, 0.00562319345772266, 0.00707817357033491, 0.00362131255678833, 0.00546743767336011, 0.00710225617513061, 0.0059971516020596, 0.00642306404188275, 0.0073954532854259, 0.00568518834188581, 0.00839431118220091, 0.00659506348893046, 0.00444973260164261, 0.00627457024529576, 0.0062995464541018, 0.00427800975739956, 0.00662808353081346, 0.0057171774096787, 0.009968644939363, 0.00444232812151313, 0.00442230654880404, 0.0084533728659153, 0.00768313091248274, 0.00895348098129034, 0.00641128979623318, 0.00537738250568509, 0.00951178278774023, 0.00635371683165431, 0.00573112070560455, 0.00641481578350067, 0.010077053681016, 0.00954483542591333, 0.0056497068144381, 0.00571746472269297, 0.00782323628664017, 0.00671756872907281, 0.00546329421922565, 0.00723280571401119, 0.00903321336954832, 0.00960557907819748, 0.00936331506818533, 0.00426079891622066, 0.00856982916593552, 0.00645732181146741, 0.0102737313136458, 0.00758038181811571, 0.010839912109077, 0.00989638362079859, 0.0134368408471346, 0.0112519012764096, 0.00792108941823244, 0.0079141715541482, 0.010048289783299, 0.00702907657250762, 0.00947570707648993, 0.00911684893071651, 0.00740181794390082, 0.0116018150001764, 0.00787106715142727, 0.00926583912223577, 0.00631311116740108, 0.0104074189439416, 0.0151913482695818, 0.0084683345630765, 0.00986509304493666, 0.00841881334781647, 0.0120841050520539, 0.00916790124028921, 0.00771679449826479, 0.00819731410592794, 0.00992060173302889, 0.00925473961979151, 0.00955779012292624, 0.00896925292909145, 0.00846298318356276, 0.013674140907824 };
  static const int16_t buff_info_Conv2D_209_weights_inflated_359_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_220_weights_inflated_361_quant_scale[] = { 0.00273277005180717, 0.0106178903952241, 0.0112941144034266, 0.00730336830019951, 0.00575045822188258, 0.0109223378822207, 0.00680674938485026, 0.00554246315732598, 0.00772058265283704, 0.00466570584103465, 0.00810043606907129, 0.00927872955799103, 0.00846922770142555, 0.00838836375623941, 0.0077447984367609, 0.00336144817993045, 0.0032290683593601, 0.00789311248809099, 0.00744795612990856, 0.00309497048147023, 0.00635543279349804, 0.00893313996493816, 0.0115318074822426, 0.00350734940730035, 0.0116423135623336, 0.00690969778224826, 0.00688553228974342, 0.00963404309004545, 0.00910151656717062, 0.00878093019127846, 0.00602289475500584, 0.00984053965657949, 0.00950461998581886, 0.00727640744298697, 0.00328958150930703, 0.00611649826169014, 0.00661584688350558, 0.00996326748281717, 0.00764987338334322, 0.00591734424233437, 0.00941124279052019, 0.00809846445918083, 0.013207177631557, 0.00856858678162098, 0.00949850771576166, 0.00603492651134729, 0.00562679208815098, 0.00554851070046425, 0.00948966201394796, 0.00690898904576898, 0.0115752266719937, 0.00471928622573614, 0.00710898730903864, 0.00552659668028355, 0.00857764016836882, 0.00848421826958656, 0.00471950089558959, 0.00646453397348523, 0.00741509301587939, 0.00784021802246571, 0.00925708934664726, 0.00661394232884049, 0.00948065426200628, 0.00981483235955238, 0.00877836346626282, 0.0101937707513571, 0.00785375107079744, 0.00961129553616047, 0.00994599610567093, 0.00504663400352001, 0.0127306748181581, 0.00610421784222126, 0.00524421827867627, 0.00619871215894818, 0.0065759252756834, 0.00831753946840763, 0.00701315281912684, 0.00582049693912268, 0.00746266916394234, 0.00833896547555923, 0.00730294641107321, 0.00715385610237718, 0.011481954716146, 0.00885327998548746, 0.00573957851156592, 0.00580050237476826, 0.00692051462829113, 0.00930770486593246, 0.0093715526163578, 0.0073520909063518, 0.00873724464327097, 0.0102488631382585, 0.00829810276627541, 0.00718231638893485, 0.00890188477933407, 0.00645499583333731 };
  static const int16_t buff_info_Conv2D_220_weights_inflated_361_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_24_3_3_5[] = { 24, 3, 5, 3 };
  static const uint32_t buff_info__mem_shape_L_24_3_3_5[] = { 24, 3, 5, 3 };
  static const float buff_info_Conv2D_4_weights_submask_0_0_0_0_24_3_3_5_362_quant_scale[] = { 0.0016170593444258, 0.00381314614787698, 0.00123717286624014, 0.00192667765077204, 0.00275077251717448, 0.00072763062780723, 0.0037320947740227, 0.00150008196942508, 0.00185951287858188, 0.000922260980587453, 0.00292091746814549, 0.00141853431705385, 0.00395502895116806, 0.00475265923887491, 0.00215635099448264, 0.00326272961683571, 0.00345675554126501, 0.00356370024383068, 0.00213584955781698, 0.00160339754074812, 0.000726466299965978, 0.000747338403016329, 0.00220523495227098, 0.00260749994777143 };
  static const int16_t buff_info_Conv2D_4_weights_submask_0_0_0_0_24_3_3_5_362_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_24_3_2_5[] = { 24, 2, 5, 3 };
  static const uint32_t buff_info__mem_shape_L_24_3_2_5[] = { 24, 2, 5, 3 };
  static const float buff_info_Conv2D_4_weights_submask_0_0_3_0_24_3_2_5_363_quant_scale[] = { 0.0016170593444258, 0.00381314614787698, 0.00123717286624014, 0.00192667765077204, 0.00275077251717448, 0.00072763062780723, 0.0037320947740227, 0.00150008196942508, 0.00185951287858188, 0.000922260980587453, 0.00292091746814549, 0.00141853431705385, 0.00395502895116806, 0.00475265923887491, 0.00215635099448264, 0.00326272961683571, 0.00345675554126501, 0.00356370024383068, 0.00213584955781698, 0.00160339754074812, 0.000726466299965978, 0.000747338403016329, 0.00220523495227098, 0.00260749994777143 };
  static const int16_t buff_info_Conv2D_4_weights_submask_0_0_3_0_24_3_2_5_363_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_64_1_1_1[] = { 64, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_64_1_1_1[] = { 64, 1, 1, 1 };
  static const uint32_t buff_info__shape_32_1_1_1[] = { 32, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_32_1_1_1[] = { 32, 1, 1, 1 };
  static const uint32_t buff_info__shape_16_1_1_1[] = { 16, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_16_1_1_1[] = { 16, 1, 1, 1 };
  static const uint32_t buff_info__shape_8_1_1_1[] = { 8, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_8_1_1_1[] = { 8, 1, 1, 1 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_0_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_128_3,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_128_3,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Quantize_2_y_scale",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154480,
      .offset_end = 154484,
      .offset_limit = 154552,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Quantize_2_y_zero_point",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154528,
      .offset_end = 154529,
      .offset_limit = 154600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_12_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 151888,
      .offset_end = 152464,
      .offset_limit = 152528,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_24_24_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_24_24_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_12_weights_quant_scale,
      .offset = buff_info_Conv2D_12_weights_quant_offset,
    },
    {
      .name = "Conv2D_26_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 151216,
      .offset_end = 151888,
      .offset_limit = 151952,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_F_28_24_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_28_24_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_26_weights_quant_scale,
      .offset = buff_info_Conv2D_26_weights_quant_offset,
    },
    {
      .name = "Conv2D_34_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 153760,
      .offset_end = 154012,
      .offset_limit = 154080,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_28_1_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_28_1_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_34_weights_quant_scale,
      .offset = buff_info_Conv2D_34_weights_quant_offset,
    },
    {
      .name = "Conv2D_37_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 149600,
      .offset_end = 150496,
      .offset_limit = 150560,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 14,
      .mem_shape = buff_info__mem_shape_M_32_28_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_28_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_37_weights_quant_scale,
      .offset = buff_info_Conv2D_37_weights_quant_offset,
    },
    {
      .name = "Conv2D_55_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 147360,
      .offset_end = 148512,
      .offset_limit = 148576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_36_32_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_36_32_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_55_weights_quant_scale,
      .offset = buff_info_Conv2D_55_weights_quant_offset,
    },
    {
      .name = "Conv2D_66_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 153424,
      .offset_end = 153748,
      .offset_limit = 153816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_36_1_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_36_1_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_66_weights_quant_scale,
      .offset = buff_info_Conv2D_66_weights_quant_offset,
    },
    {
      .name = "Conv2D_69_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 145840,
      .offset_end = 147352,
      .offset_limit = 147416,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 18,
      .mem_shape = buff_info__mem_shape_M_42_36_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_42_36_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_69_weights_quant_scale,
      .offset = buff_info_Conv2D_69_weights_quant_offset,
    },
    {
      .name = "Conv2D_77_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 153040,
      .offset_end = 153418,
      .offset_limit = 153488,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_42_1_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_42_1_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_77_weights_quant_scale,
      .offset = buff_info_Conv2D_77_weights_quant_offset,
    },
    {
      .name = "Conv2D_80_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 140368,
      .offset_end = 142384,
      .offset_limit = 142448,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 21,
      .mem_shape = buff_info__mem_shape_M_48_42_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_48_42_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_80_weights_quant_scale,
      .offset = buff_info_Conv2D_80_weights_quant_offset,
    },
    {
      .name = "Conv2D_98_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 135376,
      .offset_end = 138064,
      .offset_limit = 138128,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_M_56_48_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_56_48_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_98_weights_quant_scale,
      .offset = buff_info_Conv2D_98_weights_quant_offset,
    },
    {
      .name = "Conv2D_112_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 125504,
      .offset_end = 129088,
      .offset_limit = 129152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 14,
      .mem_shape = buff_info__mem_shape_M_64_56_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_64_56_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_112_weights_quant_scale,
      .offset = buff_info_Conv2D_112_weights_quant_offset,
    },
    {
      .name = "Conv2D_126_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 116864,
      .offset_end = 121472,
      .offset_limit = 121536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_72_64_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_72_64_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_126_weights_quant_scale,
      .offset = buff_info_Conv2D_126_weights_quant_offset,
    },
    {
      .name = "Conv2D_140_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 95552,
      .offset_end = 101312,
      .offset_limit = 101376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_M_80_72_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_80_72_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_140_weights_quant_scale,
      .offset = buff_info_Conv2D_140_weights_quant_offset,
    },
    {
      .name = "Conv2D_154_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 54528,
      .offset_end = 61568,
      .offset_limit = 61632,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 20,
      .mem_shape = buff_info__mem_shape_M_88_80_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_88_80_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_154_weights_quant_scale,
      .offset = buff_info_Conv2D_154_weights_quant_offset,
    },
    {
      .name = "Conv2D_165_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 46080,
      .offset_end = 54528,
      .offset_limit = 54592,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 22,
      .mem_shape = buff_info__mem_shape_M_96_88_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_88_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_165_weights_quant_scale,
      .offset = buff_info_Conv2D_165_weights_quant_offset,
    },
    {
      .name = "Conv2D_168_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 132544,
      .offset_end = 135364,
      .offset_limit = 135432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_F_32_88_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_88_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_168_weights_quant_scale,
      .offset = buff_info_Conv2D_168_weights_quant_offset,
    },
    {
      .name = "Conv2D_190_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 0,
      .offset_end = 9216,
      .offset_limit = 9280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_M_96_96_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_190_weights_quant_scale,
      .offset = buff_info_Conv2D_190_weights_quant_offset,
    },
    {
      .name = "Conv2D_201_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 9216,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_M_96_96_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_201_weights_quant_scale,
      .offset = buff_info_Conv2D_201_weights_quant_offset,
    },
    {
      .name = "Conv2D_212_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 18432,
      .offset_end = 27648,
      .offset_limit = 27712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_M_96_96_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_212_weights_quant_scale,
      .offset = buff_info_Conv2D_212_weights_quant_offset,
    },
    {
      .name = "Conv2D_223_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 27648,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_M_96_96_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_223_weights_quant_scale,
      .offset = buff_info_Conv2D_223_weights_quant_offset,
    },
    {
      .name = "Conv2D_230_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 36864,
      .offset_end = 46080,
      .offset_limit = 46144,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_96_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_96_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_230_weights_quant_scale,
      .offset = buff_info_Conv2D_230_weights_quant_offset,
    },
    {
      .name = "Dequantize_238_x_scale",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154464,
      .offset_end = 154468,
      .offset_limit = 154536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Dequantize_238_x_zero_point",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154512,
      .offset_end = 154513,
      .offset_limit = 154584,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_240_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 152464,
      .offset_end = 153040,
      .offset_limit = 153104,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_6_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_6_96_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_240_weights_quant_scale,
      .offset = buff_info_Conv2D_240_weights_quant_offset,
    },
    {
      .name = "Conv2D_246_weights",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154016,
      .offset_end = 154192,
      .offset_limit = 154256,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 22,
      .mem_shape = buff_info__mem_shape_M_2_88_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_2_88_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_246_weights_quant_scale,
      .offset = buff_info_Conv2D_246_weights_quant_offset,
    },
    {
      .name = "Dequantize_258_x_scale",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154496,
      .offset_end = 154500,
      .offset_limit = 154568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Dequantize_258_x_zero_point",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154544,
      .offset_end = 154545,
      .offset_limit = 154616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_9_weights_inflated_334",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 142384,
      .offset_end = 144112,
      .offset_limit = 144176,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_24_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_24_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_9_weights_inflated_334_quant_scale,
      .offset = buff_info_Conv2D_9_weights_inflated_334_quant_offset,
    },
    {
      .name = "Conv2D_23_weights_inflated_336",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 144112,
      .offset_end = 145840,
      .offset_limit = 145904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_24_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_24_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_23_weights_inflated_336_quant_scale,
      .offset = buff_info_Conv2D_23_weights_inflated_336_quant_offset,
    },
    {
      .name = "Conv2D_52_weights_inflated_339",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 138064,
      .offset_end = 140368,
      .offset_limit = 140432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_32_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_52_weights_inflated_339_quant_scale,
      .offset = buff_info_Conv2D_52_weights_inflated_339_quant_offset,
    },
    {
      .name = "Conv2D_95_weights_inflated_343",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 129088,
      .offset_end = 132544,
      .offset_limit = 132608,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_48_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_48_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_95_weights_inflated_343_quant_scale,
      .offset = buff_info_Conv2D_95_weights_inflated_343_quant_offset,
    },
    {
      .name = "Conv2D_109_weights_inflated_345",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 121472,
      .offset_end = 125504,
      .offset_limit = 125568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_56_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_56_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_109_weights_inflated_345_quant_scale,
      .offset = buff_info_Conv2D_109_weights_inflated_345_quant_offset,
    },
    {
      .name = "Conv2D_123_weights_inflated_347",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 112256,
      .offset_end = 116864,
      .offset_limit = 116928,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_64_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_64_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_123_weights_inflated_347_quant_scale,
      .offset = buff_info_Conv2D_123_weights_inflated_347_quant_offset,
    },
    {
      .name = "Conv2D_137_weights_inflated_349",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 107072,
      .offset_end = 112256,
      .offset_limit = 112320,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_72_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_72_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_137_weights_inflated_349_quant_scale,
      .offset = buff_info_Conv2D_137_weights_inflated_349_quant_offset,
    },
    {
      .name = "Conv2D_151_weights_inflated_351",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 101312,
      .offset_end = 107072,
      .offset_limit = 107136,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_80_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_80_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_151_weights_inflated_351_quant_scale,
      .offset = buff_info_Conv2D_151_weights_inflated_351_quant_offset,
    },
    {
      .name = "Conv2D_162_weights_inflated_353",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 89216,
      .offset_end = 95552,
      .offset_limit = 95616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_88_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_88_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_162_weights_inflated_353_quant_scale,
      .offset = buff_info_Conv2D_162_weights_inflated_353_quant_offset,
    },
    {
      .name = "Conv2D_187_weights_inflated_355",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 61568,
      .offset_end = 68480,
      .offset_limit = 68544,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_96_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_187_weights_inflated_355_quant_scale,
      .offset = buff_info_Conv2D_187_weights_inflated_355_quant_offset,
    },
    {
      .name = "Conv2D_198_weights_inflated_357",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 68480,
      .offset_end = 75392,
      .offset_limit = 75456,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_96_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_198_weights_inflated_357_quant_scale,
      .offset = buff_info_Conv2D_198_weights_inflated_357_quant_offset,
    },
    {
      .name = "Conv2D_209_weights_inflated_359",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 75392,
      .offset_end = 82304,
      .offset_limit = 82368,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_96_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_209_weights_inflated_359_quant_scale,
      .offset = buff_info_Conv2D_209_weights_inflated_359_quant_offset,
    },
    {
      .name = "Conv2D_220_weights_inflated_361",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 82304,
      .offset_end = 89216,
      .offset_limit = 89280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_96_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_96_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_220_weights_inflated_361_quant_scale,
      .offset = buff_info_Conv2D_220_weights_inflated_361_quant_offset,
    },
    {
      .name = "Conv2D_4_weights_submask_0_0_0_0_24_3_3_5_362",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 148512,
      .offset_end = 149592,
      .offset_limit = 149656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_24_3_3_5,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_24_3_3_5,
      .per_channel = 1,
      .scale = buff_info_Conv2D_4_weights_submask_0_0_0_0_24_3_3_5_362_quant_scale,
      .offset = buff_info_Conv2D_4_weights_submask_0_0_0_0_24_3_3_5_362_quant_offset,
    },
    {
      .name = "Conv2D_4_weights_submask_0_0_3_0_24_3_2_5_363",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 150496,
      .offset_end = 151216,
      .offset_limit = 151280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_24_3_2_5,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_24_3_2_5,
      .per_channel = 1,
      .scale = buff_info_Conv2D_4_weights_submask_0_0_3_0_24_3_2_5_363_quant_scale,
      .offset = buff_info_Conv2D_4_weights_submask_0_0_3_0_24_3_2_5_363_quant_offset,
    },
    {
      .name = "Pad_20_pad_kern_67",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154192,
      .offset_end = 154256,
      .offset_limit = 154320,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_64_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_64_1_1_1,
    },
    {
      .name = "Pad_42_pad_kern_137",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154320,
      .offset_end = 154352,
      .offset_limit = 154416,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_32_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_1_1_1,
    },
    {
      .name = "Pad_49_pad_kern_150",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154288,
      .offset_end = 154320,
      .offset_limit = 154384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_32_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_1_1_1,
    },
    {
      .name = "Pad_63_pad_kern_191",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154256,
      .offset_end = 154288,
      .offset_limit = 154352,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_32_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_1_1_1,
    },
    {
      .name = "Pad_85_pad_kern_261",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154432,
      .offset_end = 154448,
      .offset_limit = 154512,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_16_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_16_1_1_1,
    },
    {
      .name = "Pad_92_pad_kern_274",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154416,
      .offset_end = 154432,
      .offset_limit = 154496,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_16_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_16_1_1_1,
    },
    {
      .name = "Pad_106_pad_kern_315",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154400,
      .offset_end = 154416,
      .offset_limit = 154480,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_16_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_16_1_1_1,
    },
    {
      .name = "Pad_120_pad_kern_356",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154384,
      .offset_end = 154400,
      .offset_limit = 154464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_16_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_16_1_1_1,
    },
    {
      .name = "Pad_134_pad_kern_397",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154368,
      .offset_end = 154384,
      .offset_limit = 154448,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_16_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_16_1_1_1,
    },
    {
      .name = "Pad_148_pad_kern_438",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154352,
      .offset_end = 154368,
      .offset_limit = 154432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_16_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_16_1_1_1,
    },
    {
      .name = "Pad_180_pad_kern_538",
      .addr_base = {(unsigned char *)(0x72000000UL) /* Equivalent hex address = 0x72000000UL */},
      .offset_start = 154448,
      .offset_end = 154456,
      .offset_limit = 154520,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_8_1_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_8_1_1_1,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_face_detection(void)
{
  static const uint32_t buff_info__shape_1_896_1[] = { 1, 896, 1, 1 };
  static const uint32_t buff_info__mem_shape_F_1_896_1[] = { 1, 896, 1 };
  static const uint32_t buff_info__shape_1_896_16[] = { 1, 896, 16, 1 };
  static const uint32_t buff_info__mem_shape_F_1_896_16[] = { 1, 896, 16 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Transpose_259_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 118272,
      .offset_limit = 118336,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_896_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_896_1,
    },
    {
      .name = "Transpose_239_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 75,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_896_16,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_896_16,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_face_detection(void)
{
  static const uint32_t buff_info__shape_1_3_128_128[] = { 1, 128, 128, 3 };
  static const uint32_t buff_info__mem_shape_L_1_3_128_128[] = { 1, 128, 128, 3 };
  static const float buff_info_Quantize_2_out_0_quant_scale[] = { 0.00784299988299608 };
  static const int16_t buff_info_Quantize_2_out_0_quant_offset[] = { -1 };
  static const uint32_t buff_info__shape_1_24_64_64[] = { 1, 64, 64, 24 };
  static const uint32_t buff_info__mem_shape_M_1_24_64_64[] = { 1, 6, 64, 64, 4 };
  static const float buff_info_Relu_7_out_0_quant_scale[] = { 0.0218591578304768 };
  static const int16_t buff_info_Relu_7_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_24_64_64[] = { 1, 64, 64, 24 };
  static const float buff_info_Conv2D_9_zero_off_out_10_quant_scale[] = { 0.0218591578304768 };
  static const int16_t buff_info_Conv2D_9_zero_off_out_10_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_9_off_bias_out_16_quant_scale[] = { 0.0428088754415512 };
  static const int16_t buff_info_Conv2D_9_off_bias_out_16_quant_offset[] = { 47 };
  static const float buff_info_Conv2D_12_off_bias_out_25_quant_scale[] = { 0.0650150701403618 };
  static const int16_t buff_info_Conv2D_12_off_bias_out_25_quant_offset[] = { 30 };
  static const float buff_info_Relu_18_out_0_quant_scale[] = { 0.0305404495447874 };
  static const int16_t buff_info_Relu_18_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_23_zero_off_out_28_quant_scale[] = { 0.0305404495447874 };
  static const int16_t buff_info_Conv2D_23_zero_off_out_28_quant_offset[] = { 0 };
  static const uint32_t buff_info__shape_1_64_24_64[] = { 1, 24, 64, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_24_64[] = { 1, 24, 64, 64 };
  static const float buff_info_Transpose_19_out_0_quant_scale[] = { 0.0305404495447874 };
  static const int16_t buff_info_Transpose_19_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_23_off_bias_out_34_quant_scale[] = { 0.062401257455349 };
  static const int16_t buff_info_Conv2D_23_off_bias_out_34_quant_offset[] = { -7 };
  static const uint32_t buff_info__mem_shape_F_1_64_24_64[] = { 1, 64, 24, 64 };
  static const uint32_t buff_info__shape_1_64_28_64[] = { 1, 28, 64, 64 };
  static const uint32_t buff_info__mem_shape_F_1_64_28_64[] = { 1, 64, 28, 64 };
  static const uint32_t buff_info__shape_1_28_64_64[] = { 1, 64, 64, 28 };
  static const uint32_t buff_info__mem_shape_L_1_28_64_64[] = { 1, 64, 64, 28 };
  static const float buff_info_Conv2D_26_off_bias_out_43_quant_scale[] = { 0.121073246002197 };
  static const int16_t buff_info_Conv2D_26_off_bias_out_43_quant_offset[] = { -13 };
  static const uint32_t buff_info__mem_shape_F_1_28_64_64[] = { 1, 28, 64, 64 };
  static const float buff_info_Transpose_21_out_0_quant_scale[] = { 0.0305404495447874 };
  static const int16_t buff_info_Transpose_21_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_28_32_32[] = { 1, 32, 32, 28 };
  static const uint32_t buff_info__mem_shape_L_1_28_32_32[] = { 1, 32, 32, 28 };
  static const float buff_info_Conv2D_34_off_bias_out_52_quant_scale[] = { 0.11693113297224 };
  static const int16_t buff_info_Conv2D_34_off_bias_out_52_quant_offset[] = { -5 };
  static const uint32_t buff_info__mem_shape_F_1_28_32_32[] = { 1, 28, 32, 32 };
  static const float buff_info_MaxPool_40_out_0_quant_scale[] = { 0.0807102024555206 };
  static const int16_t buff_info_MaxPool_40_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_32_28_32[] = { 1, 28, 32, 32 };
  static const uint32_t buff_info__mem_shape_F_1_32_28_32[] = { 1, 32, 28, 32 };
  static const float buff_info_Transpose_41_out_0_quant_scale[] = { 0.0807102024555206 };
  static const int16_t buff_info_Transpose_41_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_32_32_32[] = { 1, 32, 32, 32 };
  static const uint32_t buff_info__mem_shape_F_1_32_32_32[] = { 1, 32, 32, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_32_32[] = { 1, 32, 32, 32 };
  static const float buff_info_Conv2D_37_off_bias_out_61_quant_scale[] = { 0.100445799529552 };
  static const int16_t buff_info_Conv2D_37_off_bias_out_61_quant_offset[] = { 4 };
  static const float buff_info_Transpose_43_out_0_quant_scale[] = { 0.0807102024555206 };
  static const int16_t buff_info_Transpose_43_out_0_quant_offset[] = { -128 };
  static const float buff_info_Relu_47_out_0_quant_scale[] = { 0.0725421532988548 };
  static const int16_t buff_info_Relu_47_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_52_zero_off_out_64_quant_scale[] = { 0.0725421532988548 };
  static const int16_t buff_info_Conv2D_52_zero_off_out_64_quant_offset[] = { 0 };
  static const float buff_info_Transpose_48_out_0_quant_scale[] = { 0.0725421532988548 };
  static const int16_t buff_info_Transpose_48_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_52_off_bias_out_70_quant_scale[] = { 0.162213429808617 };
  static const int16_t buff_info_Conv2D_52_off_bias_out_70_quant_offset[] = { -38 };
  static const uint32_t buff_info__shape_1_32_36_32[] = { 1, 36, 32, 32 };
  static const uint32_t buff_info__mem_shape_F_1_32_36_32[] = { 1, 32, 36, 32 };
  static const uint32_t buff_info__shape_1_36_32_32[] = { 1, 32, 32, 36 };
  static const uint32_t buff_info__mem_shape_F_1_36_32_32[] = { 1, 36, 32, 32 };
  static const float buff_info_Transpose_50_out_0_quant_scale[] = { 0.0725421532988548 };
  static const int16_t buff_info_Transpose_50_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_36_32_32[] = { 1, 32, 32, 36 };
  static const float buff_info_Conv2D_55_off_bias_out_79_quant_scale[] = { 0.0805235356092453 };
  static const int16_t buff_info_Conv2D_55_off_bias_out_79_quant_offset[] = { 10 };
  static const float buff_info_Relu_61_out_0_quant_scale[] = { 0.0731675401329994 };
  static const int16_t buff_info_Relu_61_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_66_off_bias_out_88_quant_scale[] = { 0.237331345677376 };
  static const int16_t buff_info_Conv2D_66_off_bias_out_88_quant_offset[] = { 6 };
  static const float buff_info_Transpose_62_out_0_quant_scale[] = { 0.0731675401329994 };
  static const int16_t buff_info_Transpose_62_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_32_42_32[] = { 1, 42, 32, 32 };
  static const uint32_t buff_info__mem_shape_F_1_32_42_32[] = { 1, 32, 42, 32 };
  static const uint32_t buff_info__shape_1_42_32_32[] = { 1, 32, 32, 42 };
  static const uint32_t buff_info__mem_shape_L_1_42_32_32[] = { 1, 32, 32, 42 };
  static const float buff_info_Conv2D_69_off_bias_out_97_quant_scale[] = { 0.0844188630580902 };
  static const int16_t buff_info_Conv2D_69_off_bias_out_97_quant_offset[] = { 55 };
  static const uint32_t buff_info__mem_shape_F_1_42_32_32[] = { 1, 42, 32, 32 };
  static const float buff_info_Transpose_64_out_0_quant_scale[] = { 0.0731675401329994 };
  static const int16_t buff_info_Transpose_64_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_42_16_16[] = { 1, 16, 16, 42 };
  static const uint32_t buff_info__mem_shape_L_1_42_16_16[] = { 1, 16, 16, 42 };
  static const float buff_info_Conv2D_77_off_bias_out_106_quant_scale[] = { 0.103940829634666 };
  static const int16_t buff_info_Conv2D_77_off_bias_out_106_quant_offset[] = { -23 };
  static const uint32_t buff_info__mem_shape_F_1_42_16_16[] = { 1, 42, 16, 16 };
  static const float buff_info_MaxPool_83_out_0_quant_scale[] = { 0.0374998189508915 };
  static const int16_t buff_info_MaxPool_83_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_16_42_16[] = { 1, 42, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_42_16[] = { 1, 16, 42, 16 };
  static const float buff_info_Transpose_84_out_0_quant_scale[] = { 0.0374998189508915 };
  static const int16_t buff_info_Transpose_84_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_16_48_16[] = { 1, 48, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_48_16[] = { 1, 16, 48, 16 };
  static const uint32_t buff_info__shape_1_48_16_16[] = { 1, 16, 16, 48 };
  static const uint32_t buff_info__mem_shape_L_1_48_16_16[] = { 1, 16, 16, 48 };
  static const float buff_info_Conv2D_80_off_bias_out_115_quant_scale[] = { 0.0548089183866978 };
  static const int16_t buff_info_Conv2D_80_off_bias_out_115_quant_offset[] = { -25 };
  static const uint32_t buff_info__mem_shape_F_1_48_16_16[] = { 1, 48, 16, 16 };
  static const float buff_info_Transpose_86_out_0_quant_scale[] = { 0.0374998189508915 };
  static const int16_t buff_info_Transpose_86_out_0_quant_offset[] = { -128 };
  static const float buff_info_Relu_90_out_0_quant_scale[] = { 0.0532370805740356 };
  static const int16_t buff_info_Relu_90_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_95_zero_off_out_118_quant_scale[] = { 0.0532370805740356 };
  static const int16_t buff_info_Conv2D_95_zero_off_out_118_quant_offset[] = { 0 };
  static const float buff_info_Transpose_91_out_0_quant_scale[] = { 0.0532370805740356 };
  static const int16_t buff_info_Transpose_91_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_16_56_16[] = { 1, 56, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_56_16[] = { 1, 16, 56, 16 };
  static const float buff_info_Conv2D_95_off_bias_out_124_quant_scale[] = { 0.100035294890404 };
  static const int16_t buff_info_Conv2D_95_off_bias_out_124_quant_offset[] = { -4 };
  static const uint32_t buff_info__shape_1_56_16_16[] = { 1, 16, 16, 56 };
  static const uint32_t buff_info__mem_shape_F_1_56_16_16[] = { 1, 56, 16, 16 };
  static const float buff_info_Transpose_93_out_0_quant_scale[] = { 0.0532370805740356 };
  static const int16_t buff_info_Transpose_93_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_56_16_16[] = { 1, 16, 16, 56 };
  static const float buff_info_Conv2D_98_off_bias_out_133_quant_scale[] = { 0.0653967633843422 };
  static const int16_t buff_info_Conv2D_98_off_bias_out_133_quant_offset[] = { 2 };
  static const float buff_info_Relu_104_out_0_quant_scale[] = { 0.0702017843723297 };
  static const int16_t buff_info_Relu_104_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_109_zero_off_out_136_quant_scale[] = { 0.0702017843723297 };
  static const int16_t buff_info_Conv2D_109_zero_off_out_136_quant_offset[] = { 0 };
  static const float buff_info_Transpose_105_out_0_quant_scale[] = { 0.0702017843723297 };
  static const int16_t buff_info_Transpose_105_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_16_64_16[] = { 1, 64, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_64_16[] = { 1, 16, 64, 16 };
  static const float buff_info_Conv2D_109_off_bias_out_142_quant_scale[] = { 0.13240884244442 };
  static const int16_t buff_info_Conv2D_109_off_bias_out_142_quant_offset[] = { -12 };
  static const uint32_t buff_info__shape_1_64_16_16[] = { 1, 16, 16, 64 };
  static const uint32_t buff_info__mem_shape_F_1_64_16_16[] = { 1, 64, 16, 16 };
  static const float buff_info_Transpose_107_out_0_quant_scale[] = { 0.0702017843723297 };
  static const int16_t buff_info_Transpose_107_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_64_16_16[] = { 1, 16, 16, 64 };
  static const float buff_info_Conv2D_112_off_bias_out_151_quant_scale[] = { 0.0810333266854286 };
  static const int16_t buff_info_Conv2D_112_off_bias_out_151_quant_offset[] = { 42 };
  static const float buff_info_Relu_118_out_0_quant_scale[] = { 0.051225870847702 };
  static const int16_t buff_info_Relu_118_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_123_zero_off_out_154_quant_scale[] = { 0.051225870847702 };
  static const int16_t buff_info_Conv2D_123_zero_off_out_154_quant_offset[] = { 0 };
  static const float buff_info_Transpose_119_out_0_quant_scale[] = { 0.051225870847702 };
  static const int16_t buff_info_Transpose_119_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_16_72_16[] = { 1, 72, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_72_16[] = { 1, 16, 72, 16 };
  static const float buff_info_Conv2D_123_off_bias_out_160_quant_scale[] = { 0.121414005756378 };
  static const int16_t buff_info_Conv2D_123_off_bias_out_160_quant_offset[] = { 6 };
  static const uint32_t buff_info__shape_1_72_16_16[] = { 1, 16, 16, 72 };
  static const uint32_t buff_info__mem_shape_F_1_72_16_16[] = { 1, 72, 16, 16 };
  static const float buff_info_Transpose_121_out_0_quant_scale[] = { 0.051225870847702 };
  static const int16_t buff_info_Transpose_121_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_72_16_16[] = { 1, 16, 16, 72 };
  static const float buff_info_Conv2D_126_off_bias_out_169_quant_scale[] = { 0.0814059898257256 };
  static const int16_t buff_info_Conv2D_126_off_bias_out_169_quant_offset[] = { 23 };
  static const float buff_info_Relu_132_out_0_quant_scale[] = { 0.0515139698982239 };
  static const int16_t buff_info_Relu_132_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_137_zero_off_out_172_quant_scale[] = { 0.0515139698982239 };
  static const int16_t buff_info_Conv2D_137_zero_off_out_172_quant_offset[] = { 0 };
  static const float buff_info_Transpose_133_out_0_quant_scale[] = { 0.0515139698982239 };
  static const int16_t buff_info_Transpose_133_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_16_80_16[] = { 1, 80, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_80_16[] = { 1, 16, 80, 16 };
  static const float buff_info_Conv2D_137_off_bias_out_178_quant_scale[] = { 0.165202870965004 };
  static const int16_t buff_info_Conv2D_137_off_bias_out_178_quant_offset[] = { 16 };
  static const uint32_t buff_info__shape_1_80_16_16[] = { 1, 16, 16, 80 };
  static const uint32_t buff_info__mem_shape_F_1_80_16_16[] = { 1, 80, 16, 16 };
  static const float buff_info_Transpose_135_out_0_quant_scale[] = { 0.0515139698982239 };
  static const int16_t buff_info_Transpose_135_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_80_16_16[] = { 1, 16, 16, 80 };
  static const float buff_info_Conv2D_140_off_bias_out_187_quant_scale[] = { 0.108723796904087 };
  static const int16_t buff_info_Conv2D_140_off_bias_out_187_quant_offset[] = { 31 };
  static const float buff_info_Relu_146_out_0_quant_scale[] = { 0.0534896068274975 };
  static const int16_t buff_info_Relu_146_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_151_zero_off_out_190_quant_scale[] = { 0.0534896068274975 };
  static const int16_t buff_info_Conv2D_151_zero_off_out_190_quant_offset[] = { 0 };
  static const float buff_info_Transpose_147_out_0_quant_scale[] = { 0.0534896068274975 };
  static const int16_t buff_info_Transpose_147_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_151_off_bias_out_196_quant_scale[] = { 0.188916921615601 };
  static const int16_t buff_info_Conv2D_151_off_bias_out_196_quant_offset[] = { 22 };
  static const uint32_t buff_info__shape_1_16_88_16[] = { 1, 88, 16, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_88_16[] = { 1, 16, 88, 16 };
  static const uint32_t buff_info__shape_1_88_16_16[] = { 1, 16, 16, 88 };
  static const uint32_t buff_info__mem_shape_F_1_88_16_16[] = { 1, 88, 16, 16 };
  static const float buff_info_Transpose_149_out_0_quant_scale[] = { 0.0534896068274975 };
  static const int16_t buff_info_Transpose_149_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_88_16_16[] = { 1, 16, 16, 88 };
  static const float buff_info_Conv2D_154_off_bias_out_205_quant_scale[] = { 0.127832233905792 };
  static const int16_t buff_info_Conv2D_154_off_bias_out_205_quant_offset[] = { 27 };
  static const uint32_t buff_info__shape_1_88_8_8[] = { 1, 8, 8, 88 };
  static const uint32_t buff_info__mem_shape_F_1_88_8_8[] = { 1, 88, 8, 8 };
  static const float buff_info_MaxPool_178_out_0_quant_scale[] = { 0.0792408511042595 };
  static const int16_t buff_info_MaxPool_178_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__shape_1_8_88_8[] = { 1, 88, 8, 8 };
  static const uint32_t buff_info__mem_shape_F_1_8_88_8[] = { 1, 8, 88, 8 };
  static const float buff_info_Transpose_179_out_0_quant_scale[] = { 0.0792408511042595 };
  static const int16_t buff_info_Transpose_179_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_168_zero_off_out_226_quant_scale[] = { 0.0792408511042595 };
  static const int16_t buff_info_Conv2D_168_zero_off_out_226_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_162_zero_off_out_208_quant_scale[] = { 0.0792408511042595 };
  static const int16_t buff_info_Conv2D_162_zero_off_out_208_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_246_zero_off_out_325_quant_scale[] = { 0.0792408511042595 };
  static const int16_t buff_info_Conv2D_246_zero_off_out_325_quant_offset[] = { 0 };
  static const uint32_t buff_info__mem_shape_L_1_88_8_8[] = { 1, 8, 8, 88 };
  static const float buff_info_Conv2D_162_off_bias_out_214_quant_scale[] = { 0.222410768270493 };
  static const int16_t buff_info_Conv2D_162_off_bias_out_214_quant_offset[] = { 6 };
  static const uint32_t buff_info__shape_1_32_16_16[] = { 1, 16, 16, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_16_16[] = { 1, 16, 16, 32 };
  static const float buff_info_Conv2D_168_off_bias_out_232_quant_scale[] = { 0.331603050231934 };
  static const int16_t buff_info_Conv2D_168_off_bias_out_232_quant_offset[] = { -39 };
  static const uint32_t buff_info__shape_1_16_16_32[] = { 1, 16, 32, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_16_32[] = { 1, 16, 16, 32 };
  static const float buff_info_Transpose_171_out_0_quant_scale[] = { 0.331603050231934 };
  static const int16_t buff_info_Transpose_171_out_0_quant_offset[] = { -39 };
  static const uint32_t buff_info__shape_1_8_96_8[] = { 1, 96, 8, 8 };
  static const uint32_t buff_info__mem_shape_F_1_8_96_8[] = { 1, 8, 96, 8 };
  static const uint32_t buff_info__shape_1_512_16_1[] = { 1, 16, 1, 512 };
  static const uint32_t buff_info__mem_shape_F_1_512_16_1[] = { 1, 512, 16, 1 };
  static const float buff_info_Reshape_172_out_0_inserted_out831_quant_scale[] = { 0.331603050231934 };
  static const int16_t buff_info_Reshape_172_out_0_inserted_out831_quant_offset[] = { -39 };
  static const uint32_t buff_info__mem_shape_L_1_512_16_1[] = { 1, 16, 1, 512 };
  static const float buff_info_Reshape_172_out_0_inserted_out831_inserted_out833_quant_scale[] = { 0.331603050231934 };
  static const int16_t buff_info_Reshape_172_out_0_inserted_out831_inserted_out833_quant_offset[] = { -39 };
  static const uint32_t buff_info__shape_1_16_1_512[] = { 1, 1, 512, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_1_512[] = { 1, 16, 1, 512 };
  static const float buff_info_Transpose_173_out_0_quant_scale[] = { 0.331603050231934 };
  static const int16_t buff_info_Transpose_173_out_0_quant_offset[] = { -39 };
  static const uint32_t buff_info__shape_1_16_512[] = { 1, 16, 512, 1 };
  static const uint32_t buff_info__mem_shape_F_1_16_512[] = { 1, 16, 512 };
  static const float buff_info_Transpose_173_out_0_cp_in_132_quant_scale[] = { 0.331603050231934 };
  static const int16_t buff_info_Transpose_173_out_0_cp_in_132_quant_offset[] = { -39 };
  static const uint32_t buff_info__shape_1_1_512[] = { 1, 1, 512, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_512[] = { 1, 1, 512 };
  static const float buff_info_Quantize_254_out_0_quant_scale[] = { 0.130844622850418 };
  static const int16_t buff_info_Quantize_254_out_0_quant_offset[] = { 65 };
  static const uint32_t buff_info__shape_1_96_8_8[] = { 1, 8, 8, 96 };
  static const uint32_t buff_info__mem_shape_F_1_96_8_8[] = { 1, 96, 8, 8 };
  static const float buff_info_Transpose_181_out_0_quant_scale[] = { 0.0792408511042595 };
  static const int16_t buff_info_Transpose_181_out_0_quant_offset[] = { -128 };
  static const uint32_t buff_info__mem_shape_L_1_96_8_8[] = { 1, 8, 8, 96 };
  static const float buff_info_Conv2D_165_off_bias_out_223_quant_scale[] = { 0.117102965712547 };
  static const int16_t buff_info_Conv2D_165_off_bias_out_223_quant_offset[] = { 37 };
  static const float buff_info_Quantize_176_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Quantize_176_out_0_quant_offset[] = { -57 };
  static const float buff_info_Relu_185_out_0_quant_scale[] = { 0.0787840858101845 };
  static const int16_t buff_info_Relu_185_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_187_zero_off_out_235_quant_scale[] = { 0.0787840858101845 };
  static const int16_t buff_info_Conv2D_187_zero_off_out_235_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_187_off_bias_out_241_quant_scale[] = { 0.164321094751358 };
  static const int16_t buff_info_Conv2D_187_off_bias_out_241_quant_offset[] = { 3 };
  static const float buff_info_Conv2D_190_off_bias_out_250_quant_scale[] = { 0.0995052829384804 };
  static const int16_t buff_info_Conv2D_190_off_bias_out_250_quant_offset[] = { 44 };
  static const float buff_info_Relu_196_out_0_quant_scale[] = { 0.0825336799025536 };
  static const int16_t buff_info_Relu_196_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_198_zero_off_out_253_quant_scale[] = { 0.0825336799025536 };
  static const int16_t buff_info_Conv2D_198_zero_off_out_253_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_198_off_bias_out_259_quant_scale[] = { 0.215295433998108 };
  static const int16_t buff_info_Conv2D_198_off_bias_out_259_quant_offset[] = { -2 };
  static const float buff_info_Conv2D_201_off_bias_out_268_quant_scale[] = { 0.11211421340704 };
  static const int16_t buff_info_Conv2D_201_off_bias_out_268_quant_offset[] = { 33 };
  static const float buff_info_Relu_207_out_0_quant_scale[] = { 0.100331827998161 };
  static const int16_t buff_info_Relu_207_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_209_zero_off_out_271_quant_scale[] = { 0.100331827998161 };
  static const int16_t buff_info_Conv2D_209_zero_off_out_271_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_209_off_bias_out_277_quant_scale[] = { 0.237292304635048 };
  static const int16_t buff_info_Conv2D_209_off_bias_out_277_quant_offset[] = { -7 };
  static const float buff_info_Conv2D_212_off_bias_out_286_quant_scale[] = { 0.109353758394718 };
  static const int16_t buff_info_Conv2D_212_off_bias_out_286_quant_offset[] = { 39 };
  static const float buff_info_Relu_218_out_0_quant_scale[] = { 0.103189162909985 };
  static const int16_t buff_info_Relu_218_out_0_quant_offset[] = { -128 };
  static const float buff_info_Conv2D_220_zero_off_out_289_quant_scale[] = { 0.103189162909985 };
  static const int16_t buff_info_Conv2D_220_zero_off_out_289_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_220_off_bias_out_295_quant_scale[] = { 0.265402525663376 };
  static const int16_t buff_info_Conv2D_220_off_bias_out_295_quant_offset[] = { 2 };
  static const float buff_info_Conv2D_223_off_bias_out_304_quant_scale[] = { 0.163399264216423 };
  static const int16_t buff_info_Conv2D_223_off_bias_out_304_quant_offset[] = { 49 };
  static const float buff_info_Conv2D_240_zero_off_out_316_quant_scale[] = { 0.105228684842587 };
  static const int16_t buff_info_Conv2D_240_zero_off_out_316_quant_offset[] = { 0 };
  static const float buff_info_Conv2D_230_zero_off_out_307_quant_scale[] = { 0.105228684842587 };
  static const int16_t buff_info_Conv2D_230_zero_off_out_307_quant_offset[] = { 0 };
  static const uint32_t buff_info__shape_1_6_8_8[] = { 1, 8, 8, 6 };
  static const uint32_t buff_info__mem_shape_L_1_6_8_8[] = { 1, 8, 8, 6 };
  static const float buff_info_Conv2D_240_off_bias_out_322_quant_scale[] = { 0.130844622850418 };
  static const int16_t buff_info_Conv2D_240_off_bias_out_322_quant_offset[] = { 65 };
  static const uint32_t buff_info__shape_1_8_8_6[] = { 1, 8, 6, 8 };
  static const uint32_t buff_info__mem_shape_F_1_8_8_6[] = { 1, 8, 8, 6 };
  static const float buff_info_Transpose_243_out_0_quant_scale[] = { 0.130844622850418 };
  static const int16_t buff_info_Transpose_243_out_0_quant_offset[] = { 65 };
  static const uint32_t buff_info__shape_1_1_384[] = { 1, 1, 384, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_384[] = { 1, 1, 384 };
  static const float buff_info_Transpose_245_out_0_quant_scale[] = { 0.130844622850418 };
  static const int16_t buff_info_Transpose_245_out_0_quant_offset[] = { 65 };
  static const float buff_info_Conv2D_230_off_bias_out_313_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Conv2D_230_off_bias_out_313_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_8_8_96[] = { 1, 8, 96, 8 };
  static const uint32_t buff_info__mem_shape_F_1_8_8_96[] = { 1, 8, 8, 96 };
  static const float buff_info_Transpose_233_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Transpose_233_out_0_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_384_16_1[] = { 1, 16, 1, 384 };
  static const uint32_t buff_info__mem_shape_F_1_384_16_1[] = { 1, 384, 16, 1 };
  static const float buff_info_Reshape_234_out_0_inserted_out848_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Reshape_234_out_0_inserted_out848_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_1_896[] = { 1, 1, 896, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_896[] = { 1, 1, 896 };
  static const float buff_info_Concat_256_out_0_quant_scale[] = { 0.130844622850418 };
  static const int16_t buff_info_Concat_256_out_0_quant_offset[] = { 65 };
  static const uint32_t buff_info__mem_shape_L_1_384_16_1[] = { 1, 16, 1, 384 };
  static const float buff_info_Reshape_234_out_0_inserted_out848_inserted_out850_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Reshape_234_out_0_inserted_out848_inserted_out850_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_16_1_384[] = { 1, 1, 384, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_1_384[] = { 1, 16, 1, 384 };
  static const float buff_info_Transpose_235_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Transpose_235_out_0_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_16_384[] = { 1, 16, 384, 1 };
  static const uint32_t buff_info__mem_shape_F_1_16_384[] = { 1, 16, 384 };
  static const float buff_info_Transpose_235_out_0_cp_in_145_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Transpose_235_out_0_cp_in_145_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_16_896[] = { 1, 16, 896, 1 };
  static const uint32_t buff_info__mem_shape_F_1_16_896[] = { 1, 16, 896 };
  static const float buff_info_Concat_236_out_0_quant_scale[] = { 0.728266358375549 };
  static const int16_t buff_info_Concat_236_out_0_quant_offset[] = { -57 };
  static const uint32_t buff_info__shape_1_16_896_1[] = { 1, 896, 1, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_896_1[] = { 1, 16, 896, 1 };
  static const uint32_t buff_info__mem_shape_L_1_16_896_1[] = { 1, 896, 1, 16 };
  static const uint32_t buff_info__shape_1_896_1_16[] = { 1, 1, 16, 896 };
  static const uint32_t buff_info__mem_shape_F_1_896_1_16[] = { 1, 896, 1, 16 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Transpose_1_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_128_128,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_128_128,
    },
    {
      .name = "Quantize_2_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 196608,
      .offset_end = 245760,
      .offset_limit = 245824,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_128_128,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_3_128_128,
      .per_channel = 0,
      .scale = buff_info_Quantize_2_out_0_quant_scale,
      .offset = buff_info_Quantize_2_out_0_quant_offset,
    },
    {
      .name = "Relu_7_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 98304,
      .offset_limit = 98368,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 4,
      .mem_shape = buff_info__mem_shape_M_1_24_64_64,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Relu_7_out_0_quant_scale,
      .offset = buff_info_Relu_7_out_0_quant_offset,
    },
    {
      .name = "Conv2D_9_zero_off_out_10",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 98304,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Conv2D_9_zero_off_out_10_quant_scale,
      .offset = buff_info_Conv2D_9_zero_off_out_10_quant_offset,
    },
    {
      .name = "Conv2D_9_off_bias_out_16",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 196608,
      .offset_end = 294912,
      .offset_limit = 294976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 4,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Conv2D_9_off_bias_out_16_quant_scale,
      .offset = buff_info_Conv2D_9_off_bias_out_16_quant_offset,
    },
    {
      .name = "Conv2D_12_off_bias_out_25",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 98304,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 5,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Conv2D_12_off_bias_out_25_quant_scale,
      .offset = buff_info_Conv2D_12_off_bias_out_25_quant_offset,
    },
    {
      .name = "Relu_18_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 196608,
      .offset_end = 294912,
      .offset_limit = 294976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Relu_18_out_0_quant_scale,
      .offset = buff_info_Relu_18_out_0_quant_offset,
    },
    {
      .name = "Conv2D_23_zero_off_out_28",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 294912,
      .offset_end = 393216,
      .offset_limit = 393280,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Conv2D_23_zero_off_out_28_quant_scale,
      .offset = buff_info_Conv2D_23_zero_off_out_28_quant_offset,
    },
    {
      .name = "Transpose_19_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 98304,
      .offset_limit = 98368,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 7,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_24_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_24_64,
      .per_channel = 0,
      .scale = buff_info_Transpose_19_out_0_quant_scale,
      .offset = buff_info_Transpose_19_out_0_quant_offset,
    },
    {
      .name = "Conv2D_23_off_bias_out_34",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 98304,
      .offset_end = 196608,
      .offset_limit = 196672,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 24,
      .mem_shape = buff_info__mem_shape_L_1_24_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_24_64_64,
      .per_channel = 0,
      .scale = buff_info_Conv2D_23_off_bias_out_34_quant_scale,
      .offset = buff_info_Conv2D_23_off_bias_out_34_quant_offset,
    },
    {
      .name = "Transpose_19_out_0_cp_in_147",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 98304,
      .offset_limit = 98368,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_24_64,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_24_64,
    },
    {
      .name = "Pad_20_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 229376,
      .offset_limit = 229440,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_28_64,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_28_64,
    },
    {
      .name = "Conv2D_26_off_bias_out_43",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
      .per_channel = 0,
      .scale = buff_info_Conv2D_26_off_bias_out_43_quant_scale,
      .offset = buff_info_Conv2D_26_off_bias_out_43_quant_offset,
    },
    {
      .name = "Transpose_21_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 229376,
      .offset_end = 344064,
      .offset_limit = 344128,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 10,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_28_64_64,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_28_64_64,
      .per_channel = 0,
      .scale = buff_info_Transpose_21_out_0_quant_scale,
      .offset = buff_info_Transpose_21_out_0_quant_offset,
    },
    {
      .name = "Conv2D_34_off_bias_out_52",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 143360,
      .offset_limit = 143424,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 28,
      .mem_shape = buff_info__mem_shape_L_1_28_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_28_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_34_off_bias_out_52_quant_scale,
      .offset = buff_info_Conv2D_34_off_bias_out_52_quant_offset,
    },
    {
      .name = "MaxPool_40_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 344064,
      .offset_end = 372736,
      .offset_limit = 372800,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_28_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_28_32_32,
      .per_channel = 0,
      .scale = buff_info_MaxPool_40_out_0_quant_scale,
      .offset = buff_info_MaxPool_40_out_0_quant_offset,
    },
    {
      .name = "Transpose_41_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 344064,
      .offset_end = 372736,
      .offset_limit = 372800,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_28_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_28_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_41_out_0_quant_scale,
      .offset = buff_info_Transpose_41_out_0_quant_offset,
    },
    {
      .name = "Pad_42_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 32768,
      .offset_end = 65536,
      .offset_limit = 65600,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
    },
    {
      .name = "Conv2D_37_off_bias_out_61",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 32768,
      .offset_limit = 32832,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_37_off_bias_out_61_quant_scale,
      .offset = buff_info_Conv2D_37_off_bias_out_61_quant_offset,
    },
    {
      .name = "Transpose_43_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 65536,
      .offset_end = 98304,
      .offset_limit = 98368,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_43_out_0_quant_scale,
      .offset = buff_info_Transpose_43_out_0_quant_offset,
    },
    {
      .name = "Relu_47_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 32768,
      .offset_end = 65536,
      .offset_limit = 65600,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
      .per_channel = 0,
      .scale = buff_info_Relu_47_out_0_quant_scale,
      .offset = buff_info_Relu_47_out_0_quant_offset,
    },
    {
      .name = "Conv2D_52_zero_off_out_64",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 98304,
      .offset_end = 131072,
      .offset_limit = 131136,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_52_zero_off_out_64_quant_scale,
      .offset = buff_info_Conv2D_52_zero_off_out_64_quant_offset,
    },
    {
      .name = "Transpose_48_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 65536,
      .offset_end = 98304,
      .offset_limit = 98368,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 15,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_48_out_0_quant_scale,
      .offset = buff_info_Transpose_48_out_0_quant_offset,
    },
    {
      .name = "Conv2D_52_off_bias_out_70",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 131072,
      .offset_end = 163840,
      .offset_limit = 163904,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_52_off_bias_out_70_quant_scale,
      .offset = buff_info_Conv2D_52_off_bias_out_70_quant_offset,
    },
    {
      .name = "Pad_49_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_36_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_36_32,
    },
    {
      .name = "Transpose_50_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 36864,
      .offset_end = 73728,
      .offset_limit = 73792,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 17,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_50_out_0_quant_scale,
      .offset = buff_info_Transpose_50_out_0_quant_offset,
    },
    {
      .name = "Conv2D_55_off_bias_out_79",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 18,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_55_off_bias_out_79_quant_scale,
      .offset = buff_info_Conv2D_55_off_bias_out_79_quant_offset,
    },
    {
      .name = "Relu_61_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 73728,
      .offset_end = 110592,
      .offset_limit = 110656,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
      .per_channel = 0,
      .scale = buff_info_Relu_61_out_0_quant_scale,
      .offset = buff_info_Relu_61_out_0_quant_offset,
    },
    {
      .name = "Conv2D_66_off_bias_out_88",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 110592,
      .offset_end = 147456,
      .offset_limit = 147520,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 36,
      .mem_shape = buff_info__mem_shape_L_1_36_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_36_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_66_off_bias_out_88_quant_scale,
      .offset = buff_info_Conv2D_66_off_bias_out_88_quant_offset,
    },
    {
      .name = "Transpose_62_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 147456,
      .offset_end = 184320,
      .offset_limit = 184384,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 20,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_36_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_36_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_62_out_0_quant_scale,
      .offset = buff_info_Transpose_62_out_0_quant_offset,
    },
    {
      .name = "Pad_63_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 43008,
      .offset_end = 86016,
      .offset_limit = 86080,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_42_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_42_32,
    },
    {
      .name = "Conv2D_69_off_bias_out_97",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 43008,
      .offset_limit = 43072,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_42_32_32,
      .per_channel = 0,
      .scale = buff_info_Conv2D_69_off_bias_out_97_quant_scale,
      .offset = buff_info_Conv2D_69_off_bias_out_97_quant_offset,
    },
    {
      .name = "Transpose_64_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 86016,
      .offset_end = 129024,
      .offset_limit = 129088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_42_32_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_42_32_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_64_out_0_quant_scale,
      .offset = buff_info_Transpose_64_out_0_quant_offset,
    },
    {
      .name = "Conv2D_77_off_bias_out_106",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 53760,
      .offset_end = 64512,
      .offset_limit = 64576,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 42,
      .mem_shape = buff_info__mem_shape_L_1_42_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_42_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_77_off_bias_out_106_quant_scale,
      .offset = buff_info_Conv2D_77_off_bias_out_106_quant_offset,
    },
    {
      .name = "MaxPool_83_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 43008,
      .offset_end = 53760,
      .offset_limit = 53824,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_42_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_42_16_16,
      .per_channel = 0,
      .scale = buff_info_MaxPool_83_out_0_quant_scale,
      .offset = buff_info_MaxPool_83_out_0_quant_offset,
    },
    {
      .name = "Transpose_84_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 43008,
      .offset_end = 53760,
      .offset_limit = 53824,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_42_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_42_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_84_out_0_quant_scale,
      .offset = buff_info_Transpose_84_out_0_quant_offset,
    },
    {
      .name = "Pad_85_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 24,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_48_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_48_16,
    },
    {
      .name = "Conv2D_80_off_bias_out_115",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 12288,
      .offset_limit = 12352,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 24,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_80_off_bias_out_115_quant_scale,
      .offset = buff_info_Conv2D_80_off_bias_out_115_quant_offset,
    },
    {
      .name = "Transpose_86_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 24576,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 25,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_86_out_0_quant_scale,
      .offset = buff_info_Transpose_86_out_0_quant_offset,
    },
    {
      .name = "Relu_90_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 26,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
      .per_channel = 0,
      .scale = buff_info_Relu_90_out_0_quant_scale,
      .offset = buff_info_Relu_90_out_0_quant_offset,
    },
    {
      .name = "Conv2D_95_zero_off_out_118",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 36864,
      .offset_end = 49152,
      .offset_limit = 49216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 26,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_95_zero_off_out_118_quant_scale,
      .offset = buff_info_Conv2D_95_zero_off_out_118_quant_offset,
    },
    {
      .name = "Transpose_91_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 24576,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 27,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_48_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_48_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_91_out_0_quant_scale,
      .offset = buff_info_Transpose_91_out_0_quant_offset,
    },
    {
      .name = "Pad_92_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 14336,
      .offset_limit = 14400,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 28,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_56_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_56_16,
    },
    {
      .name = "Conv2D_95_off_bias_out_124",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 49152,
      .offset_end = 61440,
      .offset_limit = 61504,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 28,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_48_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_95_off_bias_out_124_quant_scale,
      .offset = buff_info_Conv2D_95_off_bias_out_124_quant_offset,
    },
    {
      .name = "Transpose_93_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 14336,
      .offset_end = 28672,
      .offset_limit = 28736,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 29,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_93_out_0_quant_scale,
      .offset = buff_info_Transpose_93_out_0_quant_offset,
    },
    {
      .name = "Conv2D_98_off_bias_out_133",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 14336,
      .offset_limit = 14400,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 30,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_98_off_bias_out_133_quant_scale,
      .offset = buff_info_Conv2D_98_off_bias_out_133_quant_offset,
    },
    {
      .name = "Relu_104_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 28672,
      .offset_end = 43008,
      .offset_limit = 43072,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 31,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
      .per_channel = 0,
      .scale = buff_info_Relu_104_out_0_quant_scale,
      .offset = buff_info_Relu_104_out_0_quant_offset,
    },
    {
      .name = "Conv2D_109_zero_off_out_136",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 43008,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 31,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_109_zero_off_out_136_quant_scale,
      .offset = buff_info_Conv2D_109_zero_off_out_136_quant_offset,
    },
    {
      .name = "Transpose_105_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 71680,
      .offset_limit = 71744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 32,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_56_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_56_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_105_out_0_quant_scale,
      .offset = buff_info_Transpose_105_out_0_quant_offset,
    },
    {
      .name = "Pad_106_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 16384,
      .offset_limit = 16448,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_64_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_64_16,
    },
    {
      .name = "Conv2D_109_off_bias_out_142",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 71680,
      .offset_end = 86016,
      .offset_limit = 86080,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 56,
      .mem_shape = buff_info__mem_shape_L_1_56_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_56_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_109_off_bias_out_142_quant_scale,
      .offset = buff_info_Conv2D_109_off_bias_out_142_quant_offset,
    },
    {
      .name = "Transpose_107_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 16384,
      .offset_end = 32768,
      .offset_limit = 32832,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 34,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_107_out_0_quant_scale,
      .offset = buff_info_Transpose_107_out_0_quant_offset,
    },
    {
      .name = "Conv2D_112_off_bias_out_151",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 16384,
      .offset_limit = 16448,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 35,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_112_off_bias_out_151_quant_scale,
      .offset = buff_info_Conv2D_112_off_bias_out_151_quant_offset,
    },
    {
      .name = "Relu_118_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 32768,
      .offset_end = 49152,
      .offset_limit = 49216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 36,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
      .per_channel = 0,
      .scale = buff_info_Relu_118_out_0_quant_scale,
      .offset = buff_info_Relu_118_out_0_quant_offset,
    },
    {
      .name = "Conv2D_123_zero_off_out_154",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 53248,
      .offset_end = 69632,
      .offset_limit = 69696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 36,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_123_zero_off_out_154_quant_scale,
      .offset = buff_info_Conv2D_123_zero_off_out_154_quant_offset,
    },
    {
      .name = "Transpose_119_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 69632,
      .offset_end = 86016,
      .offset_limit = 86080,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 37,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_64_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_64_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_119_out_0_quant_scale,
      .offset = buff_info_Transpose_119_out_0_quant_offset,
    },
    {
      .name = "Pad_120_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 38,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_72_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_72_16,
    },
    {
      .name = "Conv2D_123_off_bias_out_160",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 36864,
      .offset_end = 53248,
      .offset_limit = 53312,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 38,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_123_off_bias_out_160_quant_scale,
      .offset = buff_info_Conv2D_123_off_bias_out_160_quant_offset,
    },
    {
      .name = "Transpose_121_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 36864,
      .offset_limit = 36928,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 39,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_121_out_0_quant_scale,
      .offset = buff_info_Transpose_121_out_0_quant_offset,
    },
    {
      .name = "Conv2D_126_off_bias_out_169",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 40,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_126_off_bias_out_169_quant_scale,
      .offset = buff_info_Conv2D_126_off_bias_out_169_quant_offset,
    },
    {
      .name = "Relu_132_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 36864,
      .offset_end = 55296,
      .offset_limit = 55360,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 41,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
      .per_channel = 0,
      .scale = buff_info_Relu_132_out_0_quant_scale,
      .offset = buff_info_Relu_132_out_0_quant_offset,
    },
    {
      .name = "Conv2D_137_zero_off_out_172",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 55296,
      .offset_end = 73728,
      .offset_limit = 73792,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 41,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_137_zero_off_out_172_quant_scale,
      .offset = buff_info_Conv2D_137_zero_off_out_172_quant_offset,
    },
    {
      .name = "Transpose_133_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 73728,
      .offset_end = 92160,
      .offset_limit = 92224,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 42,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_72_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_72_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_133_out_0_quant_scale,
      .offset = buff_info_Transpose_133_out_0_quant_offset,
    },
    {
      .name = "Pad_134_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 20480,
      .offset_limit = 20544,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 43,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_80_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_16,
    },
    {
      .name = "Conv2D_137_off_bias_out_178",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 92160,
      .offset_end = 110592,
      .offset_limit = 110656,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 43,
      .batch = 72,
      .mem_shape = buff_info__mem_shape_L_1_72_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_72_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_137_off_bias_out_178_quant_scale,
      .offset = buff_info_Conv2D_137_off_bias_out_178_quant_offset,
    },
    {
      .name = "Transpose_135_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 20480,
      .offset_end = 40960,
      .offset_limit = 41024,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 44,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_135_out_0_quant_scale,
      .offset = buff_info_Transpose_135_out_0_quant_offset,
    },
    {
      .name = "Conv2D_140_off_bias_out_187",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 20480,
      .offset_limit = 20544,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 45,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_140_off_bias_out_187_quant_scale,
      .offset = buff_info_Conv2D_140_off_bias_out_187_quant_offset,
    },
    {
      .name = "Relu_146_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 40960,
      .offset_end = 61440,
      .offset_limit = 61504,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
      .per_channel = 0,
      .scale = buff_info_Relu_146_out_0_quant_scale,
      .offset = buff_info_Relu_146_out_0_quant_offset,
    },
    {
      .name = "Conv2D_151_zero_off_out_190",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 61440,
      .offset_end = 81920,
      .offset_limit = 81984,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_151_zero_off_out_190_quant_scale,
      .offset = buff_info_Conv2D_151_zero_off_out_190_quant_offset,
    },
    {
      .name = "Transpose_147_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 81920,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 47,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_80_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_147_out_0_quant_scale,
      .offset = buff_info_Transpose_147_out_0_quant_offset,
    },
    {
      .name = "Conv2D_151_off_bias_out_196",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 122880,
      .offset_limit = 122944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 48,
      .batch = 80,
      .mem_shape = buff_info__mem_shape_L_1_80_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_80_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_151_off_bias_out_196_quant_scale,
      .offset = buff_info_Conv2D_151_off_bias_out_196_quant_offset,
    },
    {
      .name = "Pad_148_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 22528,
      .offset_limit = 22592,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 48,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_88_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_88_16,
    },
    {
      .name = "Transpose_149_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 22528,
      .offset_end = 45056,
      .offset_limit = 45120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 49,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
      .per_channel = 0,
      .scale = buff_info_Transpose_149_out_0_quant_scale,
      .offset = buff_info_Transpose_149_out_0_quant_offset,
    },
    {
      .name = "Conv2D_154_off_bias_out_205",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 45056,
      .offset_end = 67584,
      .offset_limit = 67648,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 50,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_154_off_bias_out_205_quant_scale,
      .offset = buff_info_Conv2D_154_off_bias_out_205_quant_offset,
    },
    {
      .name = "MaxPool_178_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 112640,
      .offset_end = 118272,
      .offset_limit = 118336,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_88_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_8_8,
      .per_channel = 0,
      .scale = buff_info_MaxPool_178_out_0_quant_scale,
      .offset = buff_info_MaxPool_178_out_0_quant_offset,
    },
    {
      .name = "Transpose_179_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 112640,
      .offset_end = 118272,
      .offset_limit = 118336,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_8_88_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_8_88_8,
      .per_channel = 0,
      .scale = buff_info_Transpose_179_out_0_quant_scale,
      .offset = buff_info_Transpose_179_out_0_quant_offset,
    },
    {
      .name = "Conv2D_168_zero_off_out_226",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 90112,
      .offset_end = 112640,
      .offset_limit = 112704,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_168_zero_off_out_226_quant_scale,
      .offset = buff_info_Conv2D_168_zero_off_out_226_quant_offset,
    },
    {
      .name = "Conv2D_162_zero_off_out_208",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 67584,
      .offset_end = 90112,
      .offset_limit = 90176,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_162_zero_off_out_208_quant_scale,
      .offset = buff_info_Conv2D_162_zero_off_out_208_quant_offset,
    },
    {
      .name = "Conv2D_246_zero_off_out_325",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 22528,
      .offset_limit = 22592,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_246_zero_off_out_325_quant_scale,
      .offset = buff_info_Conv2D_246_zero_off_out_325_quant_offset,
    },
    {
      .name = "Conv2D_162_off_bias_out_214",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 47104,
      .offset_end = 52736,
      .offset_limit = 52800,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 88,
      .mem_shape = buff_info__mem_shape_L_1_88_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_88_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_162_off_bias_out_214_quant_scale,
      .offset = buff_info_Conv2D_162_off_bias_out_214_quant_offset,
    },
    {
      .name = "Conv2D_168_off_bias_out_232",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 22528,
      .offset_end = 30720,
      .offset_limit = 30784,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_16_16,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_16_16,
      .per_channel = 0,
      .scale = buff_info_Conv2D_168_off_bias_out_232_quant_scale,
      .offset = buff_info_Conv2D_168_off_bias_out_232_quant_offset,
    },
    {
      .name = "Transpose_171_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 22528,
      .offset_end = 30720,
      .offset_limit = 30784,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_16_32,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_16_32,
      .per_channel = 0,
      .scale = buff_info_Transpose_171_out_0_quant_scale,
      .offset = buff_info_Transpose_171_out_0_quant_offset,
    },
    {
      .name = "Pad_180_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 38912,
      .offset_end = 45056,
      .offset_limit = 45120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_8_96_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_8_96_8,
    },
    {
      .name = "Reshape_172_out_0_inserted_out831",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 22528,
      .offset_end = 30720,
      .offset_limit = 30784,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_512_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_16_1,
      .per_channel = 0,
      .scale = buff_info_Reshape_172_out_0_inserted_out831_quant_scale,
      .offset = buff_info_Reshape_172_out_0_inserted_out831_quant_offset,
    },
    {
      .name = "Reshape_172_out_0_inserted_out831_inserted_out833",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 30720,
      .offset_end = 38912,
      .offset_limit = 38976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_16_1,
      .per_channel = 0,
      .scale = buff_info_Reshape_172_out_0_inserted_out831_inserted_out833_quant_scale,
      .offset = buff_info_Reshape_172_out_0_inserted_out831_inserted_out833_quant_offset,
    },
    {
      .name = "Transpose_173_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 30720,
      .offset_end = 38912,
      .offset_limit = 38976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_1_512,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_512,
      .per_channel = 0,
      .scale = buff_info_Transpose_173_out_0_quant_scale,
      .offset = buff_info_Transpose_173_out_0_quant_offset,
    },
    {
      .name = "Transpose_173_out_0_cp_in_132",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 30720,
      .offset_end = 38912,
      .offset_limit = 38976,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_512,
      .per_channel = 0,
      .scale = buff_info_Transpose_173_out_0_cp_in_132_quant_scale,
      .offset = buff_info_Transpose_173_out_0_cp_in_132_quant_offset,
    },
    {
      .name = "Quantize_254_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 53248,
      .offset_end = 53760,
      .offset_limit = 53824,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1_512,
      .per_channel = 0,
      .scale = buff_info_Quantize_254_out_0_quant_scale,
      .offset = buff_info_Quantize_254_out_0_quant_offset,
    },
    {
      .name = "Transpose_181_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 12288,
      .offset_limit = 12352,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 54,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Transpose_181_out_0_quant_scale,
      .offset = buff_info_Transpose_181_out_0_quant_offset,
    },
    {
      .name = "Conv2D_165_off_bias_out_223",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 55,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_165_off_bias_out_223_quant_scale,
      .offset = buff_info_Conv2D_165_off_bias_out_223_quant_offset,
    },
    {
      .name = "Quantize_176_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 38912,
      .offset_end = 47104,
      .offset_limit = 47168,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 55,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_512,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_512,
      .per_channel = 0,
      .scale = buff_info_Quantize_176_out_0_quant_scale,
      .offset = buff_info_Quantize_176_out_0_quant_offset,
    },
    {
      .name = "Relu_185_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 56,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Relu_185_out_0_quant_scale,
      .offset = buff_info_Relu_185_out_0_quant_offset,
    },
    {
      .name = "Conv2D_187_zero_off_out_235",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 56,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_187_zero_off_out_235_quant_scale,
      .offset = buff_info_Conv2D_187_zero_off_out_235_quant_offset,
    },
    {
      .name = "Conv2D_187_off_bias_out_241",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 12288,
      .offset_limit = 12352,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 57,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_187_off_bias_out_241_quant_scale,
      .offset = buff_info_Conv2D_187_off_bias_out_241_quant_offset,
    },
    {
      .name = "Conv2D_190_off_bias_out_250",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 58,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_190_off_bias_out_250_quant_scale,
      .offset = buff_info_Conv2D_190_off_bias_out_250_quant_offset,
    },
    {
      .name = "Relu_196_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 12288,
      .offset_limit = 12352,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 59,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Relu_196_out_0_quant_scale,
      .offset = buff_info_Relu_196_out_0_quant_offset,
    },
    {
      .name = "Conv2D_198_zero_off_out_253",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 59,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_198_zero_off_out_253_quant_scale,
      .offset = buff_info_Conv2D_198_zero_off_out_253_quant_offset,
    },
    {
      .name = "Conv2D_198_off_bias_out_259",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 60,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_198_off_bias_out_259_quant_scale,
      .offset = buff_info_Conv2D_198_off_bias_out_259_quant_offset,
    },
    {
      .name = "Conv2D_201_off_bias_out_268",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 61,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_201_off_bias_out_268_quant_scale,
      .offset = buff_info_Conv2D_201_off_bias_out_268_quant_offset,
    },
    {
      .name = "Relu_207_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 62,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Relu_207_out_0_quant_scale,
      .offset = buff_info_Relu_207_out_0_quant_offset,
    },
    {
      .name = "Conv2D_209_zero_off_out_271",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 62,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_209_zero_off_out_271_quant_scale,
      .offset = buff_info_Conv2D_209_zero_off_out_271_quant_offset,
    },
    {
      .name = "Conv2D_209_off_bias_out_277",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 12288,
      .offset_limit = 12352,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 63,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_209_off_bias_out_277_quant_scale,
      .offset = buff_info_Conv2D_209_off_bias_out_277_quant_offset,
    },
    {
      .name = "Conv2D_212_off_bias_out_286",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 64,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_212_off_bias_out_286_quant_scale,
      .offset = buff_info_Conv2D_212_off_bias_out_286_quant_offset,
    },
    {
      .name = "Relu_218_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 12288,
      .offset_limit = 12352,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 65,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Relu_218_out_0_quant_scale,
      .offset = buff_info_Relu_218_out_0_quant_offset,
    },
    {
      .name = "Conv2D_220_zero_off_out_289",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 65,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_220_zero_off_out_289_quant_scale,
      .offset = buff_info_Conv2D_220_zero_off_out_289_quant_offset,
    },
    {
      .name = "Conv2D_220_off_bias_out_295",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 66,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_220_off_bias_out_295_quant_scale,
      .offset = buff_info_Conv2D_220_off_bias_out_295_quant_offset,
    },
    {
      .name = "Conv2D_223_off_bias_out_304",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 12288,
      .offset_end = 18432,
      .offset_limit = 18496,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 67,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_223_off_bias_out_304_quant_scale,
      .offset = buff_info_Conv2D_223_off_bias_out_304_quant_offset,
    },
    {
      .name = "Conv2D_240_zero_off_out_316",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 18432,
      .offset_end = 24576,
      .offset_limit = 24640,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 68,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_240_zero_off_out_316_quant_scale,
      .offset = buff_info_Conv2D_240_zero_off_out_316_quant_offset,
    },
    {
      .name = "Conv2D_230_zero_off_out_307",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 24576,
      .offset_end = 30720,
      .offset_limit = 30784,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 68,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 8,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_UINT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_230_zero_off_out_307_quant_scale,
      .offset = buff_info_Conv2D_230_zero_off_out_307_quant_offset,
    },
    {
      .name = "Conv2D_240_off_bias_out_322",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 53760,
      .offset_end = 54144,
      .offset_limit = 54208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 6,
      .mem_shape = buff_info__mem_shape_L_1_6_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_6_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_240_off_bias_out_322_quant_scale,
      .offset = buff_info_Conv2D_240_off_bias_out_322_quant_offset,
    },
    {
      .name = "Transpose_243_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 53760,
      .offset_end = 54144,
      .offset_limit = 54208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_8_8_6,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_8_8_6,
      .per_channel = 0,
      .scale = buff_info_Transpose_243_out_0_quant_scale,
      .offset = buff_info_Transpose_243_out_0_quant_offset,
    },
    {
      .name = "Transpose_245_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 53760,
      .offset_end = 54144,
      .offset_limit = 54208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_384,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1_384,
      .per_channel = 0,
      .scale = buff_info_Transpose_245_out_0_quant_scale,
      .offset = buff_info_Transpose_245_out_0_quant_offset,
    },
    {
      .name = "Conv2D_230_off_bias_out_313",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_8_8,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_96_8_8,
      .per_channel = 0,
      .scale = buff_info_Conv2D_230_off_bias_out_313_quant_scale,
      .offset = buff_info_Conv2D_230_off_bias_out_313_quant_offset,
    },
    {
      .name = "Transpose_233_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_8_8_96,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_8_8_96,
      .per_channel = 0,
      .scale = buff_info_Transpose_233_out_0_quant_scale,
      .offset = buff_info_Transpose_233_out_0_quant_offset,
    },
    {
      .name = "Reshape_234_out_0_inserted_out848",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 6144,
      .offset_limit = 6208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_384_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_384_16_1,
      .per_channel = 0,
      .scale = buff_info_Reshape_234_out_0_inserted_out848_quant_scale,
      .offset = buff_info_Reshape_234_out_0_inserted_out848_quant_offset,
    },
    {
      .name = "Concat_256_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 6144,
      .offset_end = 7040,
      .offset_limit = 7104,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1_896,
      .per_channel = 0,
      .scale = buff_info_Concat_256_out_0_quant_scale,
      .offset = buff_info_Concat_256_out_0_quant_offset,
    },
    {
      .name = "Reshape_234_out_0_inserted_out848_inserted_out850",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 47104,
      .offset_end = 53248,
      .offset_limit = 53312,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_L_1_384_16_1,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_384_16_1,
      .per_channel = 0,
      .scale = buff_info_Reshape_234_out_0_inserted_out848_inserted_out850_quant_scale,
      .offset = buff_info_Reshape_234_out_0_inserted_out848_inserted_out850_quant_offset,
    },
    {
      .name = "Transpose_235_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 47104,
      .offset_end = 53248,
      .offset_limit = 53312,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_1_384,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_384,
      .per_channel = 0,
      .scale = buff_info_Transpose_235_out_0_quant_scale,
      .offset = buff_info_Transpose_235_out_0_quant_offset,
    },
    {
      .name = "Transpose_235_out_0_cp_in_145",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 47104,
      .offset_end = 53248,
      .offset_limit = 53312,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_384,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_384,
      .per_channel = 0,
      .scale = buff_info_Transpose_235_out_0_cp_in_145_quant_scale,
      .offset = buff_info_Transpose_235_out_0_cp_in_145_quant_offset,
    },
    {
      .name = "Dequantize_258_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 114688,
      .offset_end = 118272,
      .offset_limit = 118336,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 71,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_896,
    },
    {
      .name = "Concat_236_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 71680,
      .offset_limit = 71744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896,
      .per_channel = 0,
      .scale = buff_info_Concat_236_out_0_quant_scale,
      .offset = buff_info_Concat_236_out_0_quant_offset,
    },
    {
      .name = "Dequantize_238_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 73,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_896,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896,
    },
    {
      .name = "Dequantize_238_out_0_inserted_out851",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 57344,
      .offset_limit = 57408,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 74,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_896_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896_1,
    },
    {
      .name = "Dequantize_238_out_0_inserted_out851_inserted_out853",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 75,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_896_1,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_896_1,
    },
    {
      .name = "Transpose_239_out_0_cp_in_146",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 57344,
      .offset_end = 114688,
      .offset_limit = 114752,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 75,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_896_1_16,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_896_1_16,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

