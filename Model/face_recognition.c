/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    face_recognition.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "27f5d5bcb9ca9522b73a54d7aec841664ee448cd"
 * GIT_BRANCH      "STAI-2.1"
 * GIT_DESCRIPTION "atonn-v1.1.0-31-g27f5d5bcb"
 *
 * Command Line options:
 * --onnx-input = "C:/Users/pele/.stm32cubemx/network_output/face_recognition_sface_2021dec_PerChannel_quant_random_1_OE_3_2_0.onnx"
 * --out-dir-prefix = "C:/Users/pele/AppData/Local/Temp/mxAI_workspace7281123301390002117569074674884898/neural_art__face_recognition/"
 * --network-name = "face_recognition"
 * --all-buffers-info = true
 * --mvei = true
 * --load-mdesc-file = "C:/ST/STEdgeAI/2.1/Utilities/configs/stm32n6"
 * --load-mpool-file = "C:/ST/STEdgeAI/2.1/scripts/N6_scripts/my_mpools/stm32n6_n6-allmems-O3"
 * --cache-maintenance = true
 * --enable-virtual-mem-pools = true
 * --native-float = true
 * --json-quant-file = "C:/Users/pele/.stm32cubemx/network_output/face_recognition_sface_2021dec_PerChannel_quant_random_1_OE_3_2_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --enable-epoch-controller = true
 * --output-info-file = "c_info"
 * --Oauto-sched = true
 *
 * auto* option expanded into:
 *   alt-scheduler = false
 */

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"
#include "ecloader.h"

#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 0 || LL_ATON_VERSION_DEV != 31
#  warning "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 7 is 6.12 MB */
/* index=7 file postfix=xSPI1 name=hyperRAM offset=0x90000000  absolute_mode size=33554424 READ_WRITE THROUGHPUT=MID LATENCY=HIGH byte width=2 freq ratio=5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=380 write_power=340 use4initializers=YES score=82  */
/* global pool 8 is 9.72 MB */
/* index=8 file postfix=xSPI2 name=octoFlash offset=0x70380000  absolute_mode size=117440504 READ_ONLY THROUGHPUT=MID LATENCY=HIGH byte width=1 freq ratio=6 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=110 write_power=400 use4initializers=YES score=50  */
/* global pool 1 is 428.75 KB */
/* index=1 file postfix=AXISRAM5 name=npuRAM5 offset=0x342e0000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 2 is 392.00 KB */
/* index=2 file postfix=AXISRAM4 name=npuRAM4 offset=0x34270000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 3 is 448.00 KB */
/* index=3 file postfix=AXISRAM3 name=npuRAM3 offset=0x34200000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 0 is ? */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458744 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */
/* global pool 11 is 2.29 MB */
/* index=11 file postfix=AXISRAM2_AXISRAM3_AXISRAM4_AXISRAM5_AXISRAM6 name=cpuRAM2_npuRAM3_npuRAM4_npuRAM5_npuRAM6 offset=0x34100000  absolute_mode size=2883576 vpool READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=16.201 use4initializers=NO score=85  */
/* global pool 4 is 1.00 MB */
/* index=4 file postfix=AXISRAM2 name=cpuRAM2 offset=0x34100000  absolute_mode size=1048576 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=17.324 write_power=15.321 use4initializers=NO score=84  */
/* global pool 5 is ? */
/* index=5 file postfix=AXISRAM1 name=cpuRAM1 offset=0x34064000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=16.616 write_power=14.522 use4initializers=NO score=84  */
/* global pool 6 is ? */
/* index=6 file postfix=AXIFLEXMEM name=flexMEM offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=9.381 write_power=8.569 use4initializers=NO score=84  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_face_recognition(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_face_recognition(uint32_t num)
{
  { 
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_face_recognition(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_face_recognition(uint32_t num)
{
  { 
    return NULL;
  }
}

#include "face_recognition_ecblobs.h"

/* scheduling epoch=0    nodes=345 ------------------------------------------------------------------- */

// Epoch Controller Blob (name='_ec_blob_1') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_1') start function
static void _ec_blob_cache_start_func_1(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 301056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) /* Equivalent hex address = 0x34304c00UL */, 150528);

};


/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_2(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sub node=Sub_3 */
  Arith_sw_info arith1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 3,
    .general.input.dim.num_elem = 37632,
    .general.input.stride.b = 150528,
    .general.input.stride.h = 1344,
    .general.input.stride.w = 12,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) /* Equivalent hex address = 0x34304c00UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 1,
    .operand.stride.b = 4,
    .operand.stride.h = 4,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194416))) /* Equivalent hex address = 0x70d38df0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 3,
    .general.output.dim.num_elem = 37632,
    .general.output.stride.b = 150528,
    .general.output.stride.h = 1344,
    .general.output.stride.w = 12,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHSUB,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sub_3 mapped on EmbedNets (FLOAT) as Sub | Category: Computational */
  ll_sw_forward_arith(&arith1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 150528);

}


/* scheduling epoch=3    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_3(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id608 */
  Quantizelinear_sw_info quantizelinear2_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 3,
    .general.input.dim.num_elem = 37632,
    .general.input.stride.b = 150528,
    .general.input.stride.h = 1344,
    .general.input.stride.w = 12,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194704))) /* Equivalent hex address = 0x70d38f10UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195632))) /* Equivalent hex address = 0x70d392b0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 3,
    .general.output.dim.num_elem = 37632,
    .general.output.stride.b = 37632,
    .general.output.stride.h = 336,
    .general.output.stride.w = 3,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) /* Equivalent hex address = 0x34304c00UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id608 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear2_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 188160))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 150528))) /* Equivalent hex address = 0x34304c00UL */, 37632);

}


// Epoch Controller Blob (name='_ec_blob_4') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_4') start function
static void _ec_blob_cache_start_func_4(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

};


/* scheduling epoch=6    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_6(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id609 */
  Dequantizelinear_sw_info dequantizelinear3_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 32,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193856))) /* Equivalent hex address = 0x70d38bc0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194800))) /* Equivalent hex address = 0x70d38f70UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id609 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear3_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1605632);

}


/* scheduling epoch=7    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_7(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_10 */
  Bn_sw_info bn4_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 32,
    .scale.dim.num_elem = 32,
    .scale.stride.b = 128,
    .scale.stride.h = 128,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10192784))) /* Equivalent hex address = 0x70d38790UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10192912))) /* Equivalent hex address = 0x70d38810UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 32,
    .mean.dim.num_elem = 32,
    .mean.stride.b = 128,
    .mean.stride.h = 128,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193040))) /* Equivalent hex address = 0x70d38890UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 32,
    .var.dim.num_elem = 32,
    .var.stride.b = 128,
    .var.stride.h = 128,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193168))) /* Equivalent hex address = 0x70d38910UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_10 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn4_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1605632);

}


/* scheduling epoch=8    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_8(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_11 */
  Activ_sw_info activ5_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 33,
    .operand.dim.num_elem = 33,
    .operand.stride.b = 132,
    .operand.stride.h = 132,
    .operand.stride.w = 132,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10192368))) /* Equivalent hex address = 0x70d385f0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_11 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ5_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */, 1605632);

}


/* scheduling epoch=9    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_9(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id610 */
  Quantizelinear_sw_info quantizelinear6_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194016))) /* Equivalent hex address = 0x70d38c60UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194960))) /* Equivalent hex address = 0x70d39010UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 32,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id610 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear6_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


// Epoch Controller Blob (name='_ec_blob_10') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_10') start function
static void _ec_blob_cache_start_func_10(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

};


/* scheduling epoch=11   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_11(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id611 */
  Dequantizelinear_sw_info dequantizelinear7_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 32,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193872))) /* Equivalent hex address = 0x70d38bd0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194816))) /* Equivalent hex address = 0x70d38f80UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id611 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear7_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1605632);

}


/* scheduling epoch=12   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_12(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_16 */
  Bn_sw_info bn8_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 32,
    .scale.dim.num_elem = 32,
    .scale.stride.b = 128,
    .scale.stride.h = 128,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193296))) /* Equivalent hex address = 0x70d38990UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193424))) /* Equivalent hex address = 0x70d38a10UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 32,
    .mean.dim.num_elem = 32,
    .mean.stride.b = 128,
    .mean.stride.h = 128,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193552))) /* Equivalent hex address = 0x70d38a90UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 32,
    .var.dim.num_elem = 32,
    .var.stride.b = 128,
    .var.stride.h = 128,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193680))) /* Equivalent hex address = 0x70d38b10UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_16 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn8_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1605632);

}


/* scheduling epoch=13   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_13(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_17 */
  Activ_sw_info activ9_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 33,
    .operand.dim.num_elem = 33,
    .operand.stride.b = 132,
    .operand.stride.h = 132,
    .operand.stride.w = 132,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10192512))) /* Equivalent hex address = 0x70d38680UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_17 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ9_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1605632);

}


/* scheduling epoch=14   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_14(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id612 */
  Quantizelinear_sw_info quantizelinear10_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194032))) /* Equivalent hex address = 0x70d38c70UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194976))) /* Equivalent hex address = 0x70d39020UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 32,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id612 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear10_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


// Epoch Controller Blob (name='_ec_blob_15') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_15') start function
static void _ec_blob_cache_start_func_15(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

};


/* scheduling epoch=16   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_16(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id614 */
  Dequantizelinear_sw_info dequantizelinear11_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 802816,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 64,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193840))) /* Equivalent hex address = 0x70d38bb0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194784))) /* Equivalent hex address = 0x70d38f60UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 802816,
    .general.output.stride.b = 3211264,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id614 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear11_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 3211264);

}


/* scheduling epoch=17   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_17(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_21 */
  Bn_sw_info bn12_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 802816,
    .general.input.stride.b = 3211264,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 64,
    .scale.dim.num_elem = 64,
    .scale.stride.b = 256,
    .scale.stride.h = 256,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10190320))) /* Equivalent hex address = 0x70d37df0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10190576))) /* Equivalent hex address = 0x70d37ef0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 64,
    .mean.dim.num_elem = 64,
    .mean.stride.b = 256,
    .mean.stride.h = 256,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10190832))) /* Equivalent hex address = 0x70d37ff0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 64,
    .var.dim.num_elem = 64,
    .var.stride.b = 256,
    .var.stride.h = 256,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10191088))) /* Equivalent hex address = 0x70d380f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 802816,
    .general.output.stride.b = 3211264,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) /* Equivalent hex address = 0x90310000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_21 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn12_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 6422528))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) /* Equivalent hex address = 0x90310000UL */, 3211264);

}


/* scheduling epoch=18   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_18(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_22 */
  Activ_sw_info activ13_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 802816,
    .general.input.stride.b = 3211264,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) /* Equivalent hex address = 0x90310000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 65,
    .operand.dim.num_elem = 65,
    .operand.stride.b = 260,
    .operand.stride.h = 260,
    .operand.stride.w = 260,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10189776))) /* Equivalent hex address = 0x70d37bd0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 802816,
    .general.output.stride.b = 3211264,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_22 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ13_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 3211264);

}


/* scheduling epoch=19   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_19(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id615 */
  Quantizelinear_sw_info quantizelinear14_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 112,
    .general.input.dim.tensor_w = 112,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 802816,
    .general.input.stride.b = 3211264,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193936))) /* Equivalent hex address = 0x70d38c10UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194880))) /* Equivalent hex address = 0x70d38fc0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 112,
    .general.output.dim.tensor_w = 112,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 802816,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 64,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id615 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear14_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


// Epoch Controller Blob (name='_ec_blob_20') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_20') start function
static void _ec_blob_cache_start_func_20(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

};


/* scheduling epoch=21   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_21(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id616 */
  Dequantizelinear_sw_info dequantizelinear15_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 64,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193952))) /* Equivalent hex address = 0x70d38c20UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194896))) /* Equivalent hex address = 0x70d38fd0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id616 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear15_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=22   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_22(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_27 */
  Bn_sw_info bn16_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 64,
    .scale.dim.num_elem = 64,
    .scale.stride.b = 256,
    .scale.stride.h = 256,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10191344))) /* Equivalent hex address = 0x70d381f0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10191600))) /* Equivalent hex address = 0x70d382f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 64,
    .mean.dim.num_elem = 64,
    .mean.stride.b = 256,
    .mean.stride.h = 256,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10191856))) /* Equivalent hex address = 0x70d383f0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 64,
    .var.dim.num_elem = 64,
    .var.stride.b = 256,
    .var.stride.h = 256,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10192112))) /* Equivalent hex address = 0x70d384f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_27 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn16_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */, 802816);

}


/* scheduling epoch=23   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_23(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_28 */
  Activ_sw_info activ17_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 65,
    .operand.dim.num_elem = 65,
    .operand.stride.b = 260,
    .operand.stride.h = 260,
    .operand.stride.w = 260,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10190048))) /* Equivalent hex address = 0x70d37ce0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_28 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ17_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=24   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_24(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id617 */
  Quantizelinear_sw_info quantizelinear18_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194288))) /* Equivalent hex address = 0x70d38d70UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195232))) /* Equivalent hex address = 0x70d39120UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 64,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id617 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear18_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 200704);

}


// Epoch Controller Blob (name='_ec_blob_25') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_25') start function
static void _ec_blob_cache_start_func_25(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

};


/* scheduling epoch=26   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_26(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id621 */
  Dequantizelinear_sw_info dequantizelinear19_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 128,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193888))) /* Equivalent hex address = 0x70d38be0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194832))) /* Equivalent hex address = 0x70d38f90UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id621 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear19_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1605632);

}


/* scheduling epoch=27   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_27(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_32 */
  Bn_sw_info bn20_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 128,
    .scale.dim.num_elem = 128,
    .scale.stride.b = 512,
    .scale.stride.h = 512,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10179536))) /* Equivalent hex address = 0x70d353d0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10180048))) /* Equivalent hex address = 0x70d355d0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 128,
    .mean.dim.num_elem = 128,
    .mean.stride.b = 512,
    .mean.stride.h = 512,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10180560))) /* Equivalent hex address = 0x70d357d0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 128,
    .var.dim.num_elem = 128,
    .var.stride.b = 512,
    .var.stride.h = 512,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10181072))) /* Equivalent hex address = 0x70d359d0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_32 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn20_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1605632);

}


/* scheduling epoch=28   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_28(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_33 */
  Activ_sw_info activ21_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 129,
    .operand.dim.num_elem = 129,
    .operand.stride.b = 516,
    .operand.stride.h = 516,
    .operand.stride.w = 516,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10176400))) /* Equivalent hex address = 0x70d34790UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_33 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ21_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */, 1605632);

}


/* scheduling epoch=29   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_29(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id622 */
  Quantizelinear_sw_info quantizelinear22_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194048))) /* Equivalent hex address = 0x70d38c80UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194992))) /* Equivalent hex address = 0x70d39030UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 128,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id622 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear22_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


// Epoch Controller Blob (name='_ec_blob_30') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_30') start function
static void _ec_blob_cache_start_func_30(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

};


/* scheduling epoch=31   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_31(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id623 */
  Dequantizelinear_sw_info dequantizelinear23_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 128,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193904))) /* Equivalent hex address = 0x70d38bf0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194848))) /* Equivalent hex address = 0x70d38fa0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id623 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear23_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1605632);

}


/* scheduling epoch=32   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_32(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_38 */
  Bn_sw_info bn24_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 128,
    .scale.dim.num_elem = 128,
    .scale.stride.b = 512,
    .scale.stride.h = 512,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10181584))) /* Equivalent hex address = 0x70d35bd0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10182096))) /* Equivalent hex address = 0x70d35dd0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 128,
    .mean.dim.num_elem = 128,
    .mean.stride.b = 512,
    .mean.stride.h = 512,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10182608))) /* Equivalent hex address = 0x70d35fd0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 128,
    .var.dim.num_elem = 128,
    .var.stride.b = 512,
    .var.stride.h = 512,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10183120))) /* Equivalent hex address = 0x70d361d0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_38 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn24_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1605632);

}


/* scheduling epoch=33   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_33(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_39 */
  Activ_sw_info activ25_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 129,
    .operand.dim.num_elem = 129,
    .operand.stride.b = 516,
    .operand.stride.h = 516,
    .operand.stride.w = 516,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10176928))) /* Equivalent hex address = 0x70d349a0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_39 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ25_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3211264))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */, 1605632);

}


/* scheduling epoch=34   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_34(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id624 */
  Quantizelinear_sw_info quantizelinear26_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) /* Equivalent hex address = 0x90188000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194064))) /* Equivalent hex address = 0x70d38c90UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195008))) /* Equivalent hex address = 0x70d39040UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 128,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id624 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear26_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


// Epoch Controller Blob (name='_ec_blob_35') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_35') start function
static void _ec_blob_cache_start_func_35(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

};


/* scheduling epoch=36   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_36(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id628 */
  Dequantizelinear_sw_info dequantizelinear27_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 128,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193920))) /* Equivalent hex address = 0x70d38c00UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194864))) /* Equivalent hex address = 0x70d38fb0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id628 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear27_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1605632);

}


/* scheduling epoch=37   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_37(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_43 */
  Bn_sw_info bn28_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 128,
    .scale.dim.num_elem = 128,
    .scale.stride.b = 512,
    .scale.stride.h = 512,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10183632))) /* Equivalent hex address = 0x70d363d0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10184144))) /* Equivalent hex address = 0x70d365d0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 128,
    .mean.dim.num_elem = 128,
    .mean.stride.b = 512,
    .mean.stride.h = 512,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10184656))) /* Equivalent hex address = 0x70d367d0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 128,
    .var.dim.num_elem = 128,
    .var.stride.b = 512,
    .var.stride.h = 512,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10185168))) /* Equivalent hex address = 0x70d369d0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_43 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn28_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1605632);

}


/* scheduling epoch=38   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_38(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_44 */
  Activ_sw_info activ29_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 129,
    .operand.dim.num_elem = 129,
    .operand.stride.b = 516,
    .operand.stride.h = 516,
    .operand.stride.w = 516,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10177456))) /* Equivalent hex address = 0x70d34bb0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 1605632,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_44 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ29_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1605632);

}


/* scheduling epoch=39   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_39(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id629 */
  Quantizelinear_sw_info quantizelinear30_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 56,
    .general.input.dim.tensor_w = 56,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 401408,
    .general.input.stride.b = 1605632,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194080))) /* Equivalent hex address = 0x70d38ca0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195024))) /* Equivalent hex address = 0x70d39050UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 56,
    .general.output.dim.tensor_w = 56,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 401408,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 128,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id629 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear30_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


// Epoch Controller Blob (name='_ec_blob_40') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_40') start function
static void _ec_blob_cache_start_func_40(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=41   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_41(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id630 */
  Dequantizelinear_sw_info dequantizelinear31_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 128,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194096))) /* Equivalent hex address = 0x70d38cb0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195040))) /* Equivalent hex address = 0x70d39060UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id630 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear31_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=42   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_42(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_49 */
  Bn_sw_info bn32_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 128,
    .scale.dim.num_elem = 128,
    .scale.stride.b = 512,
    .scale.stride.h = 512,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10185680))) /* Equivalent hex address = 0x70d36bd0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10186192))) /* Equivalent hex address = 0x70d36dd0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 128,
    .mean.dim.num_elem = 128,
    .mean.stride.b = 512,
    .mean.stride.h = 512,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10186704))) /* Equivalent hex address = 0x70d36fd0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 128,
    .var.dim.num_elem = 128,
    .var.stride.b = 512,
    .var.stride.h = 512,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10187216))) /* Equivalent hex address = 0x70d371d0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_49 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn32_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=43   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_43(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_50 */
  Activ_sw_info activ33_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 129,
    .operand.dim.num_elem = 129,
    .operand.stride.b = 516,
    .operand.stride.h = 516,
    .operand.stride.w = 516,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10177984))) /* Equivalent hex address = 0x70d34dc0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_50 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ33_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=44   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_44(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id631 */
  Quantizelinear_sw_info quantizelinear34_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194432))) /* Equivalent hex address = 0x70d38e00UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195360))) /* Equivalent hex address = 0x70d391a0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 128,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id631 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear34_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_45') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_45') start function
static void _ec_blob_cache_start_func_45(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

};


/* scheduling epoch=47   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_47(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id635 */
  Dequantizelinear_sw_info dequantizelinear35_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 256,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193968))) /* Equivalent hex address = 0x70d38c30UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194912))) /* Equivalent hex address = 0x70d38fe0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id635 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear35_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=48   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_48(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_54 */
  Bn_sw_info bn36_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 256,
    .scale.dim.num_elem = 256,
    .scale.stride.b = 1024,
    .scale.stride.h = 1024,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10159152))) /* Equivalent hex address = 0x70d30430UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10160176))) /* Equivalent hex address = 0x70d30830UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 256,
    .mean.dim.num_elem = 256,
    .mean.stride.b = 1024,
    .mean.stride.h = 1024,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10161200))) /* Equivalent hex address = 0x70d30c30UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 256,
    .var.dim.num_elem = 256,
    .var.stride.b = 1024,
    .var.stride.h = 1024,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10162224))) /* Equivalent hex address = 0x70d31030UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_54 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn36_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */, 802816);

}


/* scheduling epoch=49   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_49(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_55 */
  Activ_sw_info activ37_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 257,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 1028,
    .operand.stride.w = 1028,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10154992))) /* Equivalent hex address = 0x70d2f3f0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_55 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ37_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=50   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_50(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id636 */
  Quantizelinear_sw_info quantizelinear38_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194304))) /* Equivalent hex address = 0x70d38d80UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195248))) /* Equivalent hex address = 0x70d39130UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 256,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id636 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear38_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 200704);

}


// Epoch Controller Blob (name='_ec_blob_51') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_51') start function
static void _ec_blob_cache_start_func_51(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

};


/* scheduling epoch=52   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_52(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id637 */
  Dequantizelinear_sw_info dequantizelinear39_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 256,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193984))) /* Equivalent hex address = 0x70d38c40UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194928))) /* Equivalent hex address = 0x70d38ff0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id637 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear39_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=53   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_53(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_60 */
  Bn_sw_info bn40_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 256,
    .scale.dim.num_elem = 256,
    .scale.stride.b = 1024,
    .scale.stride.h = 1024,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10163248))) /* Equivalent hex address = 0x70d31430UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10164272))) /* Equivalent hex address = 0x70d31830UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 256,
    .mean.dim.num_elem = 256,
    .mean.stride.b = 1024,
    .mean.stride.h = 1024,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10165296))) /* Equivalent hex address = 0x70d31c30UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 256,
    .var.dim.num_elem = 256,
    .var.stride.b = 1024,
    .var.stride.h = 1024,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10166320))) /* Equivalent hex address = 0x70d32030UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_60 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn40_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */, 802816);

}


/* scheduling epoch=54   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_54(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_61 */
  Activ_sw_info activ41_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 257,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 1028,
    .operand.stride.w = 1028,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10156032))) /* Equivalent hex address = 0x70d2f800UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_61 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ41_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=55   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_55(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id638 */
  Quantizelinear_sw_info quantizelinear42_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194320))) /* Equivalent hex address = 0x70d38d90UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195264))) /* Equivalent hex address = 0x70d39140UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 256,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id638 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear42_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


// Epoch Controller Blob (name='_ec_blob_56') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_56') start function
static void _ec_blob_cache_start_func_56(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

};


/* scheduling epoch=58   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_58(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id642 */
  Dequantizelinear_sw_info dequantizelinear43_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 256,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194000))) /* Equivalent hex address = 0x70d38c50UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194944))) /* Equivalent hex address = 0x70d39000UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id642 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear43_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=59   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_59(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_65 */
  Bn_sw_info bn44_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 256,
    .scale.dim.num_elem = 256,
    .scale.stride.b = 1024,
    .scale.stride.h = 1024,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10167344))) /* Equivalent hex address = 0x70d32430UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10168368))) /* Equivalent hex address = 0x70d32830UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 256,
    .mean.dim.num_elem = 256,
    .mean.stride.b = 1024,
    .mean.stride.h = 1024,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10169392))) /* Equivalent hex address = 0x70d32c30UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 256,
    .var.dim.num_elem = 256,
    .var.stride.b = 1024,
    .var.stride.h = 1024,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10170416))) /* Equivalent hex address = 0x70d33030UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_65 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn44_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1605632))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */, 802816);

}


/* scheduling epoch=60   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_60(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_66 */
  Activ_sw_info activ45_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) /* Equivalent hex address = 0x341c4000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 257,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 1028,
    .operand.stride.w = 1028,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10157072))) /* Equivalent hex address = 0x70d2fc10UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 802816,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_66 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ45_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 802816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 802816);

}


/* scheduling epoch=61   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_61(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id643 */
  Quantizelinear_sw_info quantizelinear46_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 28,
    .general.input.dim.tensor_w = 28,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 200704,
    .general.input.stride.b = 802816,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194336))) /* Equivalent hex address = 0x70d38da0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195280))) /* Equivalent hex address = 0x70d39150UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 28,
    .general.output.dim.tensor_w = 28,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 200704,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 256,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id643 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear46_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


// Epoch Controller Blob (name='_ec_blob_62') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_62') start function
static void _ec_blob_cache_start_func_62(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

};


/* scheduling epoch=63   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_63(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id644 */
  Dequantizelinear_sw_info dequantizelinear47_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 50176,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 256,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194352))) /* Equivalent hex address = 0x70d38db0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195296))) /* Equivalent hex address = 0x70d39160UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id644 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear47_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=64   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_64(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_71 */
  Bn_sw_info bn48_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 256,
    .scale.dim.num_elem = 256,
    .scale.stride.b = 1024,
    .scale.stride.h = 1024,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10171440))) /* Equivalent hex address = 0x70d33430UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10172464))) /* Equivalent hex address = 0x70d33830UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 256,
    .mean.dim.num_elem = 256,
    .mean.stride.b = 1024,
    .mean.stride.h = 1024,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10173488))) /* Equivalent hex address = 0x70d33c30UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 256,
    .var.dim.num_elem = 256,
    .var.stride.b = 1024,
    .var.stride.h = 1024,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10174512))) /* Equivalent hex address = 0x70d34030UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_71 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn48_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 200704);

}


/* scheduling epoch=65   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_65(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_72 */
  Activ_sw_info activ49_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 257,
    .operand.dim.num_elem = 257,
    .operand.stride.b = 1028,
    .operand.stride.h = 1028,
    .operand.stride.w = 1028,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10158112))) /* Equivalent hex address = 0x70d30020UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_72 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ49_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=66   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_66(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id645 */
  Quantizelinear_sw_info quantizelinear50_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194656))) /* Equivalent hex address = 0x70d38ee0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195584))) /* Equivalent hex address = 0x70d39280UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 50176,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 256,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id645 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear50_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

}


// Epoch Controller Blob (name='_ec_blob_67') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_67') start function
static void _ec_blob_cache_start_func_67(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=69   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_69(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id649 */
  Dequantizelinear_sw_info dequantizelinear51_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194112))) /* Equivalent hex address = 0x70d38cc0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195056))) /* Equivalent hex address = 0x70d39070UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id649 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear51_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=70   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_70(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_76 */
  Bn_sw_info bn52_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10054640))) /* Equivalent hex address = 0x70d16bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10056688))) /* Equivalent hex address = 0x70d173f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10058736))) /* Equivalent hex address = 0x70d17bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10060784))) /* Equivalent hex address = 0x70d183f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_76 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn52_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=71   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_71(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_77 */
  Activ_sw_info activ53_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10029872))) /* Equivalent hex address = 0x70d10b30UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_77 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ53_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=72   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_72(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id650 */
  Quantizelinear_sw_info quantizelinear54_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194448))) /* Equivalent hex address = 0x70d38e10UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195376))) /* Equivalent hex address = 0x70d391b0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id650 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear54_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_73') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_73') start function
static void _ec_blob_cache_start_func_73(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */, 100352);

};


/* scheduling epoch=74   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_74(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id651 */
  Dequantizelinear_sw_info dequantizelinear55_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194128))) /* Equivalent hex address = 0x70d38cd0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195072))) /* Equivalent hex address = 0x70d39080UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id651 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear55_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=75   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_75(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_82 */
  Bn_sw_info bn56_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10062832))) /* Equivalent hex address = 0x70d18bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10064880))) /* Equivalent hex address = 0x70d193f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10066928))) /* Equivalent hex address = 0x70d19bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10068976))) /* Equivalent hex address = 0x70d1a3f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_82 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn56_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=76   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_76(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_83 */
  Activ_sw_info activ57_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10031936))) /* Equivalent hex address = 0x70d11340UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_83 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ57_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=77   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_77(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id652 */
  Quantizelinear_sw_info quantizelinear58_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194464))) /* Equivalent hex address = 0x70d38e20UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195392))) /* Equivalent hex address = 0x70d391c0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id652 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear58_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_78') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_78') start function
static void _ec_blob_cache_start_func_78(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=80   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_80(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id656 */
  Dequantizelinear_sw_info dequantizelinear59_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194144))) /* Equivalent hex address = 0x70d38ce0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195088))) /* Equivalent hex address = 0x70d39090UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id656 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear59_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=81   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_81(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_87 */
  Bn_sw_info bn60_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10071024))) /* Equivalent hex address = 0x70d1abf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10073072))) /* Equivalent hex address = 0x70d1b3f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10075120))) /* Equivalent hex address = 0x70d1bbf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10077168))) /* Equivalent hex address = 0x70d1c3f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_87 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn60_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=82   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_82(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_88 */
  Activ_sw_info activ61_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10034000))) /* Equivalent hex address = 0x70d11b50UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_88 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ61_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=83   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_83(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id657 */
  Quantizelinear_sw_info quantizelinear62_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194480))) /* Equivalent hex address = 0x70d38e30UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195408))) /* Equivalent hex address = 0x70d391d0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id657 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear62_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_84') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_84') start function
static void _ec_blob_cache_start_func_84(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */, 100352);

};


/* scheduling epoch=85   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_85(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id658 */
  Dequantizelinear_sw_info dequantizelinear63_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194160))) /* Equivalent hex address = 0x70d38cf0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195104))) /* Equivalent hex address = 0x70d390a0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id658 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear63_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=86   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_86(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_93 */
  Bn_sw_info bn64_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10079216))) /* Equivalent hex address = 0x70d1cbf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10081264))) /* Equivalent hex address = 0x70d1d3f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10083312))) /* Equivalent hex address = 0x70d1dbf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10085360))) /* Equivalent hex address = 0x70d1e3f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_93 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn64_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=87   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_87(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_94 */
  Activ_sw_info activ65_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10036064))) /* Equivalent hex address = 0x70d12360UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_94 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ65_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=88   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_88(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id659 */
  Quantizelinear_sw_info quantizelinear66_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194496))) /* Equivalent hex address = 0x70d38e40UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195424))) /* Equivalent hex address = 0x70d391e0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id659 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear66_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_89') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_89') start function
static void _ec_blob_cache_start_func_89(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=91   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_91(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id663 */
  Dequantizelinear_sw_info dequantizelinear67_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194176))) /* Equivalent hex address = 0x70d38d00UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195120))) /* Equivalent hex address = 0x70d390b0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id663 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear67_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=92   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_92(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_98 */
  Bn_sw_info bn68_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10087408))) /* Equivalent hex address = 0x70d1ebf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10089456))) /* Equivalent hex address = 0x70d1f3f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10091504))) /* Equivalent hex address = 0x70d1fbf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10093552))) /* Equivalent hex address = 0x70d203f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_98 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn68_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=93   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_93(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_99 */
  Activ_sw_info activ69_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10038128))) /* Equivalent hex address = 0x70d12b70UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_99 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ69_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=94   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_94(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id664 */
  Quantizelinear_sw_info quantizelinear70_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194512))) /* Equivalent hex address = 0x70d38e50UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195440))) /* Equivalent hex address = 0x70d391f0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id664 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear70_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_95') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_95') start function
static void _ec_blob_cache_start_func_95(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */, 100352);

};


/* scheduling epoch=96   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_96(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id665 */
  Dequantizelinear_sw_info dequantizelinear71_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194192))) /* Equivalent hex address = 0x70d38d10UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195136))) /* Equivalent hex address = 0x70d390c0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id665 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear71_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=97   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_97(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_104 */
  Bn_sw_info bn72_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10095600))) /* Equivalent hex address = 0x70d20bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10097648))) /* Equivalent hex address = 0x70d213f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10099696))) /* Equivalent hex address = 0x70d21bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10101744))) /* Equivalent hex address = 0x70d223f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_104 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn72_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=98   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_98(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_105 */
  Activ_sw_info activ73_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10040192))) /* Equivalent hex address = 0x70d13380UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_105 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ73_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=99   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_99(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id666 */
  Quantizelinear_sw_info quantizelinear74_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194528))) /* Equivalent hex address = 0x70d38e60UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195456))) /* Equivalent hex address = 0x70d39200UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id666 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear74_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_100') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_100') start function
static void _ec_blob_cache_start_func_100(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=102  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_102(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id670 */
  Dequantizelinear_sw_info dequantizelinear75_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194208))) /* Equivalent hex address = 0x70d38d20UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195152))) /* Equivalent hex address = 0x70d390d0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id670 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear75_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=103  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_103(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_109 */
  Bn_sw_info bn76_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10103792))) /* Equivalent hex address = 0x70d22bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10105840))) /* Equivalent hex address = 0x70d233f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10107888))) /* Equivalent hex address = 0x70d23bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10109936))) /* Equivalent hex address = 0x70d243f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_109 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn76_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=104  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_104(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_110 */
  Activ_sw_info activ77_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10042256))) /* Equivalent hex address = 0x70d13b90UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_110 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ77_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=105  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_105(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id671 */
  Quantizelinear_sw_info quantizelinear78_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194544))) /* Equivalent hex address = 0x70d38e70UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195472))) /* Equivalent hex address = 0x70d39210UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id671 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear78_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_106') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_106') start function
static void _ec_blob_cache_start_func_106(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */, 100352);

};


/* scheduling epoch=107  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_107(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id672 */
  Dequantizelinear_sw_info dequantizelinear79_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194224))) /* Equivalent hex address = 0x70d38d30UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195168))) /* Equivalent hex address = 0x70d390e0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id672 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear79_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=108  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_108(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_115 */
  Bn_sw_info bn80_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10111984))) /* Equivalent hex address = 0x70d24bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10114032))) /* Equivalent hex address = 0x70d253f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10116080))) /* Equivalent hex address = 0x70d25bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10118128))) /* Equivalent hex address = 0x70d263f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_115 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn80_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=109  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_109(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_116 */
  Activ_sw_info activ81_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10044320))) /* Equivalent hex address = 0x70d143a0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_116 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ81_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=110  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_110(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id673 */
  Quantizelinear_sw_info quantizelinear82_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194560))) /* Equivalent hex address = 0x70d38e80UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195488))) /* Equivalent hex address = 0x70d39220UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id673 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear82_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_111') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_111') start function
static void _ec_blob_cache_start_func_111(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=113  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_113(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id677 */
  Dequantizelinear_sw_info dequantizelinear83_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194240))) /* Equivalent hex address = 0x70d38d40UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195184))) /* Equivalent hex address = 0x70d390f0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id677 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear83_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=114  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_114(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_120 */
  Bn_sw_info bn84_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10120176))) /* Equivalent hex address = 0x70d26bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10122224))) /* Equivalent hex address = 0x70d273f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10124272))) /* Equivalent hex address = 0x70d27bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10126320))) /* Equivalent hex address = 0x70d283f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_120 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn84_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=115  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_115(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_121 */
  Activ_sw_info activ85_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10046384))) /* Equivalent hex address = 0x70d14bb0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_121 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ85_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=116  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_116(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id678 */
  Quantizelinear_sw_info quantizelinear86_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194576))) /* Equivalent hex address = 0x70d38e90UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195504))) /* Equivalent hex address = 0x70d39230UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id678 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear86_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_117') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_117') start function
static void _ec_blob_cache_start_func_117(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 200704))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */, 100352);

};


/* scheduling epoch=118  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_118(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id679 */
  Dequantizelinear_sw_info dequantizelinear87_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) /* Equivalent hex address = 0x34288800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194256))) /* Equivalent hex address = 0x70d38d50UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195200))) /* Equivalent hex address = 0x70d39100UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id679 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear87_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=119  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_119(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_126 */
  Bn_sw_info bn88_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10128368))) /* Equivalent hex address = 0x70d28bf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10130416))) /* Equivalent hex address = 0x70d293f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10132464))) /* Equivalent hex address = 0x70d29bf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10134512))) /* Equivalent hex address = 0x70d2a3f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_126 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn88_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=120  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_120(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_127 */
  Activ_sw_info activ89_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10048448))) /* Equivalent hex address = 0x70d153c0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_127 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ89_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=121  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_121(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id680 */
  Quantizelinear_sw_info quantizelinear90_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194592))) /* Equivalent hex address = 0x70d38ea0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195520))) /* Equivalent hex address = 0x70d39240UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id680 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear90_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_122') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_122') start function
static void _ec_blob_cache_start_func_122(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

};


/* scheduling epoch=124  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_124(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id684 */
  Dequantizelinear_sw_info dequantizelinear91_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194272))) /* Equivalent hex address = 0x70d38d60UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195216))) /* Equivalent hex address = 0x70d39110UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id684 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear91_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=125  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_125(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_131 */
  Bn_sw_info bn92_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10136560))) /* Equivalent hex address = 0x70d2abf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10138608))) /* Equivalent hex address = 0x70d2b3f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10140656))) /* Equivalent hex address = 0x70d2bbf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10142704))) /* Equivalent hex address = 0x70d2c3f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_131 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn92_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 401408);

}


/* scheduling epoch=126  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_126(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_132 */
  Activ_sw_info activ93_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10050512))) /* Equivalent hex address = 0x70d15bd0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 401408,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_132 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ93_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 401408);

}


/* scheduling epoch=127  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_127(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id685 */
  Quantizelinear_sw_info quantizelinear94_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 100352,
    .general.input.stride.b = 401408,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194608))) /* Equivalent hex address = 0x70d38eb0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195536))) /* Equivalent hex address = 0x70d39250UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 14,
    .general.output.dim.tensor_w = 14,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 100352,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id685 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear94_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 100352);

}


// Epoch Controller Blob (name='_ec_blob_128') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_128') start function
static void _ec_blob_cache_start_func_128(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 125440))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */, 25088);

};


/* scheduling epoch=129  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_129(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id686 */
  Dequantizelinear_sw_info dequantizelinear95_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 25088,
    .general.input.stride.h = 3584,
    .general.input.stride.w = 512,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194624))) /* Equivalent hex address = 0x70d38ec0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195552))) /* Equivalent hex address = 0x70d39260UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 25088,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id686 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear95_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 100352);

}


/* scheduling epoch=130  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_130(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_137 */
  Bn_sw_info bn96_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 512,
    .scale.dim.num_elem = 512,
    .scale.stride.b = 2048,
    .scale.stride.h = 2048,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10144752))) /* Equivalent hex address = 0x70d2cbf0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 512,
    .bias.dim.num_elem = 512,
    .bias.stride.b = 2048,
    .bias.stride.h = 2048,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10146800))) /* Equivalent hex address = 0x70d2d3f0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 512,
    .mean.dim.num_elem = 512,
    .mean.stride.b = 2048,
    .mean.stride.h = 2048,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10148848))) /* Equivalent hex address = 0x70d2dbf0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 512,
    .var.dim.num_elem = 512,
    .var.stride.b = 2048,
    .var.stride.h = 2048,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10150896))) /* Equivalent hex address = 0x70d2e3f0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 25088,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_137 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn96_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */, 100352);

}


/* scheduling epoch=131  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_131(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_138 */
  Activ_sw_info activ97_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 513,
    .operand.dim.num_elem = 513,
    .operand.stride.b = 2052,
    .operand.stride.h = 2052,
    .operand.stride.w = 2052,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10052576))) /* Equivalent hex address = 0x70d163e0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 25088,
    .general.output.stride.b = 100352,
    .general.output.stride.h = 14336,
    .general.output.stride.w = 2048,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_138 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ97_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 100352);

}


/* scheduling epoch=132  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_132(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id687 */
  Quantizelinear_sw_info quantizelinear98_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 14336,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194720))) /* Equivalent hex address = 0x70d38f20UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195648))) /* Equivalent hex address = 0x70d392c0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 512,
    .general.output.dim.num_elem = 25088,
    .general.output.stride.b = 25088,
    .general.output.stride.h = 3584,
    .general.output.stride.w = 512,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id687 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear98_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 125440))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100352))) /* Equivalent hex address = 0x342f8800UL */, 25088);

}


// Epoch Controller Blob (name='_ec_blob_133') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_133') start function
static void _ec_blob_cache_start_func_133(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

};


/* scheduling epoch=135  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_135(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id691 */
  Dequantizelinear_sw_info dequantizelinear99_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 50176,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194368))) /* Equivalent hex address = 0x70d38dc0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195312))) /* Equivalent hex address = 0x70d39170UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id691 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear99_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=136  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_136(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_142 */
  Bn_sw_info bn100_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 1024,
    .scale.dim.num_elem = 1024,
    .scale.stride.b = 4096,
    .scale.stride.h = 4096,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9970224))) /* Equivalent hex address = 0x70d02230UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 1024,
    .bias.dim.num_elem = 1024,
    .bias.stride.b = 4096,
    .bias.stride.h = 4096,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9974320))) /* Equivalent hex address = 0x70d03230UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 1024,
    .mean.dim.num_elem = 1024,
    .mean.stride.b = 4096,
    .mean.stride.h = 4096,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9978416))) /* Equivalent hex address = 0x70d04230UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 1024,
    .var.dim.num_elem = 1024,
    .var.stride.b = 4096,
    .var.stride.h = 4096,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9982512))) /* Equivalent hex address = 0x70d05230UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_142 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn100_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 200704);

}


/* scheduling epoch=137  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_137(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_143 */
  Activ_sw_info activ101_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1025,
    .operand.dim.num_elem = 1025,
    .operand.stride.b = 4100,
    .operand.stride.h = 4100,
    .operand.stride.w = 4100,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9957888))) /* Equivalent hex address = 0x70cff200UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_143 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ101_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=138  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_138(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id692 */
  Quantizelinear_sw_info quantizelinear102_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194672))) /* Equivalent hex address = 0x70d38ef0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195600))) /* Equivalent hex address = 0x70d39290UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 50176,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id692 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear102_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

}


// Epoch Controller Blob (name='_ec_blob_139') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_139') start function
static void _ec_blob_cache_start_func_139(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 301056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) /* Equivalent hex address = 0x3431d400UL */, 50176);

};


/* scheduling epoch=140  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_140(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id693 */
  Dequantizelinear_sw_info dequantizelinear103_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 50176,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) /* Equivalent hex address = 0x3431d400UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194384))) /* Equivalent hex address = 0x70d38dd0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195328))) /* Equivalent hex address = 0x70d39180UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id693 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear103_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=141  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_141(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_148 */
  Bn_sw_info bn104_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 1024,
    .scale.dim.num_elem = 1024,
    .scale.stride.b = 4096,
    .scale.stride.h = 4096,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9986608))) /* Equivalent hex address = 0x70d06230UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 1024,
    .bias.dim.num_elem = 1024,
    .bias.stride.b = 4096,
    .bias.stride.h = 4096,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9990704))) /* Equivalent hex address = 0x70d07230UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 1024,
    .mean.dim.num_elem = 1024,
    .mean.stride.b = 4096,
    .mean.stride.h = 4096,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9994800))) /* Equivalent hex address = 0x70d08230UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 1024,
    .var.dim.num_elem = 1024,
    .var.stride.b = 4096,
    .var.stride.h = 4096,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9998896))) /* Equivalent hex address = 0x70d09230UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_148 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn104_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 200704);

}


/* scheduling epoch=142  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_142(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_149 */
  Activ_sw_info activ105_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1025,
    .operand.dim.num_elem = 1025,
    .operand.stride.b = 4100,
    .operand.stride.h = 4100,
    .operand.stride.w = 4100,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9962000))) /* Equivalent hex address = 0x70d00210UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_149 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ105_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=143  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_143(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=QuantizeLinear_inserted_id694 */
  Quantizelinear_sw_info quantizelinear106_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194688))) /* Equivalent hex address = 0x70d38f00UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195616))) /* Equivalent hex address = 0x70d392a0UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 50176,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node QuantizeLinear_inserted_id694 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear106_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

}


// Epoch Controller Blob (name='_ec_blob_144') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_144') start function
static void _ec_blob_cache_start_func_144(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

};


/* scheduling epoch=146  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_146(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id698 */
  Dequantizelinear_sw_info dequantizelinear107_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 50176,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194400))) /* Equivalent hex address = 0x70d38de0UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195344))) /* Equivalent hex address = 0x70d39190UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id698 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear107_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=147  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_147(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_153 */
  Bn_sw_info bn108_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 1024,
    .scale.dim.num_elem = 1024,
    .scale.stride.b = 4096,
    .scale.stride.h = 4096,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10002992))) /* Equivalent hex address = 0x70d0a230UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 1024,
    .bias.dim.num_elem = 1024,
    .bias.stride.b = 4096,
    .bias.stride.h = 4096,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10007088))) /* Equivalent hex address = 0x70d0b230UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 1024,
    .mean.dim.num_elem = 1024,
    .mean.stride.b = 4096,
    .mean.stride.h = 4096,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10011184))) /* Equivalent hex address = 0x70d0c230UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 1024,
    .var.dim.num_elem = 1024,
    .var.stride.b = 4096,
    .var.stride.h = 4096,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10015280))) /* Equivalent hex address = 0x70d0d230UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_153 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn108_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 200704);

}


/* scheduling epoch=148  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_148(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=PRelu node=PReLU_154 */
  Activ_sw_info activ109_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1025,
    .operand.dim.num_elem = 1025,
    .operand.stride.b = 4100,
    .operand.stride.h = 4100,
    .operand.stride.w = 4100,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 9966112))) /* Equivalent hex address = 0x70d01220UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_PRELU,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node PReLU_154 mapped on EmbedNets (FLOAT) as PRelu | Category: Computational */
  ll_sw_forward_activ(&activ109_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=149  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_149(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=BatchNormalization_155bn_mul276 */
  Arith_sw_info arith110_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1024,
    .operand.dim.num_elem = 1024,
    .operand.stride.b = 4096,
    .operand.stride.h = 4096,
    .operand.stride.w = 4096,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10019376))) /* Equivalent hex address = 0x70d0e230UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_155bn_mul276 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith110_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 401408))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 200704);

}


/* scheduling epoch=150  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_150(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=BatchNormalization_155bn_add278 */
  Arith_sw_info arith111_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1024,
    .operand.dim.num_elem = 1024,
    .operand.stride.b = 4096,
    .operand.stride.h = 4096,
    .operand.stride.w = 4096,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10023472))) /* Equivalent hex address = 0x70d0f230UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 200704,
    .general.output.stride.h = 28672,
    .general.output.stride.w = 4096,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_155bn_add278 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith111_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 200704);

}


/* scheduling epoch=151  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_151(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=QuantizeLinear node=Quantize_156 */
  Quantizelinear_sw_info quantizelinear112_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 7,
    .general.input.dim.tensor_w = 7,
    .general.input.dim.tensor_c = 1024,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 200704,
    .general.input.stride.h = 28672,
    .general.input.stride.w = 4096,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194640))) /* Equivalent hex address = 0x70d38ed0UL */,
    .os.format.is_signed = 1,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195568))) /* Equivalent hex address = 0x70d39270UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 7,
    .general.output.dim.tensor_w = 7,
    .general.output.dim.tensor_c = 1024,
    .general.output.dim.num_elem = 50176,
    .general.output.stride.b = 50176,
    .general.output.stride.h = 7168,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_QUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Quantize_156 mapped on EmbedNets (INTEGER) as QuantizeLinear | Category: Format-Converter */
  ll_sw_forward_quantizelinear(&quantizelinear112_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 200704))) /* Equivalent hex address = 0x34311000UL */, 50176);

}


// Epoch Controller Blob (name='_ec_blob_152') micro instructions needed

// Epoch Controller Blob (name='_ec_blob_152') start function
static void _ec_blob_cache_start_func_152(const void *epoch_block) {
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 301056))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) /* Equivalent hex address = 0x3431d400UL */, 50176);

};


/* scheduling epoch=153  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_153(const void *epoch_block)
{
  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 201856))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 201888))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 201856))) /* Equivalent hex address = 0x34311480UL */, 32);

  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Gemm_159_conv_4 */
  Conv_integer_sw_info conv_integer113_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 50176,
    .general.input.dim.num_elem = 50176,
    .general.input.stride.b = 50176,
    .general.input.stride.h = 50176,
    .general.input.stride.w = 50176,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 250880))) /* Equivalent hex address = 0x3431d400UL */,
    .general.input.format.is_signed = 1,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 50176,
    .weights.dim.num_elem = 6422528,
    .weights.stride.b = 50176,
    .weights.stride.h = 50176,
    .weights.stride.w = 50176,
    .weights.stride.c = 1,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 0))) /* Equivalent hex address = 0x70380000UL */,
    .weights.format.is_signed = 1,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10178512))) /* Equivalent hex address = 0x70d34fd0UL */,
    .bias.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193808))) /* Equivalent hex address = 0x70d38b90UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194752))) /* Equivalent hex address = 0x70d38f40UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "ws" tensor-related info: */
    .ws.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10179024))) /* Equivalent hex address = 0x70d351d0UL */,
    .ws.format.is_signed = 0,
    .ws.dim.num_elem = 128,
    /* "wzp" tensor-related info: */
    .wzp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10192656))) /* Equivalent hex address = 0x70d38710UL */,
    .wzp.format.is_signed = 1,
    .wzp.dim.num_elem = 128,
    /* "os" tensor-related info: */
    .os.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10193824))) /* Equivalent hex address = 0x70d38ba0UL */,
    .os.format.is_signed = 0,
    .os.dim.num_elem = 1,
    /* "ozp" tensor-related info: */
    .ozp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194768))) /* Equivalent hex address = 0x70d38f50UL */,
    .ozp.format.is_signed = 1,
    .ozp.dim.num_elem = 1,
    /* "scratch" tensor-related info: */
    .scratch.dim.tensor_b = 1,
    .scratch.dim.tensor_h = 1,
    .scratch.dim.tensor_w = 1,
    .scratch.dim.tensor_c = 201731,
    .scratch.dim.num_elem = 201731,
    .scratch.stride.b = 201731,
    .scratch.stride.h = 201731,
    .scratch.stride.w = 201731,
    .scratch.stride.c = 1,
    .scratch.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .scratch.format.is_signed = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 128,
    .general.output.stride.b = 128,
    .general.output.stride.h = 128,
    .general.output.stride.w = 128,
    .general.output.stride.c = 1,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 201744))) /* Equivalent hex address = 0x34311410UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .fwd_func = LL_SW_SSSA_PW_CONV,
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_159_conv_4 mapped on EmbedNets (INTEGER) as Conv | Category: Computational */
  ll_sw_forward_conv_integer(&conv_integer113_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 201888))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 201888);

}


// Epoch Controller Blob (name='_ec_blob_154') micro instructions needed


/* scheduling epoch=155  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_155(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=DequantizeLinear node=DequantizeLinear_inserted_id699 */
  Dequantizelinear_sw_info dequantizelinear114_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 128,
    .general.input.stride.b = 128,
    .general.input.stride.h = 128,
    .general.input.stride.w = 1,
    .general.input.stride.c = 1,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 201744))) /* Equivalent hex address = 0x34311410UL */,
    .general.input.format.is_signed = 1,
    /* "is" tensor-related info: */
    .is.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10194736))) /* Equivalent hex address = 0x70d38f30UL */,
    .is.format.is_signed = 0,
    .is.dim.num_elem = 1,
    /* "izp" tensor-related info: */
    .izp.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10195664))) /* Equivalent hex address = 0x70d392d0UL */,
    .izp.format.is_signed = 1,
    .izp.dim.num_elem = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 128,
    .general.output.stride.b = 512,
    .general.output.stride.h = 512,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 512))) /* Equivalent hex address = 0x342e0200UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_DEQUANTIZELINEAR,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node DequantizeLinear_inserted_id699 mapped on EmbedNets (INTEGER) as DequantizeLinear | Category: Format-Converter */
  ll_sw_forward_dequantizelinear(&dequantizelinear114_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 512))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 1024))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 512))) /* Equivalent hex address = 0x342e0200UL */, 512);

}


/* scheduling epoch=156  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_156(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=BatchNormalization node=BatchNormalization_160 */
  Bn_sw_info bn115_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 128,
    .general.input.stride.b = 512,
    .general.input.stride.h = 512,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 512))) /* Equivalent hex address = 0x342e0200UL */,
    .general.input.format.is_signed = 0,
    /* "scale" tensor-related info: */
    .scale.dim.tensor_b = 1,
    .scale.dim.tensor_h = 1,
    .scale.dim.tensor_w = 1,
    .scale.dim.tensor_c = 128,
    .scale.dim.num_elem = 128,
    .scale.stride.b = 512,
    .scale.stride.h = 512,
    .scale.stride.w = 4,
    .scale.stride.c = 4,
    .scale.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10187728))) /* Equivalent hex address = 0x70d373d0UL */,
    .scale.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10188240))) /* Equivalent hex address = 0x70d375d0UL */,
    .bias.format.is_signed = 0,
    /* "mean" tensor-related info: */
    .mean.dim.tensor_b = 1,
    .mean.dim.tensor_h = 1,
    .mean.dim.tensor_w = 1,
    .mean.dim.tensor_c = 128,
    .mean.dim.num_elem = 128,
    .mean.stride.b = 512,
    .mean.stride.h = 512,
    .mean.stride.w = 4,
    .mean.stride.c = 4,
    .mean.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10188752))) /* Equivalent hex address = 0x70d377d0UL */,
    .mean.format.is_signed = 0,
    /* "var" tensor-related info: */
    .var.dim.tensor_b = 1,
    .var.dim.tensor_h = 1,
    .var.dim.tensor_w = 1,
    .var.dim.tensor_c = 128,
    .var.dim.num_elem = 128,
    .var.stride.b = 512,
    .var.stride.h = 512,
    .var.stride.w = 4,
    .var.stride.c = 4,
    .var.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x70380000UL + 10189264))) /* Equivalent hex address = 0x70d379d0UL */,
    .var.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 128,
    .general.output.stride.b = 512,
    .general.output.stride.h = 512,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_BATCHNORM,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node BatchNormalization_160 mapped on EmbedNets (FLOAT) as BatchNormalization | Category: Computational */
  ll_sw_forward_bn(&bn115_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 512))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 512);

}


/* scheduling epoch=157  nodes=1   ------------------------------------------------------------------- */

/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_face_recognition(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = _ec_blob_cache_start_func_1,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_1),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 1,
      .last_epoch_num = 1,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_2,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_3,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 3,
      .last_epoch_num = 3,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_4,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_4),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 4,
      .last_epoch_num = 5,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_6,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 6,
      .last_epoch_num = 6,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_7,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 7,
      .last_epoch_num = 7,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_8,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 8,
      .last_epoch_num = 8,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_9,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 9,
      .last_epoch_num = 9,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_10,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_10),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 10,
      .last_epoch_num = 10,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_11,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 11,
      .last_epoch_num = 11,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_12,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 12,
      .last_epoch_num = 12,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_13,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 13,
      .last_epoch_num = 13,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_14,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 14,
      .last_epoch_num = 14,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_15,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_15),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 15,
      .last_epoch_num = 15,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_16,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 16,
      .last_epoch_num = 16,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_17,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 17,
      .last_epoch_num = 17,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_18,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 18,
      .last_epoch_num = 18,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_19,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 19,
      .last_epoch_num = 19,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_20,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_20),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 20,
      .last_epoch_num = 20,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_21,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 21,
      .last_epoch_num = 21,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_22,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 22,
      .last_epoch_num = 22,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_23,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 23,
      .last_epoch_num = 23,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_24,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 24,
      .last_epoch_num = 24,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_25,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_25),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 25,
      .last_epoch_num = 25,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_26,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 26,
      .last_epoch_num = 26,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_27,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 27,
      .last_epoch_num = 27,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_28,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 28,
      .last_epoch_num = 28,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_29,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 29,
      .last_epoch_num = 29,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_30,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_30),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 30,
      .last_epoch_num = 30,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_31,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 31,
      .last_epoch_num = 31,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_32,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 32,
      .last_epoch_num = 32,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_33,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 33,
      .last_epoch_num = 33,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_34,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 34,
      .last_epoch_num = 34,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_35,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_35),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 35,
      .last_epoch_num = 35,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_36,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 36,
      .last_epoch_num = 36,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_37,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 37,
      .last_epoch_num = 37,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_38,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 38,
      .last_epoch_num = 38,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_39,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 39,
      .last_epoch_num = 39,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_40,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_40),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 40,
      .last_epoch_num = 40,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_41,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 41,
      .last_epoch_num = 41,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_42,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 42,
      .last_epoch_num = 42,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_43,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 43,
      .last_epoch_num = 43,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_44,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 44,
      .last_epoch_num = 44,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_45,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_45),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 45,
      .last_epoch_num = 46,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_47,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 47,
      .last_epoch_num = 47,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_48,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 48,
      .last_epoch_num = 48,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_49,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 49,
      .last_epoch_num = 49,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_50,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 50,
      .last_epoch_num = 50,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_51,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_51),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 51,
      .last_epoch_num = 51,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_52,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 52,
      .last_epoch_num = 52,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_53,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 53,
      .last_epoch_num = 53,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_54,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 54,
      .last_epoch_num = 54,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_55,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 55,
      .last_epoch_num = 55,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_56,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_56),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 56,
      .last_epoch_num = 57,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_58,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 58,
      .last_epoch_num = 58,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_59,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 59,
      .last_epoch_num = 59,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_60,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 60,
      .last_epoch_num = 60,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_61,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 61,
      .last_epoch_num = 61,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_62,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_62),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 62,
      .last_epoch_num = 62,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_63,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 63,
      .last_epoch_num = 63,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_64,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 64,
      .last_epoch_num = 64,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_65,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 65,
      .last_epoch_num = 65,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_66,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 66,
      .last_epoch_num = 66,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_67,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_67),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 67,
      .last_epoch_num = 68,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_69,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 69,
      .last_epoch_num = 69,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_70,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 70,
      .last_epoch_num = 70,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_71,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 71,
      .last_epoch_num = 71,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_72,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 72,
      .last_epoch_num = 72,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_73,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_73),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 73,
      .last_epoch_num = 73,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_74,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 74,
      .last_epoch_num = 74,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_75,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 75,
      .last_epoch_num = 75,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_76,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 76,
      .last_epoch_num = 76,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_77,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 77,
      .last_epoch_num = 77,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_78,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_78),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 78,
      .last_epoch_num = 79,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_80,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 80,
      .last_epoch_num = 80,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_81,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 81,
      .last_epoch_num = 81,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_82,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 82,
      .last_epoch_num = 82,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_83,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 83,
      .last_epoch_num = 83,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_84,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_84),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 84,
      .last_epoch_num = 84,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_85,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 85,
      .last_epoch_num = 85,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_86,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 86,
      .last_epoch_num = 86,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_87,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 87,
      .last_epoch_num = 87,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_88,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 88,
      .last_epoch_num = 88,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_89,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_89),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 89,
      .last_epoch_num = 90,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_91,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 91,
      .last_epoch_num = 91,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_92,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 92,
      .last_epoch_num = 92,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_93,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 93,
      .last_epoch_num = 93,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_94,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 94,
      .last_epoch_num = 94,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_95,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_95),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 95,
      .last_epoch_num = 95,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_96,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 96,
      .last_epoch_num = 96,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_97,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 97,
      .last_epoch_num = 97,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_98,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 98,
      .last_epoch_num = 98,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_99,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 99,
      .last_epoch_num = 99,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_100,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_100),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 100,
      .last_epoch_num = 101,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_102,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 102,
      .last_epoch_num = 102,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_103,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 103,
      .last_epoch_num = 103,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_104,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 104,
      .last_epoch_num = 104,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_105,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 105,
      .last_epoch_num = 105,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_106,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_106),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 106,
      .last_epoch_num = 106,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_107,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 107,
      .last_epoch_num = 107,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_108,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 108,
      .last_epoch_num = 108,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_109,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 109,
      .last_epoch_num = 109,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_110,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 110,
      .last_epoch_num = 110,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_111,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_111),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 111,
      .last_epoch_num = 112,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_113,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 113,
      .last_epoch_num = 113,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_114,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 114,
      .last_epoch_num = 114,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_115,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 115,
      .last_epoch_num = 115,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_116,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 116,
      .last_epoch_num = 116,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_117,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_117),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 117,
      .last_epoch_num = 117,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_118,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 118,
      .last_epoch_num = 118,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_119,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 119,
      .last_epoch_num = 119,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_120,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 120,
      .last_epoch_num = 120,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_121,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 121,
      .last_epoch_num = 121,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_122,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_122),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 122,
      .last_epoch_num = 123,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_124,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 124,
      .last_epoch_num = 124,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_125,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 125,
      .last_epoch_num = 125,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_126,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 126,
      .last_epoch_num = 126,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_127,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 127,
      .last_epoch_num = 127,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_128,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_128),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 128,
      .last_epoch_num = 128,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_129,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 129,
      .last_epoch_num = 129,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_130,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 130,
      .last_epoch_num = 130,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_131,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 131,
      .last_epoch_num = 131,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_132,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 132,
      .last_epoch_num = 132,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_133,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_133),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 133,
      .last_epoch_num = 134,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_135,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 135,
      .last_epoch_num = 135,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_136,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 136,
      .last_epoch_num = 136,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_137,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 137,
      .last_epoch_num = 137,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_138,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 138,
      .last_epoch_num = 138,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_139,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_139),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 139,
      .last_epoch_num = 139,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_140,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 140,
      .last_epoch_num = 140,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_141,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 141,
      .last_epoch_num = 141,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_142,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 142,
      .last_epoch_num = 142,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_143,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 143,
      .last_epoch_num = 143,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_144,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_144),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 144,
      .last_epoch_num = 145,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_146,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 146,
      .last_epoch_num = 146,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_147,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 147,
      .last_epoch_num = 147,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_148,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 148,
      .last_epoch_num = 148,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_149,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 149,
      .last_epoch_num = 149,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_150,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 150,
      .last_epoch_num = 150,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_151,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 151,
      .last_epoch_num = 151,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = _ec_blob_cache_start_func_152,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_152),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 152,
      .last_epoch_num = 152,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_153,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 153,
      .last_epoch_num = 153,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = NULL,
      .blob_address = (uintptr_t)(_ec_blob_154),
      .wait_mask = 0,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_blob | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 154,
      .last_epoch_num = 154,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_155,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 155,
      .last_epoch_num = 155,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_156,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 156,
      .last_epoch_num = 156,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_face_recognition(void)
{
  static const uint32_t buff_info__shape_1_3_112_112[] = { 1, 112, 112, 3 };
  static const uint32_t buff_info__mem_shape_F_1_3_112_112[] = { 1, 3, 112, 112 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_1[] = { 1, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_U_1[] = { 1 };
  static const uint32_t buff_info__shape_32_3_3_3[] = { 32, 3, 3, 3 };
  static const uint32_t buff_info__mem_shape_L_32_3_3_3[] = { 32, 3, 3, 3 };
  static const float buff_info_Conv2D_7_weights_quant_scale[] = { 0.00920622516423464, 0.00153067975770682, 0.00211141584441066, 0.00575023982673883, 0.00356953195296228, 0.00217220513150096, 0.0025203856639564, 0.00490287085995078, 0.00419283425435424, 0.00132189248688519, 0.00233910838142037, 0.00220767315477133, 0.00391532434150577, 0.00409765681251884, 0.00381439784541726, 0.00292863347567618, 0.00562589755281806, 0.00687111169099808, 0.00300211529247463, 0.00540690729394555, 0.00266038766130805, 0.0026982412673533, 0.00277009326964617, 0.00295293051749468, 0.00403787521645427, 0.00310239428654313, 0.00441715214401484, 0.0057804579846561, 0.00418674945831299, 0.00173275289125741, 0.00781140010803938, 0.00685378722846508 };
  static const int16_t buff_info_Conv2D_7_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_32[] = { 1, 1, 32, 1 };
  static const uint32_t buff_info__mem_shape_U_32[] = { 32 };
  static const uint32_t buff_info__shape_64_32_1_1[] = { 64, 1, 1, 32 };
  static const uint32_t buff_info__mem_shape_M_64_32_1_1[] = { 64, 2, 1, 1, 16 };
  static const float buff_info_Conv2D_18_weights_quant_scale[] = { 0.00203866371884942, 0.00404962059110403, 0.00271553685888648, 0.00310721714049578, 0.00374164362438023, 0.00353160942904651, 0.0033566365018487, 0.00266002165153623, 0.0022956351749599, 0.00784438941627741, 0.00347393401898444, 0.00392488809302449, 0.00497710378840566, 0.00343335303477943, 0.00136377953458577, 0.00455474387854338, 0.00337005988694727, 0.00224079377949238, 0.00441611791029572, 0.00433038407936692, 0.00379502703435719, 0.00571984844282269, 0.00564999040216208, 0.00330092501826584, 0.00239762244746089, 0.00665309932082891, 0.00251926621422172, 0.00866166315972805, 0.0025321813300252, 0.00263782939873636, 0.00260328617878258, 0.00574759626761079, 0.00266321236267686, 0.00290177972055972, 0.00367054040543735, 0.00429377052932978, 0.00329227885231376, 0.00318157393485308, 0.003796829842031, 0.00621119746938348, 0.00354189961217344, 0.00275121768936515, 0.00357690500095487, 0.00351010402664542, 0.00251981592737138, 0.00284434342756867, 0.00494008930400014, 0.00255080964416265, 0.00224640895612538, 0.00629291776567698, 0.00379831017926335, 0.0028660383541137, 0.00465893838554621, 0.00422489689663053, 0.00407718261703849, 0.00341408583335578, 0.00553281186148524, 0.00306497444398701, 0.00922658015042543, 0.00312432018108666, 0.00390485743992031, 0.00355717539787292, 0.00187918788287789, 0.00359518965706229 };
  static const int16_t buff_info_Conv2D_18_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_64[] = { 1, 1, 64, 1 };
  static const uint32_t buff_info__mem_shape_U_64[] = { 64 };
  static const uint32_t buff_info__shape_128_64_1_1[] = { 128, 1, 1, 64 };
  static const uint32_t buff_info__mem_shape_M_128_64_1_1[] = { 128, 4, 1, 1, 16 };
  static const float buff_info_Conv2D_29_weights_quant_scale[] = { 0.00180236308369786, 0.00269234203733504, 0.00317502743564546, 0.00442770775407553, 0.00187940197065473, 0.00222095963545144, 0.00381506094709039, 0.00223437184467912, 0.0028323067817837, 0.00174861086998135, 0.00518798688426614, 0.00421470170840621, 0.00506828632205725, 0.00217174575664103, 0.00382426753640175, 0.00403572618961334, 0.00263574020937085, 0.00314148026518524, 0.00204515201039612, 0.00265021319501102, 0.00305407005362213, 0.00332276802510023, 0.00210501765832305, 0.00229419278912246, 0.00220787734724581, 0.00291285896673799, 0.00556790642440319, 0.0015905984910205, 0.00647254241630435, 0.00256892736069858, 0.00273151835426688, 0.00194052385631949, 0.00156462611630559, 0.00178093276917934, 0.0019588831346482, 0.0028232685290277, 0.0040121516212821, 0.00239748600870371, 0.00364607875235379, 0.00262593105435371, 0.00247270055115223, 0.00339596904814243, 0.00216872850432992, 0.00271707447245717, 0.00298043270595372, 0.00308574480004609, 0.0027863101568073, 0.00369138224050403, 0.00374029017984867, 0.00237256451509893, 0.00176177243702114, 0.00222610752098262, 0.00544542912393808, 0.00169052788987756, 0.00172365759499371, 0.00612044334411621, 0.00344640715047717, 0.00310729863122106, 0.0023002412635833, 0.00237418455071747, 0.00268825911916792, 0.00464170146733522, 0.0022755372337997, 0.00183307251427323, 0.00335458037443459, 0.00232376507483423, 0.00213313731364906, 0.00209716591052711, 0.00328747066669166, 0.00535055063664913, 0.00266783079132438, 0.00284407543949783, 0.00318166241049767, 0.00276377587579191, 0.00253600277937949, 0.00200785393826663, 0.00429702596738935, 0.00232045701704919, 0.00231624022126198, 0.00332342018373311, 0.00244766962714493, 0.00301400362513959, 0.00179783429484814, 0.00416708784177899, 0.00182592240162194, 0.00530978059396148, 0.00229718792252243, 0.00200404436327517, 0.00193216151092201, 0.00229516252875328, 0.0019894375000149, 0.0035527644213289, 0.00232074107043445, 0.00313331140205264, 0.00267272396013141, 0.00192346365656704, 0.00298476708121598, 0.00393303437158465, 0.00291794841177762, 0.00204329588450491, 0.00189905136357993, 0.00292947562411427, 0.00262234942056239, 0.00420648604631424, 0.00231458875350654, 0.00172519439365715, 0.00370194832794368, 0.00234265276230872, 0.00250145397149026, 0.0015905222389847, 0.00312066171318293, 0.00205864058807492, 0.00425949180498719, 0.00201070308685303, 0.00282773142680526, 0.0033897589892149, 0.00218316609971225, 0.00205255090259016, 0.00296628684736788, 0.0023120145779103, 0.0019780530128628, 0.00195233861450106, 0.00185138231609017, 0.00303228525444865, 0.00279865507036448, 0.00439258106052876, 0.00209294748492539, 0.00400572316721082 };
  static const int16_t buff_info_Conv2D_29_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_128[] = { 1, 1, 128, 1 };
  static const uint32_t buff_info__mem_shape_U_128[] = { 128 };
  static const uint32_t buff_info__shape_128_128_1_1[] = { 128, 1, 1, 128 };
  static const uint32_t buff_info__mem_shape_M_128_128_1_1[] = { 128, 8, 1, 1, 16 };
  static const float buff_info_Conv2D_40_weights_quant_scale[] = { 0.00264545925892889, 0.00277947634458542, 0.0025637405924499, 0.00284342560917139, 0.00270126992836595, 0.00243812776170671, 0.00374083174392581, 0.00701641151681542, 0.00347401131875813, 0.00325852842070162, 0.0027854610234499, 0.00295165088027716, 0.00206174072809517, 0.00328738428652287, 0.001729303621687, 0.00192641688045114, 0.00912007503211498, 0.00264851981773973, 0.00405312795192003, 0.00508766388520598, 0.00291473558172584, 0.00303644756786525, 0.00532001350075006, 0.00150758936069906, 0.0023645309265703, 0.00253051915206015, 0.00221816380508244, 0.00240021850913763, 0.00252883532084525, 0.00330323586240411, 0.0025101532228291, 0.00198775599710643, 0.00293475016951561, 0.00181741535197943, 0.00193873315583915, 0.00233406573534012, 0.00342053128406405, 0.00287779187783599, 0.00222385721281171, 0.00237959437072277, 0.00236169318668544, 0.00290979281999171, 0.0070631206035614, 0.00185818725731224, 0.00155249715317041, 0.00429197121411562, 0.00196639425121248, 0.00158774410374463, 0.00316827301867306, 0.00312842288985848, 0.00239748577587306, 0.00278738304041326, 0.00300047383643687, 0.00355347665026784, 0.004714987706393, 0.00440281629562378, 0.00153824535664171, 0.00190389412455261, 0.00175916159059852, 0.00429013511165977, 0.0017866026610136, 0.00240906793624163, 0.00191432074643672, 0.00197237217798829, 0.00241021765395999, 0.00189481244888157, 0.00314800930209458, 0.00173667259514332, 0.00398081075400114, 0.00527957268059254, 0.00279299262911081, 0.00165656511671841, 0.00240456569008529, 0.00264192512258887, 0.00297279399819672, 0.00278996722772717, 0.00316473003476858, 0.001478370744735, 0.00281986291520298, 0.00220307405106723, 0.00236177793703973, 0.00261153816245496, 0.00353244598954916, 0.00165085948538035, 0.00287095224484801, 0.00398801174014807, 0.00212022569030523, 0.00327032478526235, 0.00171852018684149, 0.00358304497785866, 0.00189385702833533, 0.00240062875673175, 0.00230456376448274, 0.00817377772182226, 0.00224382849410176, 0.00619125971570611, 0.002601737389341, 0.0026767102535814, 0.00256040529347956, 0.00507699185982347, 0.00173400796484202, 0.0018784306012094, 0.00211402610875666, 0.00475404411554337, 0.00635010469704866, 0.00244305934756994, 0.00448520109057426, 0.00245832605287433, 0.00513166934251785, 0.00235072220675647, 0.00173290027305484, 0.00290467008017004, 0.00215287925675511, 0.00435391161590815, 0.00209245597943664, 0.00255712238140404, 0.0015925804618746, 0.00257458258420229, 0.00247905589640141, 0.00201927125453949, 0.00303469179198146, 0.0033174732234329, 0.00231368839740753, 0.00478488160297275, 0.00336007517762482, 0.00219920743256807, 0.00284354691393673, 0.00186143140308559 };
  static const int16_t buff_info_Conv2D_40_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_256_128_1_1[] = { 256, 1, 1, 128 };
  static const uint32_t buff_info__mem_shape_M_256_128_1_1[] = { 256, 8, 1, 1, 16 };
  static const float buff_info_Conv2D_51_weights_quant_scale[] = { 0.00152801640797406, 0.00117807555943727, 0.00261259311810136, 0.00266914023086429, 0.00228498689830303, 0.00196879426948726, 0.00236972235143185, 0.00223243050277233, 0.00269486638717353, 0.00224726903252304, 0.0021539437584579, 0.00207557226531208, 0.00224704458378255, 0.00176516815554351, 0.00177196227014065, 0.00156125007197261, 0.00181977194733918, 0.00144471926614642, 0.00142988585866988, 0.00338195008225739, 0.00177194783464074, 0.00283825397491455, 0.00230030389502645, 0.00173704512417316, 0.00174631911795586, 0.00189732445869595, 0.00169939105398953, 0.00194640981499106, 0.00169203721452504, 0.00140396459028125, 0.00276909722015262, 0.00158353452570736, 0.00177646137308329, 0.00169780687429011, 0.00157119310460985, 0.00187219271901995, 0.00149477575905621, 0.00351918768137693, 0.00241563702002168, 0.00183970329817384, 0.00188348151277751, 0.00453443080186844, 0.00155189051292837, 0.00463182106614113, 0.00440401490777731, 0.00168600981123745, 0.00310180149972439, 0.00167323928326368, 0.00184017850551754, 0.00229206890799105, 0.00174590642563999, 0.00175402918830514, 0.00171448371838778, 0.0022601408418268, 0.00131186028011143, 0.00284381862729788, 0.00153883430175483, 0.00205635069869459, 0.00181651313323528, 0.00236309133470058, 0.0015826626913622, 0.00240367418155074, 0.00410832930356264, 0.0019648508168757, 0.00179229129571468, 0.00206554541364312, 0.00672215409576893, 0.00214483914896846, 0.00172056909650564, 0.00187340180855244, 0.00159973814152181, 0.0021753606852144, 0.00193175580352545, 0.00172653677873313, 0.00196174834854901, 0.0018947982462123, 0.00193889020010829, 0.00154524319805205, 0.00168250733986497, 0.00181578833144158, 0.00188709527719766, 0.00191190734039992, 0.0016895413864404, 0.00187781197018921, 0.00255828537046909, 0.00271017919294536, 0.00222330656833947, 0.00211238977499306, 0.00212496612221003, 0.00280873873271048, 0.00188811961561441, 0.0017718969611451, 0.00228222133591771, 0.00121309526730329, 0.00174589443486184, 0.00184917252045125, 0.00143647368531674, 0.00200442085042596, 0.00264451303519309, 0.00230835867114365, 0.00129976938478649, 0.00234560994431376, 0.0018479471327737, 0.00208532041870058, 0.00263493903912604, 0.00137056701350957, 0.00175650604069233, 0.00160133931785822, 0.00144428934436291, 0.00157292594667524, 0.00484312837943435, 0.00160397530999035, 0.00490367878228426, 0.00484665902331471, 0.00224580336362123, 0.00135176500771195, 0.00141504569910467, 0.00205039442516863, 0.00237465999089181, 0.0017250165110454, 0.00276679848320782, 0.00161425850819796, 0.00210592360235751, 0.00135699729435146, 0.00170750345569104, 0.00194523495156318, 0.00138030748348683, 0.00157536345068365, 0.00174647127278149, 0.0032430172432214, 0.00156527943909168, 0.00235289847478271, 0.00199124566279352, 0.00375873479060829, 0.00471209082752466, 0.00227899313904345, 0.00221227365545928, 0.00158031401224434, 0.00297862803563476, 0.00145054724998772, 0.00416593113914132, 0.00203683995641768, 0.00196291715838015, 0.00186712574213743, 0.00204944750294089, 0.00210796017199755, 0.0016405233182013, 0.0022279703989625, 0.00355267035774887, 0.00322508765384555, 0.00121471704915166, 0.00195762771181762, 0.001801110454835, 0.00141908891964704, 0.00579471839591861, 0.00212843483313918, 0.00194339826703072, 0.00153505557682365, 0.00167345756199211, 0.00183701270725578, 0.00367652764543891, 0.00166088074911386, 0.00132896797731519, 0.00118684337940067, 0.00157189008314162, 0.00360354757867754, 0.00229069916531444, 0.00244635343551636, 0.00207403628155589, 0.00239514140412211, 0.00197161687538028, 0.00594097748398781, 0.00217563193291426, 0.00183383841067553, 0.00159916747361422, 0.00162749795708805, 0.00198724376969039, 0.00238105515018106, 0.00139475089963526, 0.00174388801679015, 0.00200542970560491, 0.00228621996939182, 0.00193913804832846, 0.00318033434450626, 0.00337325618602335, 0.00156234216410667, 0.00188944314140826, 0.00143589486833662, 0.00223912531509995, 0.0017949192551896, 0.00175027770455927, 0.00411680806428194, 0.0020879921503365, 0.00179416860919446, 0.00210288888774812, 0.00207790499553084, 0.00159611518029124, 0.00256094499491155, 0.00180799327790737, 0.00159849179908633, 0.0016259039985016, 0.00155513745266944, 0.00339816394262016, 0.00151207519229501, 0.00513934902846813, 0.0012927126372233, 0.00150886864867061, 0.00132607016712427, 0.0031090637203306, 0.00194762740284204, 0.00282638939097524, 0.00154352548997849, 0.00167527806479484, 0.00223012524656951, 0.00217007473111153, 0.00236668344587088, 0.00281814625486732, 0.00168034841772169, 0.00171245611272752, 0.00196362868882716, 0.00143985310569406, 0.0030637658201158, 0.00417026691138744, 0.00193820882122964, 0.00151566311251372, 0.00246202433481812, 0.00581243401393294, 0.00299776764586568, 0.0026063087861985, 0.00229887198656797, 0.00250216550193727, 0.00437534600496292, 0.00189577124547213, 0.00201585376635194, 0.00157651829067618, 0.00176331086549908, 0.00263708434067667, 0.00199768482707441, 0.00314430752769113, 0.00180946895852685, 0.00189387914724648, 0.00313691422343254, 0.00205263053067029, 0.00154116901103407, 0.00150556664448231, 0.00215706927701831, 0.00193454639520496, 0.00288968835957348, 0.0055461679585278, 0.00436389446258545, 0.00246123736724257, 0.00144770252518356, 0.00247643259353936, 0.00229066051542759, 0.00258331187069416, 0.00236536236479878 };
  static const int16_t buff_info_Conv2D_51_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_256[] = { 1, 1, 256, 1 };
  static const uint32_t buff_info__mem_shape_U_256[] = { 256 };
  static const uint32_t buff_info__shape_256_256_1_1[] = { 256, 1, 1, 256 };
  static const uint32_t buff_info__mem_shape_M_256_256_1_1[] = { 256, 16, 1, 1, 16 };
  static const float buff_info_Conv2D_62_weights_quant_scale[] = { 0.00225038919597864, 0.00208965688943863, 0.00166172697208822, 0.00209743529558182, 0.00251559214666486, 0.00240248604677618, 0.00254596187733114, 0.0018619013717398, 0.00231924583204091, 0.00219308934174478, 0.0038358410820365, 0.00233535701408982, 0.00407325942069292, 0.00213220226578414, 0.00202462822198868, 0.00279264175333083, 0.00188123644329607, 0.00352719728834927, 0.00230585085228086, 0.00200771354138851, 0.00191941647790372, 0.00134662247728556, 0.00266941031441092, 0.00211589969694614, 0.00558793963864446, 0.00110592099372298, 0.0017974607180804, 0.00172247714363039, 0.00209983321838081, 0.00200228858739138, 0.00268381042405963, 0.00218426762148738, 0.00203669746406376, 0.00349272834137082, 0.00283038057386875, 0.00185141013935208, 0.0015088792424649, 0.00169798382557929, 0.00438992492854595, 0.0032618239056319, 0.00211834977380931, 0.00292577035725117, 0.00138800032436848, 0.00314734480343759, 0.00235410337336361, 0.00193258607760072, 0.00295815407298505, 0.00161752116400748, 0.00171104504261166, 0.00261935964226723, 0.00237214914523065, 0.00208549853414297, 0.00277135125361383, 0.00194414460565895, 0.00155877380166203, 0.00253371312282979, 0.00289375986903906, 0.0014125412562862, 0.00345408194698393, 0.00313043105416, 0.00179364637006074, 0.00179932278115302, 0.00187463324982673, 0.00202415604144335, 0.0018113903934136, 0.00217580306343734, 0.00208817864768207, 0.00222572265192866, 0.00199732580222189, 0.00175953109283, 0.0019923229701817, 0.00197963905520737, 0.00324980635195971, 0.00243443949148059, 0.00175685551948845, 0.00177924509625882, 0.00205681403167546, 0.00151298160199076, 0.00170310866087675, 0.00199855258688331, 0.00181271706242114, 0.00293115084059536, 0.00145553157199174, 0.00290596764534712, 0.00310438172891736, 0.00175026629585773, 0.00231115613132715, 0.00192085560411215, 0.00140421139076352, 0.0014150821371004, 0.00209064199589193, 0.00189867930021137, 0.00213087443262339, 0.00172694330103695, 0.00190508202649653, 0.00595928402617574, 0.0017004010733217, 0.00156981963664293, 0.00198272103443742, 0.00176783814094961, 0.00301372050307691, 0.0021410477347672, 0.00194676825776696, 0.00213161366991699, 0.00172393000684679, 0.00173511693719774, 0.00677127810195088, 0.00165331165771931, 0.0025051252450794, 0.00207171402871609, 0.00150233786553144, 0.00203960342332721, 0.00219182460568845, 0.00194089417345822, 0.00175701838452369, 0.00195012683980167, 0.00317918206565082, 0.0027140190359205, 0.00388440093956888, 0.001830491470173, 0.00254257535561919, 0.00216575642116368, 0.001688317861408, 0.00209838058799505, 0.00210010376758873, 0.00197838991880417, 0.00227283500134945, 0.00163535494357347, 0.00158755690790713, 0.00322213815525174, 0.00186504365410656, 0.00152156734839082, 0.00190656271297485, 0.00203161942772567, 0.00198506331071258, 0.00195545144379139, 0.00188761425670236, 0.0017216702690348, 0.00193235860206187, 0.00279510021209717, 0.00347241433337331, 0.00195721513591707, 0.00214630691334605, 0.00220569386146963, 0.00213547283783555, 0.00214348244480789, 0.00363814132288098, 0.00195241300389171, 0.00153819983825088, 0.00525046465918422, 0.0017762795323506, 0.00343457725830376, 0.00206849095411599, 0.00218468927778304, 0.00217683892697096, 0.00192074582446367, 0.00187733757775277, 0.00204742467030883, 0.00189737987238914, 0.00275225727818906, 0.00207110098563135, 0.00244354270398617, 0.00201372639276087, 0.00139951496385038, 0.00204052892513573, 0.00507067004218698, 0.00208851229399443, 0.00148971460293978, 0.00157641095574945, 0.00329100782983005, 0.00165785767603666, 0.00244073383510113, 0.00197007949464023, 0.00204836716875434, 0.00253653060644865, 0.00204641232267022, 0.00206502270884812, 0.00133712717797607, 0.00207216385751963, 0.0032054481562227, 0.00192140624858439, 0.00377091765403748, 0.00204119086265564, 0.00184937857557088, 0.00157421233598143, 0.00179883779492229, 0.00262982863932848, 0.00199597957544029, 0.00212210090830922, 0.00231233611702919, 0.0026013907045126, 0.00240719621069729, 0.00187838962301612, 0.0022402498871088, 0.00301612354815006, 0.00249862461350858, 0.00166732538491488, 0.00165875686798245, 0.00189242756459862, 0.00203188555315137, 0.00264778011478484, 0.00262978859245777, 0.00188768457155675, 0.00185909401625395, 0.00223208102397621, 0.00143358809873462, 0.00239587272517383, 0.001866499427706, 0.00268757552839816, 0.00215769605711102, 0.00173743942286819, 0.00265421718358994, 0.00161427480634302, 0.00173327978700399, 0.00127757014706731, 0.00273015070706606, 0.0018658087356016, 0.00187153241131455, 0.00252390489913523, 0.0018269803840667, 0.00215559150092304, 0.00190313556231558, 0.0050105219706893, 0.00224450160749257, 0.00146598357241601, 0.00260308664292097, 0.00152039586100727, 0.00265934900380671, 0.00136488245334476, 0.00169527030084282, 0.00219882070086896, 0.00190722197294235, 0.00205181655474007, 0.0017709662206471, 0.00312370108440518, 0.00199300306849182, 0.00371770840138197, 0.0036216143053025, 0.00177950947545469, 0.00150777888484299, 0.00244219065643847, 0.00260999402962625, 0.00290720886550844, 0.00207382999360561, 0.00382062303833663, 0.00258662016130984, 0.00190992222633213, 0.00195289147086442, 0.00214340235106647, 0.00222051632590592, 0.0021138486918062, 0.0015079133445397, 0.00160156260244548, 0.00260962056927383, 0.00207572756335139, 0.00149611383676529 };
  static const int16_t buff_info_Conv2D_62_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_512_256_1_1[] = { 512, 1, 1, 256 };
  static const uint32_t buff_info__mem_shape_M_512_256_1_1[] = { 512, 16, 1, 1, 16 };
  static const float buff_info_Conv2D_73_weights_quant_scale[] = { 0.00143722491338849, 0.00110177625901997, 0.00132843002211303, 0.00330434483475983, 0.00136694847606122, 0.00166882446501404, 0.00118689087685198, 0.00189131533261389, 0.00151478336192667, 0.00158172915689647, 0.00241974624805152, 0.00174333539325744, 0.00187064975034446, 0.00149059598334134, 0.00199564499780536, 0.00130091875325888, 0.00288840872235596, 0.00283139431849122, 0.00165338022634387, 0.00363826681859791, 0.00177137390710413, 0.00261271605268121, 0.001254623522982, 0.0014527493622154, 0.00197936617769301, 0.00160035875160247, 0.0015769200399518, 0.00176955689676106, 0.00414289208129048, 0.00114138412754983, 0.00127393496222794, 0.00193568668328226, 0.00174353178590536, 0.00119282014202327, 0.00157753494568169, 0.00148617080412805, 0.00260927528142929, 0.00145492516458035, 0.00146012252662331, 0.00197576428763568, 0.00316225946880877, 0.00171280931681395, 0.00215023080818355, 0.00183491071220487, 0.0014447292778641, 0.00157433818094432, 0.00176990102045238, 0.00181806809268892, 0.00150617444887757, 0.00179634417872876, 0.00173291354440153, 0.00155585398897529, 0.00158718472812325, 0.00232584238983691, 0.00291860708966851, 0.00149269681423903, 0.00133790413383394, 0.00167164532467723, 0.00169920618645847, 0.00339137902483344, 0.00294218282215297, 0.00242802617140114, 0.00177949608769268, 0.0013232072815299, 0.00129212741740048, 0.00252595474012196, 0.00210332148708403, 0.00329042226076126, 0.00264674890786409, 0.00135604757815599, 0.00159816269297153, 0.00230976496823132, 0.00147440726868808, 0.00343245128169656, 0.00228381692431867, 0.00153571390546858, 0.00138807471375912, 0.00429156562313437, 0.00166981131769717, 0.00120212230831385, 0.00185818725731224, 0.00132693024352193, 0.00454534450545907, 0.00208229618147016, 0.0021555763669312, 0.00331591465510428, 0.00139502342790365, 0.00289300642907619, 0.00272608292289078, 0.00134669453836977, 0.00170808390248567, 0.00194621353875846, 0.00150363764259964, 0.00171347218565643, 0.00139668490737677, 0.00171881099231541, 0.00160416320431978, 0.00164720392785966, 0.00179480656515807, 0.00145682122092694, 0.00191867467947304, 0.00409676972776651, 0.00179075531195849, 0.0021128470543772, 0.00137107912451029, 0.00126829708460718, 0.00201144907623529, 0.00170346512459219, 0.00201368308626115, 0.00127654045354575, 0.00321159395389259, 0.0014776554889977, 0.00129755458328873, 0.00162785954307765, 0.00143090926576406, 0.00260098674334586, 0.00159552390687168, 0.0017492015613243, 0.0020563246216625, 0.00206579943187535, 0.00414802320301533, 0.00376412319019437, 0.0019476693123579, 0.00191043992526829, 0.0015100302407518, 0.00182272214442492, 0.00216300203464925, 0.00141523708589375, 0.00197041407227516, 0.00192231766413897, 0.00202938611619174, 0.0013677217066288, 0.00271355174481869, 0.0047327266074717, 0.00225883768871427, 0.00131074571982026, 0.00171326741110533, 0.001451097545214, 0.00157739641144872, 0.00166872225236148, 0.00175537646282464, 0.00218061567284167, 0.00186376192141324, 0.00250814040191472, 0.00204439973458648, 0.0023436825722456, 0.00160231324844062, 0.00182815094012767, 0.00127544393762946, 0.00270368042401969, 0.00153907539788634, 0.0011847804998979, 0.00272543891333044, 0.00238920073024929, 0.00236185407266021, 0.00149260950274765, 0.00159299012739211, 0.00137377355713397, 0.00165050162468106, 0.00151455879677087, 0.00118726573418826, 0.00143351464066654, 0.00172512629069388, 0.00116334529593587, 0.00156625756062567, 0.00225951801985502, 0.00197459943592548, 0.00146545236930251, 0.00161586934700608, 0.00120659521780908, 0.00151212164200842, 0.00171178171876818, 0.00153604266233742, 0.00141008349601179, 0.00158338854089379, 0.00142812367994338, 0.00207174848765135, 0.00131113757379353, 0.00159137649461627, 0.00182297534774989, 0.00314877089112997, 0.00148124934639782, 0.00299549708142877, 0.00379556091502309, 0.00277807470411062, 0.00210723001509905, 0.00152097595855594, 0.00128317635972053, 0.00167762744240463, 0.0015260154614225, 0.00151875719893724, 0.00177637569140643, 0.00157095456961542, 0.00190382939763367, 0.00640500150620937, 0.00207087467424572, 0.00227057305164635, 0.00135425955522805, 0.00165446102619171, 0.00122515298426151, 0.00179216545075178, 0.00223886594176292, 0.00189383886754513, 0.00148801226168871, 0.00308207259513438, 0.00250698672607541, 0.00209859688766301, 0.00138624967075884, 0.00196191831491888, 0.00230281869880855, 0.00190753093920648, 0.00220225914381444, 0.0022580549120903, 0.00374894146807492, 0.00120357272680849, 0.0014224014012143, 0.00174411898478866, 0.00203378405421972, 0.00175418879371136, 0.00203697453252971, 0.00148544413968921, 0.00232226145453751, 0.00148963241372257, 0.00347292888909578, 0.0020610885694623, 0.00276667112484574, 0.00168239139020443, 0.00179957540240139, 0.00172185734845698, 0.00228286348283291, 0.00131813599728048, 0.00120281928684562, 0.00217530084773898, 0.00122107600327581, 0.00159209698904306, 0.00148225715383887, 0.00520765641704202, 0.00120137585327029, 0.00119902763981372, 0.00153569574467838, 0.00112559332046658, 0.00175838160794228, 0.00167511694598943, 0.00139322597533464, 0.00174079544376582, 0.00187682674732059, 0.00352496188133955, 0.00186537741683424, 0.00314413756132126, 0.0017138283001259, 0.00119015853852034, 0.00256628589704633, 0.00136377918533981, 0.00266992952674627, 0.00170295033603907, 0.00211983500048518, 0.00140863133128732, 0.00244396110065281, 0.00205801147967577, 0.00409894157201052, 0.00206211744807661, 0.00186698941979557, 0.00230747717432678, 0.00164336571469903, 0.00180668896064162, 0.00206561083905399, 0.00211199093610048, 0.00252889399416745, 0.00231392937712371, 0.00194264750462025, 0.00180600746534765, 0.00193637225311249, 0.00166821968741715, 0.00144739646930248, 0.0037065027281642, 0.00108456402085721, 0.0017433101311326, 0.00126455223653466, 0.00131087691988796, 0.00251149432733655, 0.00185267196502537, 0.00338639412075281, 0.00176411925349385, 0.00131360744126141, 0.00160753889940679, 0.0017052365001291, 0.00186957477126271, 0.00173329282552004, 0.00323224370367825, 0.00149251543916762, 0.00124105124268681, 0.00128638069145381, 0.00159483251627535, 0.00240193516947329, 0.00152766273822635, 0.00161766691599041, 0.00196119700558484, 0.00117214734200388, 0.00166963832452893, 0.00257361680269241, 0.00236053951084614, 0.00182651006616652, 0.00152510299813002, 0.00195283838547766, 0.00163651292677969, 0.00268316501751542, 0.00160740036517382, 0.00193997903261334, 0.0014672577381134, 0.00296583236195147, 0.00215881154872477, 0.00209115212783217, 0.00324081908911467, 0.00236134883016348, 0.00241703120991588, 0.00261971494182944, 0.00254262052476406, 0.00156913755927235, 0.00157791026867926, 0.00182139524258673, 0.00290233478881419, 0.00189129670616239, 0.00180416542571038, 0.00316233467310667, 0.00215913774445653, 0.00248484243638813, 0.00120925565715879, 0.00435273814946413, 0.00142497976776212, 0.00164584838785231, 0.00177817023359239, 0.0017161100404337, 0.00184802256990224, 0.00126974994782358, 0.00208361749537289, 0.0018100900342688, 0.00157088926061988, 0.00153465697076172, 0.00139016844332218, 0.00160223722923547, 0.00143981096334755, 0.00131973274983466, 0.00172032613772899, 0.00148260372225195, 0.00196860800497234, 0.00159729563165456, 0.00193472229875624, 0.00191997922956944, 0.00373552273958921, 0.00112903432454914, 0.0016826952341944, 0.00145643949508667, 0.0045456038787961, 0.00138779019471258, 0.00194735277909786, 0.00158618902787566, 0.001513835741207, 0.00190702453255653, 0.00166354374960065, 0.00271747610531747, 0.00174309278372675, 0.0012477720156312, 0.00237630074843764, 0.00145622796844691, 0.00181618460919708, 0.00189067760948092, 0.00107253692112863, 0.00145455996971577, 0.00195787637494504, 0.00340243265964091, 0.0031774987000972, 0.00136441981885582, 0.00183449673932046, 0.00309568177908659, 0.0021007398609072, 0.00137583946343511, 0.00209997384808958, 0.00242612790316343, 0.00141723966225982, 0.00187869078945369, 0.00152977462857962, 0.00159068591892719, 0.00182887585833669, 0.00232150568626821, 0.00149247772060335, 0.00216428353451192, 0.00177393853664398, 0.00334642478264868, 0.0027590652462095, 0.00185829866677523, 0.00145357591100037, 0.00272006914019585, 0.00118096824735403, 0.00204684026539326, 0.00177211477421224, 0.00156140245962888, 0.00163563794922084, 0.00274517177604139, 0.00142048951238394, 0.00164363614749163, 0.00361432158388197, 0.0017203992465511, 0.00427474966272712, 0.00136432948056608, 0.0014313020510599, 0.00153439538553357, 0.00207943981513381, 0.00111544644460082, 0.00306725967675447, 0.0015168689424172, 0.00129122298676521, 0.00311272917315364, 0.00194030720740557, 0.00187671289313585, 0.00285563548095524, 0.00169346819166094, 0.0014392469311133, 0.00185055460315198, 0.00244497135281563, 0.0015046305488795, 0.00198416132479906, 0.00167071749456227, 0.00332364696078002, 0.00166429998353124, 0.001444315421395, 0.00155408075079322, 0.00313853286206722, 0.00256985006853938, 0.00188652705401182, 0.00375224160961807, 0.00211206893436611, 0.00151635706424713, 0.00385120580904186, 0.00129942165222019, 0.00149782723747194, 0.00306615117006004, 0.00303217745386064, 0.00225888541899621, 0.00121325463987887, 0.00252095377072692, 0.00243671424686909, 0.00184468284714967, 0.00149251613765955, 0.00115027208812535, 0.00138097861781716, 0.00220845523290336, 0.00181420496664941, 0.00159034610260278, 0.00452270172536373, 0.00162857992108911, 0.0016565810656175, 0.00132600590586662, 0.00315131898969412, 0.00151395378634334, 0.00205228314734995, 0.00180875777732581, 0.00158275093417615, 0.00158519973047078, 0.0022088719997555, 0.00270410440862179, 0.00149303639773279, 0.00218602200038731, 0.00121104368008673, 0.00211528642103076, 0.00179779808968306, 0.00363778392784297, 0.00187146384268999, 0.00213094474747777, 0.00122225400991738, 0.00106116523966193, 0.00147228327114135, 0.00198646052740514, 0.00232049520127475, 0.00189941260032356, 0.00132962653879076, 0.00622481480240822, 0.00165730353910476, 0.0033450408373028, 0.00191540713422, 0.00362614472396672, 0.00114915031008422, 0.00506566278636456, 0.00224019424058497, 0.0033956163097173, 0.00137212872505188, 0.00127728469669819, 0.00368638057261705, 0.00523840263485909, 0.00195568287745118, 0.0020503792911768, 0.00216033519245684, 0.00166320311836898, 0.00171653996221721, 0.00248044240288436, 0.00140238681342453, 0.00293886195868254, 0.00301594450138509, 0.00164239143487066, 0.00569721683859825, 0.00175433920230716, 0.00228350819088519, 0.0016856836155057, 0.00241197622381151, 0.00179786491207778, 0.00144777924288064, 0.00174449093174189, 0.00142827921081334, 0.00206223479472101, 0.00207198807038367, 0.00138670846354216, 0.00146987801417708, 0.00181333173532039 };
  static const int16_t buff_info_Conv2D_73_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_512[] = { 1, 1, 512, 1 };
  static const uint32_t buff_info__mem_shape_U_512[] = { 512 };
  static const uint32_t buff_info__shape_512_512_1_1[] = { 512, 1, 1, 512 };
  static const uint32_t buff_info__mem_shape_M_512_512_1_1[] = { 512, 32, 1, 1, 16 };
  static const float buff_info_Conv2D_84_weights_quant_scale[] = { 0.00119601050391793, 0.00258560827933252, 0.0015823794528842, 0.00105693389195949, 0.00158767343964428, 0.0013266836758703, 0.00188494450412691, 0.00202769367024302, 0.00146586552727968, 0.00133182958234102, 0.00174721470102668, 0.00144263822585344, 0.00207893596962094, 0.00170231924857944, 0.0014601870207116, 0.0018227188847959, 0.00323736621066928, 0.00218773190863431, 0.00192196760326624, 0.00226678955368698, 0.00197251210920513, 0.00121806794777513, 0.0020547395106405, 0.00158337585162371, 0.00291834166273475, 0.00160601816605777, 0.00175325432792306, 0.00110693741589785, 0.0019272631034255, 0.00125301897060126, 0.00167262542527169, 0.00261768372729421, 0.0014571804786101, 0.00146855250932276, 0.00149458542000502, 0.00186806707642972, 0.00129466038197279, 0.00168012152425945, 0.0027566822245717, 0.00130743707995862, 0.00129567563999444, 0.00119473377708346, 0.00157386600039899, 0.00175557751208544, 0.00144101993646473, 0.00210868800058961, 0.00139478861819953, 0.00145075866021216, 0.0012793286005035, 0.00202019792050123, 0.00140287436079234, 0.00121038965880871, 0.00139158417005092, 0.00111189624294639, 0.00167859857901931, 0.0017420738004148, 0.00159559876192361, 0.00141281867399812, 0.00189128029160202, 0.00192431453615427, 0.00157620164100081, 0.00176736805588007, 0.00136599619872868, 0.00132726191077381, 0.00161259504966438, 0.00296851503662765, 0.00120731082279235, 0.00114246527664363, 0.00145354028791189, 0.00141249713487923, 0.00152848230209202, 0.00213295267894864, 0.00172669545281678, 0.00143932271748781, 0.00400898978114128, 0.0025179423391819, 0.00130201154388487, 0.00212732795625925, 0.00187016825657338, 0.00134794169571251, 0.00177868257742375, 0.00217427080497146, 0.00123420194722712, 0.00120401987805963, 0.00160796463023871, 0.00218534539453685, 0.0022930190898478, 0.00169555295724422, 0.00134282163344324, 0.00178664282429963, 0.00113767944276333, 0.00170197896659374, 0.00105294422246516, 0.00164024322293699, 0.00238227378576994, 0.00209699361585081, 0.00108010170515627, 0.00128933973610401, 0.00164853362366557, 0.00164253590628505, 0.00227027130313218, 0.00175168120767921, 0.00250904681161046, 0.00153237883932889, 0.00123736134264618, 0.00282519310712814, 0.00219350075349212, 0.00353696662932634, 0.00134100508876145, 0.00256517878733575, 0.00265779485926032, 0.00246422085911036, 0.00103153509553522, 0.00101073819678277, 0.00225351681001484, 0.00282466248609126, 0.00223466102033854, 0.00189387437421829, 0.00138821569271386, 0.00189636379946023, 0.00159108499065042, 0.00176274834666401, 0.00197342853061855, 0.00127736025024205, 0.00112273183185607, 0.00134377356152982, 0.00159146671649069, 0.00237514800392091, 0.0018054039683193, 0.00122521875891834, 0.00481328833848238, 0.00144974875729531, 0.00142641924321651, 0.00158196175470948, 0.00269120326265693, 0.00133130489848554, 0.00134884670842439, 0.0014640144072473, 0.00174812681507319, 0.00182223401498049, 0.00211606430821121, 0.00137901410926133, 0.00180373224429786, 0.00182184262666851, 0.00155483337584883, 0.00165558990556747, 0.00196262891404331, 0.00112168397754431, 0.00176745420321822, 0.00129744585137814, 0.00160442572087049, 0.00166940374765545, 0.00162639538757503, 0.00133995397482067, 0.00496912840753794, 0.00120941584464163, 0.00204393570311368, 0.00126553175505251, 0.00112792465370148, 0.00331203546375036, 0.00255618640221655, 0.00120989442802966, 0.0015919468132779, 0.00134765286929905, 0.00136507698334754, 0.00169238075613976, 0.00135549146216363, 0.00237202132120728, 0.00123779138084501, 0.00133806804660708, 0.00289057707414031, 0.00134382187388837, 0.00209087273105979, 0.00141925120260566, 0.00149752700235695, 0.00153918517753482, 0.00178261904511601, 0.00153390516061336, 0.00123288691975176, 0.00237243273295462, 0.00179627758916467, 0.00181347993202507, 0.00144758657552302, 0.00145049474667758, 0.00182613835204393, 0.00160411465913057, 0.00120676634833217, 0.00106947496533394, 0.00184572907164693, 0.00142331433016807, 0.00114889454562217, 0.00140649394597858, 0.00111050193663687, 0.00152671453543007, 0.00298954267054796, 0.00170600705314428, 0.00159751740284264, 0.00153493753168732, 0.00123184139374644, 0.00291552860289812, 0.00119677721522748, 0.00156937341671437, 0.00163096841424704, 0.00214606919325888, 0.00160293013323098, 0.0017031489405781, 0.00159660645294935, 0.00157667661551386, 0.00135940208565444, 0.00176740658935159, 0.00348164234310389, 0.00219475408084691, 0.0029200934804976, 0.00272598909214139, 0.0015868004411459, 0.00147575826849788, 0.0011479330714792, 0.00147620413918048, 0.00251726387068629, 0.0011855288175866, 0.00163952121511102, 0.00175587262492627, 0.00174369686283171, 0.00145976315252483, 0.00126188003923744, 0.00127660518046468, 0.00147443276364356, 0.00334080564789474, 0.00170608283951879, 0.00252007972449064, 0.00137319462373853, 0.00127558736130595, 0.00186269031837583, 0.00123758031986654, 0.00132301531266421, 0.00131293304730207, 0.00238807778805494, 0.00128883437719196, 0.00241892808116972, 0.00186634843703359, 0.00216122856363654, 0.00148994789924473, 0.00135995470918715, 0.00147366500459611, 0.00148458161856979, 0.00132486398797482, 0.00208585569635034, 0.0016105982940644, 0.00226618675515056, 0.0025015533901751, 0.00113608420360833, 0.00181647739373147, 0.00145715347025543, 0.00146769802086055, 0.00214779796078801, 0.00177849957253784, 0.00147172599099576, 0.00183802552055568, 0.00288297818042338, 0.00144754268694669, 0.00179156183730811, 0.00100447726435959, 0.00147716409992427, 0.0012715928023681, 0.00418652826920152, 0.00156252831220627, 0.00143262394703925, 0.0015008810441941, 0.00149910803884268, 0.00146209471859038, 0.00193627539556473, 0.00127537781372666, 0.00167208490893245, 0.00259997649118304, 0.00139834627043456, 0.00304018659517169, 0.00134623423218727, 0.00108994962647557, 0.00281689153052866, 0.0010903348447755, 0.00146235409192741, 0.00166568113490939, 0.00121340353507549, 0.00220537278801203, 0.00127906282432377, 0.00388875044882298, 0.00164301670156419, 0.00123891374096274, 0.00134544563479722, 0.00215618382208049, 0.00132112484425306, 0.00300638074986637, 0.00209986907429993, 0.00311099062673748, 0.0021326772402972, 0.00129987066611648, 0.00177315692417324, 0.00116014305967838, 0.00176919077057391, 0.00141745212022215, 0.00156039046123624, 0.0014032501494512, 0.00221546925604343, 0.00127113284543157, 0.00153611274436116, 0.0015017919940874, 0.00151040055789053, 0.00185004237573594, 0.00123410229571164, 0.00126969034317881, 0.00147640402428806, 0.00147596257738769, 0.000988611718639731, 0.0011873422190547, 0.00162681308574975, 0.00178517343010753, 0.00264007924124599, 0.00151980854570866, 0.00106713501736522, 0.00149731931742281, 0.0013535221805796, 0.00188213947694749, 0.00129153195302933, 0.00224212696775794, 0.00110295775812119, 0.00185574917122722, 0.00108721072319895, 0.00438521755859256, 0.00173102202825248, 0.00139792240224779, 0.00307619315572083, 0.00208225450478494, 0.00120883528143167, 0.00166406878270209, 0.00308400811627507, 0.00143185292836279, 0.00169151485897601, 0.0026637502014637, 0.00130786770023406, 0.00133121735416353, 0.00168339221272618, 0.00211826362647116, 0.00169773655943573, 0.00183318636845797, 0.00177876197267324, 0.00295979925431311, 0.00208752509206533, 0.00190432695671916, 0.00285709090530872, 0.00177669909317046, 0.00188446138054132, 0.00133494019974023, 0.00329280155710876, 0.00174476951360703, 0.00159629690460861, 0.00127052573952824, 0.00206563831306994, 0.0016976366750896, 0.00204772292636335, 0.00122132676187903, 0.00209069950506091, 0.00182737456634641, 0.00121863069944084, 0.00214154948480427, 0.00262511544860899, 0.00129642663523555, 0.00148469675332308, 0.00121234136167914, 0.00158070959150791, 0.0018103257752955, 0.00161255663260818, 0.00149087782483548, 0.00193194055464119, 0.00138797413092107, 0.0017298583406955, 0.00179079093504697, 0.00217758724465966, 0.00113454856909811, 0.00187758333049715, 0.00125864008441567, 0.0022172078024596, 0.00184182380326092, 0.00142352283000946, 0.00106182636227459, 0.00159545033238828, 0.0030084028840065, 0.00244911154732108, 0.00201541255228221, 0.00244491943158209, 0.00149129552301019, 0.00133860588539392, 0.00138234521728009, 0.00158620334696025, 0.00142226635944098, 0.00115728296805173, 0.00184649007860571, 0.0021225621458143, 0.0022119702771306, 0.00123983772937208, 0.00277909054420888, 0.00191901286598295, 0.00155036826618016, 0.00150016346015036, 0.0012966274516657, 0.00127872219309211, 0.00104796327650547, 0.00178503629285842, 0.00180376099888235, 0.00107057602144778, 0.00225305184721947, 0.00183376017957926, 0.00107846152968705, 0.00099407322704792, 0.00176819402258843, 0.00227080262266099, 0.00148670375347137, 0.00200086995027959, 0.00130692042876035, 0.00177676579914987, 0.00159693660680205, 0.00160596729256213, 0.00345468567684293, 0.00199162890203297, 0.00137243035715073, 0.0019403554033488, 0.00222310028038919, 0.00154943484812975, 0.00145395984873176, 0.0022293645888567, 0.00320076267234981, 0.00131044920999557, 0.00385881494730711, 0.00205479538999498, 0.00229744496755302, 0.0013140287483111, 0.00180579791776836, 0.00162985746283084, 0.00276023871265352, 0.00178212171886116, 0.001468620961532, 0.00138842535670847, 0.00187593605369329, 0.0044104466214776, 0.00186574598774314, 0.00290383747778833, 0.00146695622242987, 0.00135796412359923, 0.0014103502035141, 0.00112276140134782, 0.00124631950166076, 0.00156563124619424, 0.00127016962505877, 0.00216953014023602, 0.0031005023047328, 0.00550215225666761, 0.00127240782603621, 0.00340696563944221, 0.0015814189100638, 0.00178749195765704, 0.00181216292548925, 0.00125090917572379, 0.00139076693449169, 0.00176870089489967, 0.00135896203573793, 0.00146351498551667, 0.00169330125208944, 0.00243923743255436, 0.00312525779008865, 0.00158136966638267, 0.00149325339589268, 0.00268778274767101, 0.00121926749125123, 0.00155600253492594, 0.0017365823732689, 0.0017979412805289, 0.00224563339725137, 0.00111759139690548, 0.0017030609305948, 0.00181345106102526, 0.00122753286268562, 0.00300358212552965, 0.0019243290880695, 0.00256820535287261, 0.00134761375375092, 0.00307041686028242, 0.00192792853340507, 0.00204911199398339, 0.00219192286022007, 0.00170897343195975, 0.00207869871519506, 0.00191790703684092, 0.00277414824813604, 0.0015336835058406, 0.00611145747825503, 0.00178273487836123, 0.00282523711211979, 0.0026256418786943, 0.00170774222351611, 0.00119876337703317, 0.00129889149684459, 0.00115768343675882, 0.00224598590284586, 0.00159012875519693, 0.00422588083893061, 0.00166074733715504, 0.00162068754434586, 0.00426007341593504, 0.00130351644475013, 0.00143564399331808, 0.00153018091805279, 0.00120107922703028, 0.00237298780120909 };
  static const int16_t buff_info_Conv2D_84_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_95_weights_quant_scale[] = { 0.00138616526965052, 0.00230697821825743, 0.00138058408629149, 0.002130385953933, 0.0016381855821237, 0.00229902379214764, 0.00191647966858, 0.0020674045663327, 0.00227298913523555, 0.00212330184876919, 0.00187892944086343, 0.00284063606522977, 0.00165508885402232, 0.00137544283643365, 0.00244358833879232, 0.00273102498613298, 0.00181020423769951, 0.00149053253699094, 0.00111593049950898, 0.00109217222779989, 0.00201859092339873, 0.00218988908454776, 0.00222296547144651, 0.00171892030630261, 0.00158619682770222, 0.0022378598805517, 0.0023366145323962, 0.00162907177582383, 0.00449515786021948, 0.00155728380195796, 0.00134454376529902, 0.00132115313317627, 0.00174970738589764, 0.00187026429921389, 0.00180560955777764, 0.00125123432371765, 0.00147907203063369, 0.00197852519340813, 0.00182811089325696, 0.00219674385152757, 0.0010245970916003, 0.00235523818992078, 0.00146876962389797, 0.00165854126680642, 0.00291397818364203, 0.00304993754252791, 0.00235056760720909, 0.00229839282110333, 0.00155764259397984, 0.00128069263882935, 0.00247174385003746, 0.00154923170339316, 0.00145443470682949, 0.0014662240864709, 0.00159734056796879, 0.00133045553229749, 0.00143752386793494, 0.00394641514867544, 0.00229188916273415, 0.00160638871602714, 0.00217031198553741, 0.0015745609998703, 0.00113410234916955, 0.00262859766371548, 0.00272645335644484, 0.00191867479588836, 0.00397902727127075, 0.00255910959094763, 0.00205798004753888, 0.00258994102478027, 0.00183769222348928, 0.00193961034528911, 0.00144456943962723, 0.0015700557269156, 0.00165994674898684, 0.0019350714283064, 0.00176915491465479, 0.00135043496266007, 0.00137003045529127, 0.00135633302852511, 0.00186270324047655, 0.00160013139247894, 0.00192140403669327, 0.00192076514940709, 0.00145820470061153, 0.00137812981847674, 0.00152457156218588, 0.00140397413633764, 0.00377374514937401, 0.00204647378996015, 0.00173724722117186, 0.00170271471142769, 0.0023565695155412, 0.00193493464030325, 0.00222239596769214, 0.00130774686113, 0.00208054366521537, 0.00135127943940461, 0.00230890954844654, 0.00153553707059473, 0.00202586897648871, 0.00155366724357009, 0.00122312211897224, 0.00257824081927538, 0.00158933456987143, 0.00317260972224176, 0.00118229340296239, 0.00162581400945783, 0.00146198237780482, 0.00173570844344795, 0.0010360045125708, 0.00144482287578285, 0.00202980288304389, 0.00224458426237106, 0.00115093006752431, 0.00200798525474966, 0.00170267513021827, 0.00250508589670062, 0.00100442115217447, 0.00139775045681745, 0.00209240661934018, 0.00166450592223555, 0.00232655205763876, 0.00181105919182301, 0.001439418643713, 0.00143866788130254, 0.00149532337673008, 0.0019680445548147, 0.00153089046943933, 0.00198731455020607, 0.00183158763684332, 0.00176525313872844, 0.00101888331118971, 0.00127235508989543, 0.00236452487297356, 0.0020974176004529, 0.0014943522401154, 0.00140174804255366, 0.00471684476360679, 0.00136814184952527, 0.00112101447302848, 0.0014862377429381, 0.00335366232320666, 0.00201256549917161, 0.0014975982485339, 0.00126300193369389, 0.00233377702534199, 0.00146638858132064, 0.00159623892977834, 0.00300806108862162, 0.00270586111582816, 0.00112068001180887, 0.00170185940805823, 0.00189691875129938, 0.00358851766213775, 0.00159229338169098, 0.00126545445527881, 0.00185617431998253, 0.0017122101271525, 0.0024532622192055, 0.00143014581408352, 0.00149047980085015, 0.00285238213837147, 0.00188761879689991, 0.00184378237463534, 0.00251762545667589, 0.00167632033117115, 0.00241080462001264, 0.00301167112775147, 0.00315166660584509, 0.00153589865658432, 0.00152857147622854, 0.00126531766727567, 0.00146306108217686, 0.00133739179000258, 0.0015561303589493, 0.00173185812309384, 0.00155188748613, 0.00156096206046641, 0.00187456130515784, 0.0018481413135305, 0.00221460894681513, 0.00237604882568121, 0.00243768352083862, 0.00153559271711856, 0.00169225689023733, 0.00229831133037806, 0.0023869345895946, 0.00137061032000929, 0.00306594138965011, 0.00171449745539576, 0.00202436395920813, 0.00205040327273309, 0.00279391370713711, 0.00210183393210173, 0.00233792047947645, 0.00182464835233986, 0.00147271773312241, 0.00245737587101758, 0.00150616839528084, 0.00121127965394408, 0.00160581944510341, 0.00193733489140868, 0.00138096697628498, 0.00301465392112732, 0.00184911710675806, 0.00124461913947016, 0.00216374290175736, 0.00107429432682693, 0.00146636890713125, 0.00123988324776292, 0.00167520844843239, 0.00273544783703983, 0.00181360787246376, 0.00168650015257299, 0.00160357262939215, 0.00292780948802829, 0.00229209545068443, 0.00134765368420631, 0.00103753013536334, 0.00147343648131937, 0.0019562856759876, 0.00166495563462377, 0.0018226025858894, 0.00270542874932289, 0.00179274508263916, 0.00146464013960212, 0.00214947876520455, 0.00200582598336041, 0.00419814605265856, 0.00147148151881993, 0.00151693401858211, 0.00138387735933065, 0.00266288896091282, 0.00124495837371796, 0.00190186582040042, 0.00193167943507433, 0.00201046746224165, 0.00210135290399194, 0.00202891835942864, 0.002233502920717, 0.00132951559498906, 0.00268849078565836, 0.00144709413871169, 0.00141943781636655, 0.00172948685940355, 0.00157465576194227, 0.0020175848621875, 0.00184124626684934, 0.00155997171532363, 0.00259166629984975, 0.00260093063116074, 0.00140149076469243, 0.00140661152545363, 0.00446821749210358, 0.00237614125944674, 0.00145618373062462, 0.00227479776367545, 0.00174110918305814, 0.00118733686394989, 0.00236936775036156, 0.00192219263408333, 0.00126510031986982, 0.00187116744928062, 0.00134065118618309, 0.00125787477008998, 0.00184692768380046, 0.0015271925367415, 0.00171162141487002, 0.0017522006528452, 0.00178453698754311, 0.00156295637134463, 0.00150817993562669, 0.00128823355771601, 0.00230529229156673, 0.00312897004187107, 0.00135377608239651, 0.0032295286655426, 0.0016872389242053, 0.00118651159573346, 0.00162137043662369, 0.00206010509282351, 0.00133690657094121, 0.00125074712559581, 0.00142336683347821, 0.00160302000585943, 0.00117516412865371, 0.00118775828741491, 0.00171371991746128, 0.00231122132390738, 0.00111732759978622, 0.00157135387416929, 0.00220243283547461, 0.00147466012276709, 0.00185113283805549, 0.00123426283244044, 0.0016271302010864, 0.0052184178493917, 0.00168555160053074, 0.00241092965006828, 0.00140169181395322, 0.00162894173990935, 0.00123301893472672, 0.00226740189827979, 0.0015224542003125, 0.00194263295270503, 0.00118213007226586, 0.00127848610281944, 0.00170914339832962, 0.00295139662921429, 0.00279134651646018, 0.00165947829373181, 0.0012135487049818, 0.00359513750299811, 0.00257117301225662, 0.00156889925710857, 0.00304277753457427, 0.00228215474635363, 0.00720337731763721, 0.0017618068959564, 0.00116755347698927, 0.0016411846736446, 0.00163713097572327, 0.00247127772308886, 0.0016608836594969, 0.00155962270218879, 0.00154977024067193, 0.00331710418686271, 0.00138777017127723, 0.00135879148729146, 0.00163487729150802, 0.00163091369904578, 0.00168874301016331, 0.00192684377543628, 0.00229781214147806, 0.00227263290435076, 0.000985197606496513, 0.00296975718811154, 0.0013975219335407, 0.00245566410012543, 0.00309407059103251, 0.00172313617076725, 0.00464642699807882, 0.00180942262522876, 0.00131856754887849, 0.00260053714737296, 0.00145528663415462, 0.00297333300113678, 0.00176225346513093, 0.00195555295795202, 0.00213421229273081, 0.00120760512072593, 0.00122306728735566, 0.00153347873128951, 0.0020549762994051, 0.00172332383226603, 0.00163808290380985, 0.00161543395370245, 0.00136332865804434, 0.00137063395231962, 0.00129648169968277, 0.00203655404038727, 0.001581079675816, 0.00164002948440611, 0.00167117652017623, 0.00142702541779727, 0.00123970862478018, 0.00133349990937859, 0.00188726594205946, 0.00159775651991367, 0.00194096344057471, 0.00188257684931159, 0.00165353040210903, 0.00165745348203927, 0.00164125801529735, 0.00151550397276878, 0.00182099489029497, 0.00362818408757448, 0.00201441929675639, 0.00143258820753545, 0.0012239838251844, 0.00136462529189885, 0.0017465177224949, 0.00197288882918656, 0.00250434246845543, 0.00225942744873464, 0.00481042871251702, 0.00165786000434309, 0.00128058844711632, 0.00162357266526669, 0.00203339708968997, 0.00165280944202095, 0.00288135511800647, 0.0020255830604583, 0.00179852859582752, 0.00299264444038272, 0.0013809846714139, 0.00154206738807261, 0.00141210854053497, 0.00155282276682556, 0.00181865214835852, 0.00348656787537038, 0.00165792740881443, 0.00184105010703206, 0.00182786385994405, 0.00194659573026001, 0.00134876032825559, 0.00136339606251568, 0.00180468067992479, 0.00123818556312472, 0.00110976933501661, 0.00197098008356988, 0.00190909625962377, 0.00153733172919601, 0.00129378761630505, 0.00175410881638527, 0.00211056997068226, 0.00211054109968245, 0.00548544619232416, 0.0021491851657629, 0.00327311689034104, 0.00227125035598874, 0.00153834221418947, 0.00171175040304661, 0.00172435352578759, 0.00243062316440046, 0.00279208389110863, 0.00131736823823303, 0.00149867101572454, 0.00135365908499807, 0.00124573416542262, 0.00187729764729738, 0.00292948540300131, 0.00229457020759583, 0.00106881267856807, 0.00143113080412149, 0.00143083091825247, 0.00231340038590133, 0.00131258787587285, 0.00353433191776276, 0.00124830123968422, 0.00265928800217807, 0.00218740780837834, 0.00173883175011724, 0.00262746447697282, 0.00145336508285254, 0.00185988552402705, 0.00114472303539515, 0.00188121851533651, 0.00197640317492187, 0.00157883577048779, 0.00197005667723715, 0.00228755897842348, 0.00099996046628803, 0.00144550867844373, 0.00159774417988956, 0.00205075996927917, 0.00181856309063733, 0.00230796006508172, 0.00666091497987509, 0.00124646397307515, 0.00117250822950155, 0.00132677098736167, 0.00229440629482269, 0.00152632710523903, 0.00483894487842917, 0.00196471833623946, 0.00119087053462863, 0.00221444410271943, 0.00236930721439421, 0.00229674368165433, 0.00196732371114194, 0.00189871108159423, 0.0020034066401422, 0.000922812905628234, 0.00197263853624463, 0.00224127806723118, 0.001678696135059, 0.00136493053287268, 0.00125307915732265, 0.0025433748960495, 0.00154509744606912, 0.00202893721871078, 0.00214694254100323, 0.00134464050643146, 0.00215598661452532, 0.00194663903675973, 0.00158135232049972, 0.00270751141943038, 0.00166742352303118, 0.0028283977881074, 0.0020580415148288, 0.00125377019867301, 0.00185245764441788, 0.0013365336926654, 0.0016070397105068, 0.00156316172797233, 0.00117918115574867, 0.00173937727231532, 0.00196814816445112, 0.00164615386165679, 0.00182819622568786, 0.0021643431391567, 0.00156917446292937, 0.00259168073534966, 0.00181665678974241, 0.00179860217031091, 0.00169897417072207, 0.00206373352557421, 0.00210085581056774, 0.00180944532621652, 0.00126127118710428 };
  static const int16_t buff_info_Conv2D_95_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_106_weights_quant_scale[] = { 0.00280290399678051, 0.00225875503383577, 0.00117031997069716, 0.00149456690996885, 0.00230919476598501, 0.00166205735877156, 0.00294311135075986, 0.00233468553051353, 0.00132146303076297, 0.00175846146885306, 0.00195237365551293, 0.00145179335959256, 0.00224180985242128, 0.00166676845401525, 0.00101273239124566, 0.0017912567127496, 0.00162931624799967, 0.00207779207266867, 0.00205557700246572, 0.00136755174025893, 0.00305540557019413, 0.00169067061506212, 0.00118427211418748, 0.00183827278669924, 0.00151236262172461, 0.00174186192452908, 0.00190442195162177, 0.00166104407981038, 0.00176159583497792, 0.00160038482863456, 0.00215250835753977, 0.00135731464251876, 0.0016209325985983, 0.00138358119875193, 0.0016356585547328, 0.00299665378406644, 0.00170477130450308, 0.00183816766366363, 0.00153202621731907, 0.00170760788023472, 0.00136021664366126, 0.00204147631302476, 0.00215972843579948, 0.00291246129199862, 0.00282599381171167, 0.00132129155099392, 0.00178006780333817, 0.00266274763271213, 0.00197686743922532, 0.00189919490367174, 0.00152030750177801, 0.00164002669043839, 0.00236934330314398, 0.00132973887957633, 0.00123076234012842, 0.00198323768563569, 0.00248720333911479, 0.00185465672984719, 0.00426192861050367, 0.00158789346460253, 0.00124513974878937, 0.00153786363080144, 0.00200108718127012, 0.00171349034644663, 0.00156206078827381, 0.00157539930660278, 0.00147099036257714, 0.0018368458840996, 0.0014899205416441, 0.00114814704284072, 0.00227238447405398, 0.00580694107338786, 0.00130710168741643, 0.00233590696007013, 0.00220525241456926, 0.0015949762891978, 0.00130828737746924, 0.00132386304903775, 0.00326298363506794, 0.00152594805695117, 0.00182572624180466, 0.00199510250240564, 0.00264420313760638, 0.00334120355546474, 0.0021369531750679, 0.000968148000538349, 0.00173947599250823, 0.00217044888995588, 0.0015430620405823, 0.00151602167170495, 0.00138563336804509, 0.00100010319147259, 0.00223699701018631, 0.00193130108527839, 0.00197054259479046, 0.00154359312728047, 0.0013178827939555, 0.00248377630487084, 0.00168770435266197, 0.00172579614445567, 0.00218874611891806, 0.00145293306559324, 0.00119185331277549, 0.00202555768191814, 0.00311803095974028, 0.00244644959457219, 0.00228746398352087, 0.0013216125080362, 0.00152254465501755, 0.00187752116471529, 0.00135497725568712, 0.0013038496254012, 0.00169908150564879, 0.00228297337889671, 0.00215346901677549, 0.00112741545308381, 0.00198155571706593, 0.00338748726062477, 0.00109679671004415, 0.00224705343134701, 0.00124265591148287, 0.00328351720236242, 0.00150629470590502, 0.0015912358649075, 0.00165073538664728, 0.00128141057211906, 0.00140098540578038, 0.00117886695079505, 0.00159580656327307, 0.00166981259826571, 0.00225324952043593, 0.00159323739353567, 0.00149813899770379, 0.00140083173755556, 0.00114880560431629, 0.00155151716899127, 0.0016113311285153, 0.00217483309097588, 0.00170659716241062, 0.00214051478542387, 0.0022211077157408, 0.00274641439318657, 0.00216400460340083, 0.00192644016351551, 0.00183892867062241, 0.00238755694590509, 0.00208387081511319, 0.00186704215593636, 0.00138888682704419, 0.00266415323130786, 0.00138270726893097, 0.0024028008338064, 0.00141797796823084, 0.00160351395606995, 0.00326856155879796, 0.00140149926301092, 0.00147761183325201, 0.00302518415264785, 0.00233379192650318, 0.0023400594945997, 0.00204275408759713, 0.00123203033581376, 0.00115622277371585, 0.00126553955487907, 0.00150594033766538, 0.00151198485400528, 0.0047987257130444, 0.00178493733983487, 0.00219661090523005, 0.00204748101532459, 0.00135584152303636, 0.00170777400489897, 0.00137882062699646, 0.00205202773213387, 0.00142494554165751, 0.00195970712229609, 0.00210182252340019, 0.00214835023507476, 0.00156901183072478, 0.00114336481783539, 0.00149448693264276, 0.0034543564543128, 0.0025203456170857, 0.00206016306765378, 0.00263147917576134, 0.00151050556451082, 0.00162853114306927, 0.00290803774259984, 0.00133022468071431, 0.00385398184880614, 0.00259256386198103, 0.00146609323564917, 0.00123582826927304, 0.00173809542320669, 0.00118323101196438, 0.00265842559747398, 0.00140408147126436, 0.0036591358948499, 0.0016406673239544, 0.00216969614848495, 0.00214139907620847, 0.00148562737740576, 0.00216341903433204, 0.00139963789843023, 0.00174177333246917, 0.0016096739564091, 0.00136522972024977, 0.00150871893856674, 0.00133367348462343, 0.00111805996857584, 0.000974775466602296, 0.00174961716402322, 0.00163318461272866, 0.00120520149357617, 0.00144887040369213, 0.00189408683218062, 0.00119788164738566, 0.00179202994331717, 0.0034105246886611, 0.00155611545778811, 0.00169639836531132, 0.0013464615913108, 0.00293000694364309, 0.00159610970877111, 0.00330311944708228, 0.00200054328888655, 0.00132101029157639, 0.00266510085202754, 0.001968280877918, 0.00207210634835064, 0.00214112689718604, 0.00300337304361165, 0.00250302627682686, 0.00173082284163684, 0.00445591285824776, 0.00136254075914621, 0.00187867891509086, 0.00198079529218376, 0.00121410295832902, 0.00156710459850729, 0.00166351767256856, 0.00169906113296747, 0.001646205666475, 0.00134365283884108, 0.00280480808578432, 0.0017696744762361, 0.00206058495678008, 0.00113084260374308, 0.001365109346807, 0.00174062419682741, 0.00159494124818593, 0.00217120000161231, 0.0013092050794512, 0.00141136685851961, 0.00149447028525174, 0.00399299012497067, 0.00105907721444964, 0.00233869440853596, 0.0017647345084697, 0.00127467140555382, 0.00150866084732115, 0.0015005428576842, 0.0017897478537634, 0.001964604947716, 0.00121810322161764, 0.00133606337476522, 0.00326220598071814, 0.00165767956059426, 0.00225326232612133, 0.00255340151488781, 0.00262632383964956, 0.0015756597276777, 0.00125077727716416, 0.00154148635920137, 0.00179704604670405, 0.00153301458340138, 0.00130114576313645, 0.00313312979415059, 0.00181397725827992, 0.00133464089594781, 0.0015475646359846, 0.00140619603917003, 0.00183801294770092, 0.00357185257598758, 0.00168393494095653, 0.00129302416462451, 0.00178506143856794, 0.00246003898791969, 0.00161228457000107, 0.0013691718922928, 0.00140419951640069, 0.00284826615825295, 0.00282995728775859, 0.00172377959825099, 0.00145684194285423, 0.00256364815868437, 0.00138249935116619, 0.00203429465182126, 0.00185246753972024, 0.00283108069561422, 0.00140457530505955, 0.00148947979323566, 0.00112530449405313, 0.0038763196207583, 0.00144536339212209, 0.00116964743938297, 0.00225454289466143, 0.00266334763728082, 0.0013976915506646, 0.0021164184436202, 0.00149164523463696, 0.00287595461122692, 0.00158910616301, 0.00278653879649937, 0.00245677120983601, 0.00133548397570848, 0.00168074329849333, 0.00193801533896476, 0.00135935726575553, 0.00172642013058066, 0.00280411238782108, 0.00275931903161108, 0.00187655189074576, 0.00131714495364577, 0.00188422447536141, 0.00165086751803756, 0.00126421730965376, 0.00130131363403052, 0.00127613567747176, 0.00206078286282718, 0.0017249999800697, 0.00235509616322815, 0.00149107829201967, 0.00234496919438243, 0.00103660463355482, 0.0016005786601454, 0.00149583839811385, 0.00144148350227624, 0.00208655744791031, 0.00139170209877193, 0.00119203457143158, 0.00206735683605075, 0.00185002246871591, 0.00142020406201482, 0.00211196625605226, 0.00172355305403471, 0.00185933674219996, 0.00154245854355395, 0.00159326067660004, 0.002222542418167, 0.00216518389061093, 0.00157341070007533, 0.00170640926808119, 0.00154219463001937, 0.00197102781385183, 0.00218663760460913, 0.00153305998537689, 0.0011809307616204, 0.00244506471790373, 0.00124458479695022, 0.00148015795275569, 0.00184583594091237, 0.0020771031267941, 0.00215723109431565, 0.00148632505442947, 0.00125717686023563, 0.00182635860983282, 0.00290940329432487, 0.00321152503602207, 0.00164763256907463, 0.0017869018483907, 0.00139377277810127, 0.00145678967237473, 0.00153862114530057, 0.00142040546052158, 0.00186909013427794, 0.00175374408718199, 0.00133677991107106, 0.00202032807283103, 0.00120463129132986, 0.00124282506294549, 0.00222037034109235, 0.00245565082877874, 0.00111554283648729, 0.00170528225135058, 0.00120751734357327, 0.00131557846907526, 0.00186718569602817, 0.00141204800456762, 0.00350862997584045, 0.00159123539924622, 0.00155692163389176, 0.00291251321323216, 0.00131892354693264, 0.00227043032646179, 0.00170857657212764, 0.00192053627688438, 0.00220695650205016, 0.00201476877555251, 0.00164781836792827, 0.00181766785681248, 0.00212678336538374, 0.00190636480692774, 0.00134033628273755, 0.00154948781710118, 0.00170707958750427, 0.00115640880540013, 0.00664582895115018, 0.00146613898687065, 0.00143772060982883, 0.00256027700379491, 0.00138038140721619, 0.00110130803659558, 0.00126137060578912, 0.00181161297950894, 0.00159112701658159, 0.00139216391835362, 0.00232029217295349, 0.00260255206376314, 0.00118304963689297, 0.00152618181891739, 0.0021047773770988, 0.00218738708645105, 0.00149306887760758, 0.00287155387923121, 0.00186092767398804, 0.00140780804213136, 0.0018283921526745, 0.00200182106345892, 0.00273945182561874, 0.00298802624456584, 0.00135548145044595, 0.00195094896480441, 0.00161528284661472, 0.00340359262190759, 0.00147427769843489, 0.00174995418637991, 0.00170493475161493, 0.00132594665046781, 0.00228235917165875, 0.00136157823726535, 0.00262002018280327, 0.00146074302028865, 0.00382046587765217, 0.00321471970528364, 0.00147754757199436, 0.00099944113753736, 0.00187614210881293, 0.00218495982699096, 0.0015784646384418, 0.00113816407974809, 0.00241193361580372, 0.00135290611069649, 0.00150014681275934, 0.00215876428410411, 0.00157052441500127, 0.00174174865242094, 0.00169055885635316, 0.00277910311706364, 0.00127679691649973, 0.0019101093057543, 0.00424559926614165, 0.00177361513487995, 0.00249099708162248, 0.00151158834341913, 0.00220073526725173, 0.00156702729873359, 0.0015939031727612, 0.00160592154134065, 0.001135183731094, 0.0019935795571655, 0.00379994418472052, 0.00161598541308194, 0.00154421071056277, 0.00148772483225912, 0.0017095310613513, 0.00224599172361195, 0.00202631368301809, 0.0016268293838948, 0.00356586347334087, 0.00207722326740623, 0.00170680321753025, 0.00202359608374536, 0.00224016443826258, 0.00180816347710788, 0.00161114463116974, 0.00117428286466748, 0.00133732054382563, 0.00106809148564935, 0.00257137254811823, 0.00159717712085694, 0.0017160743009299, 0.00123493396677077, 0.00152539147529751, 0.00185992172919214, 0.00143691175617278, 0.00189839454833418, 0.00203262059949338, 0.00237964512780309, 0.00212616100907326, 0.00255198520608246, 0.00134659418836236, 0.00136396451853216, 0.00198957673273981, 0.00174183666240424, 0.00292575662024319, 0.00128821504767984, 0.00258772331289947, 0.00148944708053023, 0.00113576441071928, 0.00300530297681689, 0.00142314319964498 };
  static const int16_t buff_info_Conv2D_106_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_117_weights_quant_scale[] = { 0.00240948307327926, 0.00139982590917498, 0.00137000344693661, 0.0017741781193763, 0.00124453497119248, 0.00178897450678051, 0.00167123437859118, 0.00284153502434492, 0.00146921363193542, 0.00176898052450269, 0.00154501083306968, 0.00116548547521234, 0.00112586282193661, 0.00425280816853046, 0.00300726480782032, 0.00255247647874057, 0.00220624124631286, 0.00168015947565436, 0.00168192968703806, 0.00133780925534666, 0.00161028385628015, 0.00171451235655695, 0.00157611852046102, 0.00124227069318295, 0.00134160835295916, 0.00110421457793564, 0.00166302837897092, 0.00137397705111653, 0.00200655427761376, 0.00124825921375304, 0.00164160295389593, 0.00117008620873094, 0.00175422464963049, 0.00134144199546427, 0.00130679283756763, 0.00146525597665459, 0.00211458560079336, 0.00252384622581303, 0.0016075634630397, 0.00129645282868296, 0.0010828374652192, 0.00109895272180438, 0.00210696225985885, 0.00180790829472244, 0.0016347982455045, 0.0014055777573958, 0.00248136511072516, 0.0015073549002409, 0.00133331352844834, 0.0023921940010041, 0.00478742877021432, 0.00428702868521214, 0.00196148199029267, 0.0013124217512086, 0.001215705065988, 0.00144002912566066, 0.0027041002176702, 0.00158129504416138, 0.00163246237207204, 0.0020843637175858, 0.00156179862096906, 0.00126802525483072, 0.0015741940587759, 0.00188023608643562, 0.00128792983014137, 0.00169418496079743, 0.0012149402173236, 0.00119478744454682, 0.00157236296217889, 0.00164356315508485, 0.00227941782213748, 0.00255282246507704, 0.00153962546028197, 0.00298164831474423, 0.0010937824845314, 0.00351900164969265, 0.00134811864700168, 0.00145292072556913, 0.0013586146524176, 0.00129945890512317, 0.00217979308217764, 0.0012225906830281, 0.00111305143218488, 0.00123585620895028, 0.00165638944599777, 0.00173267326317728, 0.00143978232517838, 0.001744095236063, 0.00200523063540459, 0.0013429899699986, 0.00289185717701912, 0.00121080328244716, 0.00230453349649906, 0.00106208550278097, 0.00147652626037598, 0.00151301128789783, 0.00254726759158075, 0.00275769014842808, 0.00153274484910071, 0.00209031859412789, 0.00106680695898831, 0.00127975107170641, 0.00177890306804329, 0.00147762324195355, 0.00308408262208104, 0.00132019992452115, 0.00168095924891531, 0.0020198707934469, 0.0016834877897054, 0.0012421173742041, 0.00135820638388395, 0.00115866307169199, 0.00170201307628304, 0.00126298051327467, 0.00184288318268955, 0.00325547927059233, 0.00118088454473764, 0.0012356978841126, 0.00215915124863386, 0.00201955111697316, 0.00143380020745099, 0.00149267492815852, 0.00213080085813999, 0.00114230636972934, 0.0013111726148054, 0.0016993802273646, 0.00163404934573919, 0.00117593561299145, 0.00168741936795413, 0.00109517865348607, 0.00132853456307203, 0.00147545512299985, 0.00190216489136219, 0.00122562889009714, 0.001533048809506, 0.00104952859692276, 0.00133317918516695, 0.00134753086604178, 0.00120247178710997, 0.00210734712891281, 0.00147087418008596, 0.00151690898928791, 0.00502529693767428, 0.00218922947533429, 0.00125365448184311, 0.00212330860085785, 0.00225707096979022, 0.00143331475555897, 0.0014307351084426, 0.00108678452670574, 0.0019738154951483, 0.00157292687799782, 0.00231669284403324, 0.00143875484354794, 0.00118054286576807, 0.00285057094879448, 0.00129788392223418, 0.00153126323129982, 0.00117898255120963, 0.00141834595706314, 0.0016789628425613, 0.00150173367001116, 0.00177192641422153, 0.00512664765119553, 0.00152939092367887, 0.00174142862670124, 0.00137317460030317, 0.00128219020552933, 0.00161591556388885, 0.00190522777847946, 0.00170847249682993, 0.00115658761933446, 0.00117159413639456, 0.00146655540447682, 0.00101952638942748, 0.00130665698088706, 0.0020187224727124, 0.00223504169844091, 0.00217972253449261, 0.00139112665783614, 0.00138261052779853, 0.00141020445153117, 0.001068971818313, 0.00157843413762748, 0.00152332952711731, 0.00184096815064549, 0.00274773198179901, 0.00248543964698911, 0.00118483335245401, 0.00200741994194686, 0.0012743454426527, 0.00198409077711403, 0.00271845562383533, 0.00176141143310815, 0.00285805412568152, 0.00134496123064309, 0.00148710911162198, 0.00185720005538315, 0.00270838174037635, 0.0011496206279844, 0.00291453651152551, 0.00144515105057508, 0.00142504600808024, 0.00187212438322604, 0.00141052063554525, 0.00195535039529204, 0.00117328716441989, 0.0014089901233092, 0.00139088195282966, 0.000953468785155565, 0.00155646330676973, 0.00161945237778127, 0.00507554225623608, 0.00176376081071794, 0.00136070360895246, 0.00171138951554894, 0.00146576261613518, 0.00185129174496979, 0.00182758900336921, 0.00137915194500238, 0.00172442779876292, 0.00125006982125342, 0.00130794860888273, 0.0014895637286827, 0.00112827972043306, 0.00154035096056759, 0.00160219403915107, 0.00174115935806185, 0.00209345808252692, 0.00125101779121906, 0.00181604677345604, 0.0019940291531384, 0.00176400667987764, 0.00234420644119382, 0.00119118334259838, 0.00202965969219804, 0.00121507048606873, 0.00154218228999525, 0.00174958806019276, 0.00192112755030394, 0.00185060489457101, 0.00193111959379166, 0.00125331850722432, 0.00123472965788096, 0.00144929275847971, 0.00173278932925314, 0.0014229939552024, 0.00126157607883215, 0.00193720846436918, 0.00147050397936255, 0.00163184001576155, 0.00148727279156446, 0.00201758672483265, 0.00131333875469863, 0.00162349070888013, 0.00159838236868382, 0.00174131128005683, 0.00158003577962518, 0.00110043562017381, 0.00183661736082286, 0.00170146045275033, 0.00556339370086789, 0.00153994595166296, 0.00133002130314708, 0.00101242517121136, 0.00313896127045155, 0.00132750323973596, 0.00121363834477961, 0.00203574798069894, 0.00183775217738003, 0.00164953176863492, 0.00155247189104557, 0.00259357690811157, 0.00141317956149578, 0.00145744788460433, 0.001343974028714, 0.00143848638981581, 0.00127648119814694, 0.00141573220025748, 0.00355754047632217, 0.00137342733796686, 0.0026946091093123, 0.00174571142997593, 0.00168185064103454, 0.00140537519473583, 0.00145457731559873, 0.00101851252838969, 0.00176961801480502, 0.00152288423851132, 0.00163329462520778, 0.00111918593756855, 0.0021106421481818, 0.00130739086307585, 0.00158482405822724, 0.00163403816986829, 0.00148720166180283, 0.00121669855434448, 0.00153268768917769, 0.00154885486699641, 0.0014098152751103, 0.0019006528891623, 0.00161113019566983, 0.00212218379601836, 0.00162903650198132, 0.0040680430829525, 0.00134843762498349, 0.00149919267278165, 0.00184893910773098, 0.00119608582463115, 0.00161644665058702, 0.00121066032443196, 0.00176551623735577, 0.00223271362483501, 0.00164703011978418, 0.00146022986155003, 0.00120786589104682, 0.00135062902700156, 0.00217807805165648, 0.000988235929980874, 0.00271874573081732, 0.00424630520865321, 0.00199144775979221, 0.00184115942101926, 0.00184233533218503, 0.00154426298104227, 0.00304948701523244, 0.00158480508252978, 0.00267669372260571, 0.00116179208271205, 0.00138546561356634, 0.00116435950621963, 0.0012241048971191, 0.00120737368706614, 0.00138696108479053, 0.00098996062297374, 0.0024457226973027, 0.00127777236048132, 0.00303685548715293, 0.00312141142785549, 0.00302910571917892, 0.00176177709363401, 0.00137088028714061, 0.00251994258724153, 0.00140741060022265, 0.00154087762348354, 0.00145904812961817, 0.00161406747065485, 0.00142040883656591, 0.00118364056106657, 0.00331107852980494, 0.00223853392526507, 0.00119125854689628, 0.00175717961974442, 0.00130815361626446, 0.00176102505065501, 0.00132366560865194, 0.00230907904915512, 0.00161348551046103, 0.00439174519851804, 0.00135275325737894, 0.00150382705032825, 0.00167002133093774, 0.00137010158505291, 0.00157244980800897, 0.0017364778323099, 0.00314656062982976, 0.00181522406637669, 0.00157222466077656, 0.0012738510267809, 0.00334797264076769, 0.00155798124615103, 0.00218215468339622, 0.00121972721535712, 0.00205095438286662, 0.00179402553476393, 0.0018059725407511, 0.00174400804098696, 0.00128027086611837, 0.00123093591537327, 0.00122764846310019, 0.00335782836191356, 0.00276819802820683, 0.00173001294024289, 0.00130520958919078, 0.00443386053666472, 0.00149797345511615, 0.00159189011901617, 0.00102924136444926, 0.00110320560634136, 0.00120685820002109, 0.00161702604964375, 0.00193557189777493, 0.00137110217474401, 0.00181771232746542, 0.00185220828279853, 0.00135421997401863, 0.00176710332743824, 0.0021304739639163, 0.00193598587065935, 0.00136313610710204, 0.0015871818177402, 0.0016393328551203, 0.00183112535160035, 0.00208280142396688, 0.00233513209968805, 0.00140947173349559, 0.00310157146304846, 0.00219540460966527, 0.00128456857055426, 0.00166895089205354, 0.00133321026805788, 0.00147976656444371, 0.00145840609911829, 0.00140797777567059, 0.00165630178526044, 0.00179061968810856, 0.00204229052178562, 0.0012602349743247, 0.00202883454039693, 0.00146402895916253, 0.00110603461507708, 0.00141842593438923, 0.00181505572982132, 0.00150126172229648, 0.00139258801937103, 0.00158829626161605, 0.00149635330308229, 0.00226583960466087, 0.00143899617251009, 0.00152196548879147, 0.00209703878499568, 0.00249427324160933, 0.00134732795413584, 0.00271207583136857, 0.00442471075803041, 0.00146743399091065, 0.00175270542968065, 0.0015639829216525, 0.00101304263807833, 0.0026003597304225, 0.00286197126843035, 0.00162959634326398, 0.0014323687646538, 0.00179571623448282, 0.00130884570535272, 0.00156722555402666, 0.00159536639694124, 0.00194582366384566, 0.00146677228622139, 0.00116529513616115, 0.00154375634156168, 0.00171664194203913, 0.00264454982243478, 0.00350954523310065, 0.00117907987441868, 0.00209356914274395, 0.00170384009834379, 0.00257547642104328, 0.00115716829895973, 0.00156459130812436, 0.00127502135001123, 0.00142138672526926, 0.00195673573762178, 0.0017937746597454, 0.00146433745976537, 0.00102735578548163, 0.00121143006253988, 0.00115105288568884, 0.00242385920137167, 0.00414386810734868, 0.00224234303459525, 0.00168960215523839, 0.00108987186104059, 0.00115388259291649, 0.00386127131059766, 0.00160035991575569, 0.00277157430537045, 0.00313319847919047, 0.00133838597685099, 0.00204242113977671, 0.00112650217488408, 0.00350506021641195, 0.00131744984537363, 0.00221985438838601, 0.00150747748557478, 0.00149724504444748, 0.00139009847771376, 0.00200386741198599, 0.00166645005811006, 0.00285035348497331, 0.00164897996000946, 0.00152338237967342, 0.0011806630063802, 0.00126235047355294, 0.00118330924306065, 0.00218284758739173, 0.00158186245243996, 0.00154587300494313, 0.00136668910272419, 0.00222626305185258, 0.00211894325911999, 0.00156578386668116, 0.00146806123666465, 0.00158356199972332, 0.0013648335589096, 0.0016498314216733, 0.00130416930187494, 0.00130379619076848, 0.00132972712162882, 0.00148106960114092, 0.00259203836321831, 0.00126847357023507 };
  static const int16_t buff_info_Conv2D_117_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_128_weights_quant_scale[] = { 0.0015371412737295, 0.00119619537144899, 0.00301582575775683, 0.00139064795803279, 0.0021710111759603, 0.00178456422872841, 0.00105004943907261, 0.00201973901130259, 0.00211542216129601, 0.00250067189335823, 0.0013826871290803, 0.00103176815900952, 0.00142970227170736, 0.00143328972626477, 0.0017991365166381, 0.00202956143766642, 0.00164903060067445, 0.00167418469209224, 0.00113179336767644, 0.00153241236694157, 0.00124920147936791, 0.00193727691657841, 0.00123837811406702, 0.00112430891022086, 0.00230528879910707, 0.00151964335236698, 0.00246753403916955, 0.00111969350837171, 0.00124882825184613, 0.00114909908734262, 0.00217445241287351, 0.00167845259420574, 0.00116156309377402, 0.00137155444826931, 0.001209931448102, 0.00121800845954567, 0.00123621011152864, 0.00352769368328154, 0.000985240214504302, 0.00251850066706538, 0.00124719552695751, 0.0041180863045156, 0.00140963308513165, 0.00153588165994734, 0.00138867064379156, 0.00150928110815585, 0.00125391571782529, 0.00129965262021869, 0.0022683865390718, 0.00152206048369408, 0.0011193169048056, 0.00184421380981803, 0.00275662681087852, 0.00145906617399305, 0.00127822218928486, 0.00113631307613105, 0.00102364073973149, 0.00141304614953697, 0.00150485604535788, 0.00149488507304341, 0.00118801556527615, 0.00105208496097475, 0.00114854273851961, 0.00262508797459304, 0.00105000229086727, 0.00101137114688754, 0.00145980983506888, 0.0013961490476504, 0.00127767666708678, 0.00398291787132621, 0.00258594751358032, 0.00127740739844739, 0.00145179545506835, 0.00132143392693251, 0.00145627767778933, 0.00115085043944418, 0.00133431330323219, 0.00203713774681091, 0.00244708941318095, 0.00182754499837756, 0.00204188632778823, 0.001186428591609, 0.00157691352069378, 0.00151260558050126, 0.00127930019516498, 0.00158411206211895, 0.00121488783042878, 0.0014861497329548, 0.00150350423064083, 0.00108253094367683, 0.00154516997281462, 0.00143243616912514, 0.00110317335929722, 0.000967292115092278, 0.00524145178496838, 0.00106029619928449, 0.00245151133276522, 0.00152909092139453, 0.00122694077435881, 0.00148681423161179, 0.00355358840897679, 0.00111744983587414, 0.0018632992869243, 0.00107811985071748, 0.00108025898225605, 0.00300097558647394, 0.00110728968866169, 0.00148539885412902, 0.00188051245640963, 0.00191299617290497, 0.00143632991239429, 0.00164842512458563, 0.00163344980683178, 0.00113244797103107, 0.00183527427725494, 0.00111475843004882, 0.00183662329800427, 0.00125126459170133, 0.00118792208377272, 0.00155546562746167, 0.00204993574880064, 0.00118140445556492, 0.00114660931285471, 0.0011767849791795, 0.00152726680971682, 0.00093763304175809, 0.00171490805223584, 0.00111436669249088, 0.0016213363269344, 0.00120080693159252, 0.00117990234866738, 0.00161021971143782, 0.00124174240045249, 0.00255084247328341, 0.00162076181732118, 0.00107617315370589, 0.00243639061227441, 0.00292077916674316, 0.00134743796661496, 0.00203650118783116, 0.00174267787951976, 0.00166847812943161, 0.00284621957689524, 0.00162128114607185, 0.00130839634221047, 0.00183471071068197, 0.00123379018623382, 0.00163320288993418, 0.00157041219063103, 0.000937196949962527, 0.00130801962222904, 0.00207179901190102, 0.000872284872457385, 0.00160319788847119, 0.00208208523690701, 0.00111970375292003, 0.00112716748844832, 0.00173457339406013, 0.00167135312221944, 0.00355803780257702, 0.0014049761230126, 0.00106507621239871, 0.00126917590387166, 0.00115944258868694, 0.000937456381507218, 0.00119234202429652, 0.00224951840937138, 0.00130042445380241, 0.00512406276538968, 0.00183665566146374, 0.00249087857082486, 0.00146282673813403, 0.00155942304991186, 0.00201255572028458, 0.00234051630832255, 0.00184499053284526, 0.00104939134325832, 0.00130550493486226, 0.00112433894537389, 0.00125003338325769, 0.00148122443351895, 0.00128266541287303, 0.00275583029724658, 0.00140538648702204, 0.00255557545460761, 0.00137132010422647, 0.00140246306546032, 0.00211100862361491, 0.00179863930679858, 0.00143419590312988, 0.00174785952549428, 0.00108818290755153, 0.00136399921029806, 0.00129581848159432, 0.0013141700765118, 0.00141921173781157, 0.00276314583607018, 0.00120076606981456, 0.00126481137704104, 0.00116950401570648, 0.00101714301854372, 0.0012102525215596, 0.00182954513002187, 0.00171558326110244, 0.00124770379625261, 0.00121220550499856, 0.00190242100507021, 0.00118364719673991, 0.0016591907478869, 0.00146889116149396, 0.0013557382626459, 0.00332140922546387, 0.00105879362672567, 0.00126248563174158, 0.00121019023936242, 0.00227323430590332, 0.00204176036641002, 0.00133417127653956, 0.00112364732194692, 0.00127008208073676, 0.00129399506840855, 0.00125107017811388, 0.00151819747406989, 0.00195901188999414, 0.00291869300417602, 0.00177224539220333, 0.00155282532796264, 0.00169502035714686, 0.00125003629364073, 0.00118528597522527, 0.00192760943900794, 0.00120204174891114, 0.00195936905220151, 0.00186103652231395, 0.00335656898096204, 0.00143411895260215, 0.00155509728938341, 0.00191073201131076, 0.00265874457545578, 0.00184722861740738, 0.00216309423558414, 0.00179230293724686, 0.0021015414968133, 0.00131790211889893, 0.00216888543218374, 0.00139342411421239, 0.00112962676212192, 0.00282962061464787, 0.00120628054719418, 0.0020365952514112, 0.00222670589573681, 0.00143943645525724, 0.00110506801865995, 0.00245780893601477, 0.00160864356439561, 0.00138619949575514, 0.00116636673919857, 0.0012967538787052, 0.00130206951871514, 0.000978381605818868, 0.0011598514392972, 0.00108135549817234, 0.00271447259001434, 0.00154696789104491, 0.00232371129095554, 0.0013587293215096, 0.00105458509642631, 0.0024086523335427, 0.00177023070864379, 0.00177407823503017, 0.00227259192615747, 0.00135754642542452, 0.00118078768718988, 0.00184153311420232, 0.00158472044859082, 0.00157740933354944, 0.00236677471548319, 0.00122827175073326, 0.0032359913457185, 0.00311369448900223, 0.00223923870362341, 0.00126881746109575, 0.00123167561832815, 0.00139453902374953, 0.00283409561961889, 0.0021280231885612, 0.00110050977673382, 0.00141144311055541, 0.00119898107368499, 0.0011263876222074, 0.00167003984097391, 0.00172436772845685, 0.00161977182142437, 0.00118747155647725, 0.00162126950453967, 0.0011241037864238, 0.0012169178808108, 0.00171085551846772, 0.00119413051288575, 0.00160444399807602, 0.00205785385333002, 0.00222638808190823, 0.00183917535468936, 0.00161440530791879, 0.00193030212540179, 0.00130096019711345, 0.0011709799291566, 0.0016060316702351, 0.00128876208327711, 0.00184310972690582, 0.00117364327888936, 0.00204259459860623, 0.00158546736929566, 0.00161273998674005, 0.00177291966974735, 0.00129098037723452, 0.00165400037076324, 0.00135769229382277, 0.00111606495920569, 0.00151132419705391, 0.00200476357713342, 0.00210369122214615, 0.00174031779170036, 0.00116672879084945, 0.00230392836965621, 0.0010423717321828, 0.00129297072999179, 0.00111508695408702, 0.00232552783563733, 0.00460327789187431, 0.00219279550947249, 0.00104713626205921, 0.00158695189747959, 0.00144033553078771, 0.00111061544157565, 0.001240762649104, 0.00238825497217476, 0.00198471173644066, 0.0020608848426491, 0.00210360181517899, 0.00136032712180167, 0.00113747804425657, 0.00137024407740682, 0.00118379027117044, 0.00188916362822056, 0.00132223963737488, 0.0010369032388553, 0.00183046539314091, 0.00234919064678252, 0.00192074186634272, 0.00176464056130499, 0.00239328877069056, 0.00109345756936818, 0.00159665988758206, 0.00158001214731485, 0.0014231251552701, 0.00203043478541076, 0.00206468999385834, 0.00133696326520294, 0.00242232228629291, 0.00135035871062428, 0.00117430055979639, 0.00114439451135695, 0.00556384166702628, 0.00239262916147709, 0.00148450641427189, 0.00149282661732286, 0.00140935438685119, 0.00146065023727715, 0.00112828589044511, 0.0031804817263037, 0.00130610691849142, 0.00180812587495893, 0.00133750063832849, 0.00139234203379601, 0.00123824854381382, 0.00167904712725431, 0.00238418858498335, 0.00131170509848744, 0.00119601807091385, 0.00173381122294813, 0.00415313942357898, 0.0012035951949656, 0.00380316423252225, 0.00150032481178641, 0.00127982534468174, 0.00137515226379037, 0.00143457110971212, 0.000960517500061542, 0.00187020387966186, 0.00250426470302045, 0.0014897717628628, 0.00163521175272763, 0.0015975086716935, 0.00295381643809378, 0.00166401558090001, 0.00118487724103034, 0.0013039184268564, 0.00136640516575426, 0.00141583883669227, 0.00156263646204025, 0.00135617831256241, 0.00153727212455124, 0.0014035755302757, 0.00133369001559913, 0.00144575419835746, 0.00123599357903004, 0.00191397138405591, 0.0018130955286324, 0.00153308873996139, 0.0012488717911765, 0.000874037039466202, 0.00406128074973822, 0.00160556961782277, 0.00129169097635895, 0.00122765125706792, 0.00160947267431766, 0.00119948107749224, 0.00115821918006986, 0.0012400271371007, 0.00132166873663664, 0.00214460212737322, 0.00112059945240617, 0.00159401190467179, 0.0012272180756554, 0.000947923341300339, 0.00191817188169807, 0.00164884992409497, 0.00115085986908525, 0.00242313789203763, 0.00144869205541909, 0.00149208155926317, 0.00193678215146065, 0.00127846584655344, 0.00119652622379363, 0.00137563666794449, 0.00142742297612131, 0.00454656267538667, 0.00118702289182693, 0.00138242554385215, 0.00133865640964359, 0.001778973499313, 0.00188466894906014, 0.00124785199295729, 0.00114047399256378, 0.00238349847495556, 0.00191172608174384, 0.00103343650698662, 0.00209969142451882, 0.00116096076089889, 0.00142574263736606, 0.00182931183371693, 0.00114515901077539, 0.00202045263722539, 0.00149028666783124, 0.00179511005990207, 0.00132729124743491, 0.00123850197996944, 0.00195407611317933, 0.00122063769958913, 0.00115374883171171, 0.00140226795338094, 0.00114673271309584, 0.0013770330697298, 0.00124858319759369, 0.00144639029167593, 0.00121197768021375, 0.00178636563941836, 0.0013642372796312, 0.0016111359000206, 0.00104530889075249, 0.00134271220304072, 0.00198784121312201, 0.00159950426314026, 0.00114322640001774, 0.0018743856344372, 0.00208218093030155, 0.000804833369329572, 0.00209070090204477, 0.00113637850154191, 0.00133284961339086, 0.00146766321267933, 0.00306769483722746, 0.00234577571973205, 0.00149855646304786, 0.00111458555329591, 0.00170553033240139, 0.00101189280394465, 0.00197827164083719, 0.00164282228797674, 0.00150154938455671, 0.00197724881581962, 0.00159859505947679, 0.00136581226252019, 0.001356152119115, 0.00119159964378923, 0.001634735846892, 0.00208336510695517, 0.00145340641029179, 0.00122893124353141, 0.00135948648676276, 0.00125599873717874, 0.00117318984121084, 0.00193361495621502, 0.001493064686656, 0.00211918610148132, 0.00180339359212667, 0.00122999923769385, 0.00276615098118782, 0.00116799422539771, 0.00166379194706678, 0.00151333364192396 };
  static const int16_t buff_info_Conv2D_128_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_1024_512_1_1[] = { 1024, 1, 1, 512 };
  static const uint32_t buff_info__mem_shape_M_1024_512_1_1[] = { 1024, 32, 1, 1, 16 };
  static const float buff_info_Conv2D_139_weights_quant_scale[] = { 0.00122319452930242, 0.00275238137692213, 0.000860998639836907, 0.00137780560180545, 0.000859559630043805, 0.000882920518051833, 0.00090553960762918, 0.00114918791223317, 0.000913458119612187, 0.00147171597927809, 0.000946067739278078, 0.00123794283717871, 0.000965995481237769, 0.00117951876018196, 0.00164146767929196, 0.00109409948345274, 0.00194833672139794, 0.00131252617575228, 0.00101267069112509, 0.00154708453919739, 0.000912916788365692, 0.00113116262946278, 0.00125355226919055, 0.00122207077220082, 0.0011902847327292, 0.000842403329443187, 0.00132678961381316, 0.00111839920282364, 0.000848125491756946, 0.00106252823024988, 0.00168111582752317, 0.00195463583804667, 0.00142749946098775, 0.000925470842048526, 0.00109700497705489, 0.00117063743527979, 0.000932812341488898, 0.00152343465015292, 0.00100477726664394, 0.00105849513784051, 0.00105621840339154, 0.00196932535618544, 0.00193768821191043, 0.000984436483122408, 0.00103582185693085, 0.00146014615893364, 0.00107471772935241, 0.00120029330719262, 0.000784495903644711, 0.00098404532764107, 0.00120497297029942, 0.00118238932918757, 0.00172172486782074, 0.00147071329411119, 0.00121031841263175, 0.00097773945890367, 0.000873827142640948, 0.00285252602770925, 0.00117221078835428, 0.00164521136321127, 0.00110811833292246, 0.000747757847420871, 0.00116948655340821, 0.000794978928752244, 0.000955391733441502, 0.00146119040437043, 0.00226725614629686, 0.000803475675638765, 0.000841213739477098, 0.0010484877275303, 0.00115164101589471, 0.00126240507233888, 0.0014265252975747, 0.00094557338161394, 0.00128605985082686, 0.00210643163882196, 0.00105215737130493, 0.00126333825755864, 0.000878564489539713, 0.00135722325649112, 0.00103001075331122, 0.00238449568860233, 0.00111098680645227, 0.00108824134804308, 0.00194971263408661, 0.00198722048662603, 0.00298073026351631, 0.00149156711995602, 0.0026943776756525, 0.000874854275025427, 0.00107536395080388, 0.000978072639554739, 0.00289060524664819, 0.00100489635951817, 0.00288253370672464, 0.00111857603769749, 0.000695016758982092, 0.000819162698462605, 0.00102925417013466, 0.00130542472470552, 0.00112601928412914, 0.000867839902639389, 0.000810391327831894, 0.0009707459830679, 0.00118325487710536, 0.00136854697484523, 0.00083487608935684, 0.00092307769227773, 0.00125079904682934, 0.000862047250848264, 0.00103381869848818, 0.0014746094821021, 0.00147023203317076, 0.000981745775789022, 0.00103667087387294, 0.000938178971409798, 0.000887422356754541, 0.0021668488625437, 0.00094306212849915, 0.000931927526835352, 0.000991749111562967, 0.000863091787323356, 0.00127503951080143, 0.000859506602864712, 0.00100421346724033, 0.000822541245725006, 0.000927175395190716, 0.00122193002607673, 0.00117072346620262, 0.00132503767963499, 0.00204786844551563, 0.00103139225393534, 0.00128790934104472, 0.00121991115156561, 0.00110534252598882, 0.0014913174090907, 0.000897228135727346, 0.00122967234347016, 0.0010091329459101, 0.0022927294485271, 0.00133558898232877, 0.00150857272092253, 0.00125866627786309, 0.00103036419022828, 0.00147665932308882, 0.0011278543388471, 0.00133878237102181, 0.00164882687386125, 0.00102503085508943, 0.000839519780129194, 0.00134699977934361, 0.000955214316491038, 0.00100551219657063, 0.0010844956850633, 0.00134764856193215, 0.00138794921804219, 0.00165551190730184, 0.00104965246282518, 0.000870596733875573, 0.00156128290109336, 0.00100541824940592, 0.00138511345721781, 0.00118269876111299, 0.00094133015954867, 0.00149646284990013, 0.000770484330132604, 0.000948034983593971, 0.000894909666385502, 0.00191197439562529, 0.00156789110042155, 0.0007439223700203, 0.00193383102305233, 0.00113519967999309, 0.000971654080785811, 0.00115603266749531, 0.00101892743259668, 0.00164440262597054, 0.00130963721312582, 0.00145635392982513, 0.00109990790951997, 0.00189395959023386, 0.000828159274533391, 0.000994637724943459, 0.000772811647038907, 0.00127649062778801, 0.00123001495376229, 0.00150208733975887, 0.00123847869690508, 0.00131235108710825, 0.000936571334023029, 0.000971838482655585, 0.000849272240884602, 0.00215952866710722, 0.000751028477679938, 0.000837360508739948, 0.00107225135434419, 0.00176946877036244, 0.00143245130311698, 0.00138969486579299, 0.00120683980640024, 0.00115878926590085, 0.000788913806900382, 0.0012131086550653, 0.000906016968656331, 0.00090681342408061, 0.00167365744709969, 0.00216678366996348, 0.00089252827456221, 0.00108142103999853, 0.000942535523790866, 0.00124521239195019, 0.00140136538539082, 0.00269654719159007, 0.00166472734417766, 0.00114087760448456, 0.00116378371603787, 0.0014230078086257, 0.00223641120828688, 0.00115700426977128, 0.00109741813503206, 0.00137168099172413, 0.00111569801811129, 0.0019413847476244, 0.00150349212344736, 0.00101080699823797, 0.00144094903953373, 0.000925807747989893, 0.00103030435275286, 0.000989100430160761, 0.00101706269197166, 0.000853936362545937, 0.000906272965949029, 0.000924745167139918, 0.000843145535327494, 0.00304308137856424, 0.00107308942824602, 0.00157875544391572, 0.0013590274611488, 0.00150912231765687, 0.00154986116103828, 0.0013535744510591, 0.000940389232710004, 0.000864865665789694, 0.00205631298013031, 0.00172601174563169, 0.00109238235745579, 0.0013197532389313, 0.000897799618542194, 0.000987525563687086, 0.000888506474439055, 0.000696515198796988, 0.00104413810186088, 0.00128922355361283, 0.00258199940435588, 0.00120334944222122, 0.00114884355571121, 0.00120347831398249, 0.00123504246585071, 0.00142526731360704, 0.000748844060581177, 0.00106866913847625, 0.000965344661381096, 0.00114288565237075, 0.00104343029670417, 0.000949217879679054, 0.00111254479270428, 0.000884895445778966, 0.000977822463028133, 0.00118007464334369, 0.000925364089198411, 0.0013548614224419, 0.00127730402164161, 0.00266890646889806, 0.00145818956661969, 0.000882072956301272, 0.00135438656434417, 0.00122067250777036, 0.00121974234934896, 0.0015962312463671, 0.00087650801287964, 0.000699247699230909, 0.000882861670106649, 0.0013058939948678, 0.000919279409572482, 0.00117992702871561, 0.00107274611946195, 0.000928454450331628, 0.00105962005909532, 0.00106156675610691, 0.00191872392315418, 0.0016659873072058, 0.00121340283658355, 0.00128916220273823, 0.00117629626765847, 0.00111965136602521, 0.00106820301152766, 0.000894779514055699, 0.000946785439737141, 0.000896462297532707, 0.00129679206293076, 0.000739714887458831, 0.000889771152287722, 0.000825004070065916, 0.000792945444118232, 0.000871885451488197, 0.00194965302944183, 0.000867559050675482, 0.000840896915178746, 0.000981212593615055, 0.00095746258739382, 0.00114464922808111, 0.00122780492529273, 0.000964417413342744, 0.00295748421922326, 0.000985131715424359, 0.000907353300135583, 0.00186770688742399, 0.00171887699980289, 0.000862663320731372, 0.00118422077503055, 0.00100282626226544, 0.00109710614196956, 0.00146089936606586, 0.00118240853771567, 0.00107285613194108, 0.00100730254780501, 0.00105091300792992, 0.00108176248613745, 0.00108919816557318, 0.00101956853177398, 0.00200689560733736, 0.00097756483592093, 0.00167362194042653, 0.00138812197837979, 0.00108395423740149, 0.00138699333183467, 0.000877473328728229, 0.00175576086621732, 0.00174109474755824, 0.0011052368208766, 0.00109486060682684, 0.0013094152091071, 0.000870871357619762, 0.000900454120710492, 0.000983704114332795, 0.000996118760667741, 0.00110788817983121, 0.000945709296502173, 0.000871515891049057, 0.000958481105044484, 0.0016367785865441, 0.0010965031106025, 0.0010812048567459, 0.000941635807976127, 0.00119127135258168, 0.00124654162209481, 0.00175113626755774, 0.00165985117200762, 0.00129989767447114, 0.00129412312526256, 0.00203892029821873, 0.0022019159514457, 0.00115270493552089, 0.00110165763180703, 0.00122214457951486, 0.00118958426173776, 0.00112649146467447, 0.00135402660816908, 0.00160603667609394, 0.00068355753319338, 0.0011453558690846, 0.00170177721884102, 0.000835117301903665, 0.00113134598359466, 0.000991455162875354, 0.00111024745274335, 0.00103643222246319, 0.00092003180179745, 0.00304240291006863, 0.00133563170675188, 0.00107879925053567, 0.0012773594353348, 0.00102801551111042, 0.00151679140981287, 0.000739968556445092, 0.00133902975358069, 0.00187866296619177, 0.000977943534962833, 0.000810714554972947, 0.00150250375736505, 0.000940254656597972, 0.000701156677678227, 0.00142019300255924, 0.000877796032000333, 0.000733653840143234, 0.00117647019214928, 0.00109773105941713, 0.00122517440468073, 0.00100793619640172, 0.000953670940361917, 0.00133296684361994, 0.0019714767113328, 0.00113863893784583, 0.000821855326648802, 0.00325029110535979, 0.000785107142291963, 0.00115699134767056, 0.0010308469645679, 0.0010069782147184, 0.00168303586542606, 0.000943874358199537, 0.000846463430207223, 0.00124770682305098, 0.000851918128319085, 0.00142648641485721, 0.00124887900892645, 0.000747074256651103, 0.00131975929252803, 0.00114387169014663, 0.000882360734976828, 0.00152519892435521, 0.000970575609244406, 0.000878285442013294, 0.00111336237750947, 0.00106118409894407, 0.000974644965026528, 0.00123118585906923, 0.00128538289573044, 0.00101212807931006, 0.000711892149411142, 0.00155650253873318, 0.00104350247420371, 0.00193896074779332, 0.00173620006535202, 0.000895728881005198, 0.0010027636308223, 0.00102749059442431, 0.00108367088250816, 0.00130587606690824, 0.0007899459451437, 0.000930884794797748, 0.000826881790999323, 0.00137765472754836, 0.000900691491551697, 0.00105084234382957, 0.000898180645890534, 0.00108887755777687, 0.000940385099966079, 0.00127726118080318, 0.00119184050709009, 0.00113045366015285, 0.00106797437183559, 0.000970046268776059, 0.000829357246402651, 0.000927225046325475, 0.000891715695615858, 0.000976680661551654, 0.000957277079578489, 0.000935281626880169, 0.00178221880923957, 0.000754198874346912, 0.000754389620851725, 0.00113396509550512, 0.000977414660155773, 0.000962928810622543, 0.0012526836944744, 0.00230797100812197, 0.0006792206550017, 0.00109916494693607, 0.0020893553737551, 0.00248963991180062, 0.00147926923818886, 0.000913797586690634, 0.00142913998570293, 0.00112264859490097, 0.00168083840981126, 0.000855580728966743, 0.000931967922952026, 0.00116557930596173, 0.00101404206361622, 0.00103011808823794, 0.000919984828215092, 0.00164533243514597, 0.00112611998338252, 0.000761963834520429, 0.000933330913539976, 0.00127259769942611, 0.00188913964666426, 0.00196799216791987, 0.000874699268024415, 0.00131650897674263, 0.00208457163535058, 0.000890674302354455, 0.000869484269060194, 0.000837162253446877, 0.00103372347075492, 0.000898765109013766, 0.00114413513801992, 0.00271926587447524, 0.000835566548630595, 0.000809566874522716, 0.00172587553970516, 0.000911701063159853, 0.000923775369301438, 0.00233094999566674, 0.000954347138758749, 0.00117941875942051, 0.00115898868534714, 0.00090460607316345, 0.00159607466775924, 0.00114075490273535, 0.000870061456225812, 0.000948170549236238, 0.00145284051541239, 0.000889918592292815, 0.00118676538113505, 0.001287980703637, 0.00100716156885028, 0.00110427709296346, 0.00118270388338715, 0.00104450050275773, 0.0017655398696661, 0.00130474893376231, 0.0013302672887221, 0.000942635117098689, 0.00109132390934974, 0.00141057348810136, 0.00114489404950291, 0.00219350773841143, 0.00117804284673184, 0.00149001309182495, 0.000860342697706074, 0.00110296648927033, 0.000766036042477936, 0.000787578523159027, 0.00103972153738141, 0.00112000922672451, 0.000866734131705016, 0.00117673445492983, 0.00146277842577547, 0.00105917360633612, 0.00107078673318028, 0.000939561054110527, 0.000868975184857845, 0.00138399039860815, 0.000856601574923843, 0.000734350818675011, 0.000711507571395487, 0.00121037720236927, 0.00199024099856615, 0.000829752651043236, 0.00146324175875634, 0.000861395907122642, 0.00136972544714808, 0.00110911543015391, 0.00113625195808709, 0.000937081582378596, 0.00127870473079383, 0.00132983247749507, 0.00110810960177332, 0.00163039786275476, 0.0011095724767074, 0.000875252531841397, 0.0010518318740651, 0.00229151058010757, 0.00160874961875379, 0.00108993495814502, 0.000982641475275159, 0.00145729805808514, 0.00152227352373302, 0.00122543948236853, 0.00082454941002652, 0.00113242852967232, 0.000917070545256138, 0.00096351932734251, 0.00126445479691029, 0.0011370339198038, 0.00126920209731907, 0.00113055214751512, 0.000961980491410941, 0.000943465915042907, 0.0011556560639292, 0.001651247497648, 0.00119595101568848, 0.0011726674856618, 0.00191958050709218, 0.00123898463789374, 0.00109826715197414, 0.000796459557022899, 0.000850981508847326, 0.00111310079228133, 0.000858772662468255, 0.00130742765031755, 0.000837046885862947, 0.00100280996412039, 0.00103902001865208, 0.000958288088440895, 0.000823716050945222, 0.000945091829635203, 0.00101936759892851, 0.00184859498403966, 0.00114734645467252, 0.000705601356457919, 0.00160277297254652, 0.0014377438928932, 0.000920456368476152, 0.00123244803398848, 0.00124072388280183, 0.000824122573249042, 0.000997092574834824, 0.00117499008774757, 0.00142605102155358, 0.0012367315357551, 0.00111660966649652, 0.000786705291830003, 0.00157121522352099, 0.000708390842191875, 0.00142467208206654, 0.00171193201094866, 0.00153735792264342, 0.00222097663208842, 0.00118755060248077, 0.00195202138274908, 0.000973412476014346, 0.000988823245279491, 0.00146036990918219, 0.00170689192600548, 0.000907890731468797, 0.00133700331207365, 0.00169362383894622, 0.0018091892125085, 0.00121150142513216, 0.000919619982596487, 0.0014310609549284, 0.00104190758429468, 0.000968306849244982, 0.00150770379696041, 0.000803817354608327, 0.0020611968357116, 0.00168271467555314, 0.00095716246869415, 0.000805875228252262, 0.00140943203587085, 0.000896197801921517, 0.000980851240456104, 0.00100629718508571, 0.00103891582693905, 0.000777575536631048, 0.00146862922701985, 0.00132838438730687, 0.00154685555025935, 0.0016851972322911, 0.000832606223411858, 0.000852249271702021, 0.000866974412929267, 0.00245156930759549, 0.00102783285547048, 0.000788719276897609, 0.00178866472560912, 0.00217060768045485, 0.000874878896865994, 0.000799798290245235, 0.00182420411147177, 0.000896812707651407, 0.00238944101147354, 0.00159993057604879, 0.00109385361429304, 0.00101621623616666, 0.00218686647713184, 0.0015423281583935, 0.000941749953199178, 0.000976866576820612, 0.000871077994816005, 0.00123781093861908, 0.000832555291708559, 0.00111215806100518, 0.00105218437965959, 0.00100011157337576, 0.00134243490174413, 0.00166838325094432, 0.00088503968436271, 0.00108340894803405, 0.000871371652465314, 0.000857128470670432, 0.000976609415374696, 0.000887621892616153, 0.00112852803431451, 0.00117235281504691, 0.00100494874641299, 0.00090790749527514, 0.00234373472630978, 0.000783120805863291, 0.000874887860845774, 0.000978786032646894, 0.00139282888267189, 0.00126408017240465, 0.000708115810994059, 0.00106146454345435, 0.000939584453590214, 0.000860359345097095, 0.000848016352392733, 0.0012245838297531, 0.00112847157288343, 0.00127454707399011, 0.00102187367156148, 0.000918156467378139, 0.00184234429616481, 0.000869946903549135, 0.000801064306870103, 0.00119801459368318, 0.0013791723176837, 0.000710052263457328, 0.000828920572530478, 0.000971635105088353, 0.000940788246225566, 0.000977406627498567, 0.000853960344102234, 0.00157989107538015, 0.00126623280812055, 0.00125682319048792, 0.00097789557185024, 0.000819285691250116, 0.00196852558292449, 0.00147821556311101, 0.000865915731992573, 0.00102507928386331, 0.00164747040253133, 0.00120036443695426, 0.00115766131784767, 0.00106568750925362, 0.0013836226426065, 0.00107309001032263, 0.000946725020185113, 0.000924461870454252, 0.000957435986492783, 0.000958195247221738, 0.000836444552987814, 0.00150857912376523, 0.00106161017902195, 0.00107160827610642, 0.00143225060310215, 0.000958255608566105, 0.00184865749906749, 0.000956850650254637, 0.00168333912733942, 0.00113801204133779, 0.000914504402317107, 0.00138463918119669, 0.000852539844345301, 0.000820030749309808, 0.000751296116504818, 0.000944971106946468, 0.000891211442649364, 0.000985811231657863, 0.000806298514362425, 0.00120934576261789, 0.000829938158858567, 0.000792801904026419, 0.00118499214295298, 0.000968485604971647, 0.000874218123499304, 0.000902767525985837, 0.00140148808714002, 0.00266417698003352, 0.00109366758260876, 0.000878499180544168, 0.00106458179652691, 0.00135534489527345, 0.000773041974753141, 0.00176993943750858, 0.00180729245766997, 0.00123738637194037, 0.00105691747739911, 0.00102952297311276, 0.00101261993404478, 0.00101657607592642, 0.000923209125176072, 0.00143086945172399, 0.00170522194821388, 0.000883329659700394, 0.000900857208762318, 0.000945504812989384, 0.000928368885070086, 0.0015842568827793, 0.0022563636302948, 0.000910910603124648, 0.000809232704341412, 0.000797482556663454, 0.0016178572550416, 0.000913473486434668, 0.00277877133339643, 0.000969582528341562, 0.00111895613372326, 0.0009579366305843, 0.000861791311763227, 0.000938063836656511, 0.00119956478010863, 0.000895713805221021, 0.00117322674486786, 0.00152942212298512, 0.00133579224348068, 0.00193340878468007, 0.000962430553045124, 0.00162664602976292, 0.00208440003916621, 0.0011562779545784, 0.00175015768036246, 0.000876984267961234, 0.000970985274761915, 0.00145018682815135, 0.000964799604844302, 0.00102126377169043, 0.0011511001503095, 0.00118497153744102, 0.00158778799232095, 0.00108053570147604, 0.00150121934711933, 0.00144873547833413, 0.00103599601425231, 0.00150181178469211, 0.00108261732384562, 0.00142687477637082, 0.000759749673306942, 0.00147349864710122, 0.00100126664619893, 0.00132651161402464, 0.00088475103257224, 0.00124656839761883, 0.00104928587097675, 0.000912764691747725, 0.00113480410072953, 0.00110376952216029, 0.00154820457100868, 0.000967284431681037, 0.00156873639207333, 0.00128199404571205, 0.0029872024897486, 0.00126065220683813, 0.000763922755140811, 0.000994939007796347, 0.00105710211209953, 0.00084078504005447, 0.00120713782962412, 0.00283741089515388, 0.00102319114375859, 0.00102355750277638, 0.000952538859564811, 0.000788400182500482, 0.00258036027662456, 0.000932767463382334, 0.000856084050610662, 0.00194093747995794, 0.0011242397595197, 0.000904304732102901, 0.00125338672660291, 0.00199460727162659, 0.00111757195554674, 0.000997694325633347, 0.00106027559377253, 0.00114470813423395, 0.00143564562313259, 0.00167514523491263, 0.00125911447685212, 0.00299621326848865, 0.00190041749738157, 0.001577960094437, 0.00141447957139462, 0.00100290670525283, 0.000771071296185255, 0.00123639986850321, 0.00111129914876074, 0.00147357920650393, 0.00109390867874026, 0.000959108874667436, 0.00132295547518879, 0.000970546621829271, 0.00102380919270217, 0.0016320786671713, 0.00102972087915987, 0.00115821114741266, 0.000964672479312867, 0.00159724126569927, 0.00140850141178817, 0.00186386820860207, 0.000823551090434194, 0.000932006689254194, 0.00112228246871382, 0.000963013095315546, 0.00104616477619857, 0.000923314481042325, 0.000940044468734413, 0.00110594881698489, 0.00101315171923488, 0.00179677573032677, 0.00127161887940019, 0.00084590440383181, 0.00123515399172902, 0.00103229994419962, 0.00110816827509552, 0.00206384318880737, 0.000802684866357595, 0.00103243626654148, 0.00180191255640239, 0.000960038683842868, 0.00127411133144051, 0.00104225380346179, 0.000878074497450143, 0.00157739536371082, 0.00107202201616019, 0.00103742373175919, 0.00106371927540749, 0.00108705973252654, 0.00101534021086991, 0.00115272495895624, 0.00150244683027267, 0.00130043376702815, 0.00112482905387878, 0.00137785659171641, 0.000892256444785744, 0.000876683858223259, 0.000957582902628928, 0.00110426044557244, 0.0027278009802103, 0.000948875502217561, 0.00213667121715844, 0.000820720626506954, 0.00112661940511316, 0.00100268656387925, 0.00116002315189689, 0.00113563367631286, 0.00104969798121601, 0.00121062819380313, 0.00106225407216698, 0.0011387454578653, 0.00136103061959147, 0.00102672073990107, 0.00127001374494284, 0.000817367108538747, 0.00119227718096226, 0.00086463027400896, 0.00114526622928679, 0.00103079981636256, 0.00115777528844774, 0.00106558797415346, 0.00097775156609714, 0.00137256388552487, 0.00229633110575378, 0.000839475658722222, 0.00106863083783537, 0.000885171699337661, 0.00157462095376104, 0.00107804883737117, 0.00136402423959225, 0.00104619131889194, 0.000998834031634033, 0.00117731152568012, 0.000988130806945264, 0.00114815298002213, 0.00108831888064742, 0.00166458683088422, 0.00118200620636344, 0.00119393668137491, 0.00184868229553103, 0.000711472588591278, 0.00112651532981545, 0.00142158369999379, 0.00106348749250174, 0.000907656794879586, 0.0014705287758261, 0.00103983620647341, 0.00105704006273299, 0.000926708162296563, 0.00142535287886858, 0.00116746232379228, 0.00143132242374122, 0.000814799219369888, 0.00168949330691248, 0.00106309272814542, 0.000845288101118058, 0.000821316032670438, 0.000815163890365511, 0.000708082807250321, 0.000868207891471684, 0.00113183911889791, 0.00122982426546514, 0.00148788373917341, 0.000927869637962431, 0.00137279462069273, 0.00261235563084483, 0.0012609560508281, 0.0013199943350628, 0.00133189850021154, 0.000872741104103625, 0.0018147201044485, 0.00139092293102294, 0.00115976377855986, 0.00105414749123156, 0.00104940531309694, 0.0008052391349338, 0.00125964602921158, 0.00104186113458127, 0.00227091694250703, 0.00115080294199288, 0.000805439951363951, 0.000987364910542965, 0.00145354971755296, 0.00146333465818316, 0.00087804562645033, 0.000982435536570847, 0.00119190383702517, 0.000999593175947666, 0.000856368686072528, 0.000837795436382294, 0.00110469316132367, 0.000938885379582644, 0.000914441421627998, 0.00207747472450137, 0.0009047876810655, 0.000963372003752738, 0.00214224169030786, 0.00106797879561782, 0.00110743835102767, 0.00109016662463546, 0.0012474557152018 };
  static const int16_t buff_info_Conv2D_139_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_1024[] = { 1, 1, 1024, 1 };
  static const uint32_t buff_info__mem_shape_U_1024[] = { 1024 };
  static const uint32_t buff_info__shape_1024_1024_1_1[] = { 1024, 1, 1, 1024 };
  static const uint32_t buff_info__mem_shape_M_1024_1024_1_1[] = { 1024, 64, 1, 1, 16 };
  static const float buff_info_Conv2D_150_weights_quant_scale[] = { 0.000702995457686484, 0.000939627469051629, 0.000554133206605911, 0.00111971783917397, 0.000904907239601016, 0.000646272499579936, 0.00147409318014979, 0.000593504111748189, 0.0006483169272542, 0.000993680907413363, 0.000759394490160048, 0.000565416761673987, 0.000558841682504863, 0.000713448913302273, 0.00138235883787274, 0.0010763251921162, 0.00115260761231184, 0.000719822710379958, 0.000734395754989237, 0.000700408359989524, 0.000686581886839122, 0.000628574634902179, 0.000693950860295445, 0.00068600318627432, 0.000658270553685725, 0.000814402301330119, 0.000796075095422566, 0.0011573398951441, 0.00135983829386532, 0.000682044017594308, 0.000608772330451757, 0.000941974634770304, 0.000712313049007207, 0.000970949186012149, 0.00120596843771636, 0.00135877483990043, 0.000851660210173577, 0.000604189641308039, 0.000940315250772983, 0.000893824093509465, 0.00063657114515081, 0.000641236139927059, 0.0014435201883316, 0.00142645859159529, 0.00150487176142633, 0.000592396885622293, 0.000728115264791995, 0.0007482108194381, 0.000613060721661896, 0.000744593504350632, 0.0009338179952465, 0.000924964202567935, 0.000657947093714029, 0.000822100089862943, 0.00082273205043748, 0.000511838763486594, 0.00128722109366208, 0.000759394373744726, 0.000769093923736364, 0.000613002222962677, 0.000558471481781453, 0.00086303980788216, 0.000935475458391011, 0.000618084101006389, 0.000599702238105237, 0.000727420556358993, 0.000588283292017877, 0.000700988108292222, 0.000865341920871288, 0.000781598151661456, 0.000698683725204319, 0.00103006151039153, 0.000589285977184772, 0.000743979937396944, 0.00163355213589966, 0.00105346669442952, 0.000930398877244443, 0.000692274305038154, 0.000809231540188193, 0.000727348437067121, 0.000941869453527033, 0.00107048789504915, 0.000689408683683723, 0.000791076105087996, 0.00132277980446815, 0.000687218795064837, 0.000869018258526921, 0.000967537285760045, 0.000802767695859075, 0.00114746554754674, 0.000681276374962181, 0.000681847624946386, 0.000715519418008626, 0.000966610328759998, 0.000591108808293939, 0.000662695325445384, 0.000770745100453496, 0.000637132849078625, 0.000623414991423488, 0.000707700673956424, 0.000638515222817659, 0.000649211811833084, 0.000635648146271706, 0.000698046293109655, 0.000653446419164538, 0.000632403010968119, 0.00061740237288177, 0.000662874081172049, 0.00106512568891048, 0.00109738379251212, 0.000934396171942353, 0.000598388782236725, 0.00107221677899361, 0.000567013688851148, 0.000549941440112889, 0.000842616311274469, 0.0011243688641116, 0.000823483627755195, 0.0018107743235305, 0.000750669569242746, 0.000532651029061526, 0.0005769626586698, 0.00075721280882135, 0.000635064905509353, 0.000792864826507866, 0.00104888807982206, 0.000595192075707018, 0.00124777783639729, 0.000593863951507956, 0.00123631709720939, 0.000614834192674607, 0.00060947157908231, 0.000658790580928326, 0.00099066540133208, 0.000546002644114196, 0.00113013270311058, 0.00143795984331518, 0.00162668363191187, 0.000706730235833675, 0.000566854199860245, 0.000718963681720197, 0.00102099508512765, 0.0011502921115607, 0.000625909364316612, 0.000783705560024828, 0.000643216422758996, 0.000717597140464932, 0.000550264492630959, 0.000887358677573502, 0.00151996780186892, 0.000605131092015654, 0.000940885511226952, 0.00119436183013022, 0.00053936115000397, 0.000692486471962184, 0.000756321242079139, 0.000676531693898141, 0.000565719557926059, 0.00104315450880677, 0.000774383486714214, 0.000682804442476481, 0.000585304456762969, 0.000888092152308673, 0.000621565268374979, 0.0019880086183548, 0.00071205769199878, 0.000663643353618681, 0.000696541566867381, 0.00202889856882393, 0.000605667009949684, 0.000565837428439409, 0.00143979594577104, 0.00119990482926369, 0.000652563583571464, 0.000586791080422699, 0.00104721623938531, 0.00122169277165085, 0.00065591000020504, 0.00120037654414773, 0.000567113398574293, 0.000726963742636144, 0.000615155440755188, 0.000813291291706264, 0.000847169372718781, 0.000837608065921813, 0.000749088590964675, 0.000893701741006225, 0.00270674051716924, 0.000596699479501694, 0.000747935438994318, 0.00161837483756244, 0.000890269759111106, 0.000640929327346385, 0.000900939397979528, 0.00114515551831573, 0.000630885828286409, 0.00085749349091202, 0.000534717517439276, 0.000650757225230336, 0.000583165092393756, 0.000763033051043749, 0.000637022196315229, 0.0005644605262205, 0.000974500551819801, 0.000644433544948697, 0.000809066579677165, 0.000824667746201158, 0.000672088470309973, 0.000621780753135681, 0.000726142490748316, 0.000754059466999024, 0.000648204004392028, 0.00100635085254908, 0.000623659521806985, 0.000673611939419061, 0.000607032736297697, 0.000718124327249825, 0.00158782943617553, 0.000678029318805784, 0.000625072745606303, 0.000662362901493907, 0.00102909887209535, 0.00111798394937068, 0.000632440962363034, 0.00152561522554606, 0.000749823928344995, 0.00107813091017306, 0.000764945521950722, 0.000618253252469003, 0.000716693408321589, 0.000784627860412002, 0.000595643185079098, 0.000613469164818525, 0.000606452114880085, 0.000891519885044545, 0.000718343479093164, 0.000620528589934111, 0.000636798911727965, 0.000819896755274385, 0.000654158473480493, 0.000743440352380276, 0.00141132867429405, 0.00093039422063157, 0.000669502303935587, 0.000570533680729568, 0.00138236454222351, 0.000657265074551105, 0.000780529226176441, 0.000500969763379544, 0.0010323787573725, 0.000628965382929891, 0.000565351918339729, 0.00208810088224709, 0.00103014602791518, 0.000587491027545184, 0.000965660670772195, 0.000648141780402511, 0.000970287423115224, 0.000700906093697995, 0.000896927202120423, 0.00060940912226215, 0.000668844266328961, 0.000785117503255606, 0.000607236113864928, 0.00154318823479116, 0.000751933897845447, 0.000821102526970208, 0.000963193946518004, 0.000583793385885656, 0.000703085272107273, 0.000693399866577238, 0.000650401751045138, 0.00132048805244267, 0.000881335639860481, 0.000577939965296537, 0.000941727252211422, 0.000710435211658478, 0.000604476721491665, 0.00101214076858014, 0.000607750029303133, 0.000774222600739449, 0.001022026874125, 0.000673333939630538, 0.00197328091599047, 0.000605901645030826, 0.0009205830283463, 0.000728359736967832, 0.000845658709295094, 0.00110579538159072, 0.00139046704862267, 0.000576789316255599, 0.00056199764367193, 0.000624023727141321, 0.000700587057508528, 0.000598661543335766, 0.000852142053190619, 0.000928303692489862, 0.000602954358328134, 0.000942185113672167, 0.000687445397488773, 0.00105766346678138, 0.00116533937398344, 0.0007206269656308, 0.00117038586176932, 0.000649968860670924, 0.000994580914266407, 0.00073207903187722, 0.000869690498802811, 0.00101727747824043, 0.00160770770162344, 0.0017760219052434, 0.000552025332581252, 0.000647187058348209, 0.000657023687381297, 0.00128034618683159, 0.000758746231440455, 0.000625434040557593, 0.000728151760995388, 0.00130947306752205, 0.00178662315011024, 0.000715515518095344, 0.000997963594272733, 0.00163744681049138, 0.000581905478611588, 0.000665443949401379, 0.000698526855558157, 0.000714240304660052, 0.00132917042355984, 0.000901805935427547, 0.000759679533075541, 0.00125927373301238, 0.000724410638213158, 0.00246538990177214, 0.000620061939116567, 0.000654467032290995, 0.000644535815808922, 0.000892650976311415, 0.000984866754151881, 0.000624474720098078, 0.00139237800613046, 0.000622304272837937, 0.000737091701012105, 0.000600077153649181, 0.00075075076892972, 0.00157944986131042, 0.000874732621014118, 0.000678296899423003, 0.00068892864510417, 0.00130737607832998, 0.0015500852605328, 0.00113431399222463, 0.000736754271201789, 0.000728409388102591, 0.000752977502997965, 0.000735611596610397, 0.0018186061643064, 0.00126272544730455, 0.000717633462045342, 0.000708783220034093, 0.00167564617004246, 0.000597016827668995, 0.00135519879404455, 0.000692581059411168, 0.00063391012372449, 0.00121046148706228, 0.000706086459103972, 0.000606005487497896, 0.00161131948698312, 0.000854876183439046, 0.00114741490688175, 0.000694428628776222, 0.000587640912272036, 0.000638148514553905, 0.00249794381670654, 0.000607391993980855, 0.000965240469668061, 0.000977040268480778, 0.00065809057559818, 0.000909715425223112, 0.000816534564364702, 0.00220724660903215, 0.000643622886855155, 0.000947624037507921, 0.00147908728104085, 0.00109134276863188, 0.000773254316300154, 0.000645941356197, 0.000622469640802592, 0.000798598281107843, 0.000810189172625542, 0.000935548625420779, 0.00116463785525411, 0.000961428566370159, 0.000586690905038267, 0.000659227604046464, 0.00077996589243412, 0.00133593427017331, 0.000912179180886596, 0.000730763247702271, 0.00166416168212891, 0.00101575523149222, 0.00271174171939492, 0.00073590426472947, 0.00176140246912837, 0.000881245417986065, 0.000565055292099714, 0.000694136891979724, 0.000612372998148203, 0.0010359319858253, 0.000704708800185472, 0.00134085747413337, 0.000822569592855871, 0.000789646815974265, 0.000640929909422994, 0.000730652071069926, 0.000618434511125088, 0.000701911805663258, 0.00168804544955492, 0.000691607827320695, 0.000613911368418485, 0.000604150758590549, 0.00112700217869133, 0.00111891818232834, 0.000748344988096505, 0.000971875269897282, 0.000852019526064396, 0.000965430401265621, 0.000803574512246996, 0.000764306867495179, 0.000898094440344721, 0.00172019854653627, 0.000766514800488949, 0.000717037764843553, 0.000615692639257759, 0.000643673760350794, 0.000876282574608922, 0.000638925470411777, 0.00117586727719754, 0.000547516508959234, 0.000588500173762441, 0.00157797546125948, 0.000737983791623265, 0.00155958358664066, 0.0007176767103374, 0.000758472655434161, 0.000595417513977736, 0.00129339518025517, 0.000719569914508611, 0.00105533737223595, 0.000526996096596122, 0.000556962215341628, 0.00181140948552638, 0.000606188201345503, 0.000739477283786982, 0.00127959123346955, 0.000728269282262772, 0.0017906982684508, 0.000854198529850692, 0.000818005413748324, 0.00120679545216262, 0.000907679961528629, 0.000710722350049764, 0.000687758321873844, 0.000827831623610109, 0.000850765500217676, 0.00140883075073361, 0.000635353848338127, 0.000617663667071611, 0.00071705796290189, 0.000767865742091089, 0.000926994020119309, 0.000799501838628203, 0.000612949952483177, 0.000657918164506555, 0.000754510459955782, 0.000543714733794332, 0.00158129911869764, 0.00145895499736071, 0.000563364010304213, 0.000638311146758497, 0.00113099464215338, 0.000859140360262245, 0.000940583646297455, 0.000718561641406268, 0.000696165428962559, 0.00075448805000633, 0.000694884860422462, 0.000644363230094314, 0.000734592962544411, 0.00206661247648299, 0.00187421787995845, 0.000632185838185251, 0.000542130146641284, 0.000611353025306016, 0.000524353294167668, 0.000670741370413452, 0.000619991100393236, 0.000763233576435596, 0.000565048947464675, 0.000754332111682743, 0.000553572084754705, 0.00110571226105094, 0.00129614991601557, 0.000769678445067257, 0.000961098994594067, 0.000779207912273705, 0.000645187858026475, 0.000653383089229465, 0.000717395509127527, 0.00100549671333283, 0.000783015333581716, 0.00273224990814924, 0.000683415040839463, 0.00128464610315859, 0.00101989263202995, 0.00103341252543032, 0.000720179232303053, 0.000709822459612042, 0.000626196502707899, 0.000674392911605537, 0.0011418986832723, 0.000662902952171862, 0.000833887257613242, 0.000579463085159659, 0.000719786155968904, 0.000595076009631157, 0.000700229255016893, 0.000806810799986124, 0.00174635101575404, 0.000708106032107025, 0.00246947095729411, 0.000693536247126758, 0.000690120970830321, 0.00181095779407769, 0.000681144476402551, 0.000701094977557659, 0.000636238895822316, 0.000702103308867663, 0.000593792356085032, 0.00071532919537276, 0.00059022253844887, 0.000871878291945904, 0.000605587207246572, 0.000723057426512241, 0.000603875669185072, 0.000890011840965599, 0.000737355032470077, 0.000812349782790989, 0.000581038242671639, 0.000730133673641831, 0.00110245624091476, 0.000747178099118173, 0.00070691539440304, 0.000831660523544997, 0.00057174835819751, 0.000857855600770563, 0.000762248761020601, 0.000594241195358336, 0.000681670557241887, 0.000866343034431338, 0.00103709078393877, 0.000586193345952779, 0.000630742113571614, 0.000806062133051455, 0.000778159941546619, 0.000583354965783656, 0.000669067550916225, 0.000603515421971679, 0.000684592581819743, 0.000579347717575729, 0.0010411316761747, 0.000968168373219669, 0.000757574394810945, 0.000649462279397994, 0.00123655761126429, 0.00142090395092964, 0.000768347992561758, 0.00162836629897356, 0.00205730833113194, 0.000655966927297413, 0.000900603306945413, 0.00120468414388597, 0.000622293329797685, 0.000585831934586167, 0.00153131759725511, 0.000678399461321533, 0.00102551118470728, 0.000718517869245261, 0.00174137961585075, 0.000574981910176575, 0.0024050516076386, 0.00149460649117827, 0.000583002693019807, 0.000568845483940095, 0.00134762306697667, 0.00137789640575647, 0.000777258304879069, 0.000740692368708551, 0.000637907767668366, 0.000761632341891527, 0.000676374766044319, 0.000872206001076847, 0.0005757991457358, 0.00265640625730157, 0.00070210691774264, 0.00148802134208381, 0.000604753033258021, 0.00080539844930172, 0.00101629283744842, 0.000853170466143638, 0.00119692564476281, 0.000752068182919174, 0.00075210101203993, 0.000866837217472494, 0.00068910769186914, 0.00112614419776946, 0.000723425473552197, 0.000664238701574504, 0.00123569834977388, 0.000681760720908642, 0.000623377272859216, 0.000595503428485245, 0.000699320633430034, 0.000790747872088104, 0.000813074002508074, 0.000727786216884851, 0.000553033780306578, 0.000763474381528795, 0.00141973537392914, 0.000545373419299722, 0.00141735165379941, 0.0011057109804824, 0.000791941653005779, 0.00196000258438289, 0.00165394006762654, 0.000881637562997639, 0.000853128614835441, 0.00111398380249739, 0.000557017629034817, 0.00163152720779181, 0.00086724798893556, 0.000593166914768517, 0.00141797843389213, 0.000648381013888866, 0.00126314349472523, 0.000539938453584909, 0.000581863219849765, 0.0008177948766388, 0.000599942402914166, 0.000913175230380148, 0.00059228262398392, 0.000576977559830993, 0.00074281741399318, 0.000989617197774351, 0.000593678734730929, 0.000581821426749229, 0.000843439775053412, 0.000647519016638398, 0.000718790630344301, 0.000996797229163349, 0.000986489932984114, 0.00063033401966095, 0.000724900688510388, 0.000644383544567972, 0.000863462162669748, 0.000854752317536622, 0.000842542271129787, 0.000737404625397176, 0.000679093122016639, 0.000694457557983696, 0.00113759189844131, 0.000686344457790256, 0.00135174009483308, 0.000680183933582157, 0.00100121041759849, 0.00059322954621166, 0.000627730914857239, 0.00147737108636647, 0.000617934041656554, 0.000651451875455678, 0.00250084302388132, 0.000622368243057281, 0.000633891497272998, 0.00170815212186426, 0.000617553887423128, 0.000856335915159434, 0.000563184090424329, 0.000683822087012231, 0.000632493931334466, 0.000546370400115848, 0.000706474820617586, 0.0005605403566733, 0.000665888248477131, 0.000732766871806234, 0.000604935979936272, 0.00137808860745281, 0.000600647530518472, 0.000656637537758797, 0.000540109060239047, 0.000653791299555451, 0.000643419683910906, 0.000597874575760216, 0.000672528287395835, 0.000687715306412429, 0.000652825518045574, 0.000548013777006418, 0.000878162740264088, 0.000682385521940887, 0.00236675329506397, 0.000790995196439326, 0.000638966623228043, 0.00112385372631252, 0.00082389876479283, 0.00266786594875157, 0.000764223688747734, 0.000684404978528619, 0.000546108931303024, 0.000862898188643157, 0.00089010433293879, 0.000785233278293163, 0.000732779211830348, 0.000685950159095228, 0.000898314116057009, 0.00068766187177971, 0.000772453553508967, 0.000570184900425375, 0.00059716374380514, 0.000670953537337482, 0.00105786754284054, 0.000829365802928805, 0.00175688113085926, 0.00102997676003724, 0.000583626329898834, 0.000770693062804639, 0.000695493130479008, 0.000659066077787429, 0.000617779733147472, 0.000562008179258555, 0.000720992917194963, 0.00143810210283846, 0.000638279831036925, 0.000995549373328686, 0.000555338861886412, 0.00110203016083688, 0.00080293056089431, 0.000562752073165029, 0.000670736946631223, 0.000708191422745585, 0.000651610374916345, 0.000598571263253689, 0.000858901767060161, 0.00354729266837239, 0.000673398491926491, 0.000698779302183539, 0.000571753073018044, 0.00125728151760995, 0.000782026094384491, 0.000600577739533037, 0.000695335969794542, 0.000711272354237735, 0.000659832789096981, 0.00074055977165699, 0.000675938266795129, 0.000549666408915073, 0.00114627659786493, 0.000582936278078705, 0.00126693223137408, 0.00085204717470333, 0.000696470204275101, 0.00102371897082776, 0.00133348186500371, 0.00114729860797524, 0.000676416093483567, 0.000645354564767331, 0.000527499301824719, 0.00122953951358795, 0.000626530207227916, 0.000667457992676646, 0.000831522687803954, 0.000677162897773087, 0.00143011601176113, 0.00150561833288521, 0.0005922801210545, 0.00135228154249489, 0.000766227138228714, 0.000550732249394059, 0.000695954309776425, 0.000995620270259678, 0.00131674064323306, 0.000640390673652291, 0.00167479855008423, 0.000549888936802745, 0.00140331930015236, 0.000664330320432782, 0.000673747214023024, 0.000664598948787898, 0.000660160440020263, 0.000961603422183543, 0.000763507618103176, 0.00161435466725379, 0.000943376682698727, 0.00080760900164023, 0.000790635589510202, 0.000865777023136616, 0.000543040630873293, 0.00056020135525614, 0.000554548285435885, 0.000650108850095421, 0.000618485559243709, 0.000757751695346087, 0.000831760000437498, 0.000921159633435309, 0.00062395230634138, 0.0010640254477039, 0.000644102168735117, 0.000824865885078907, 0.000804096809588373, 0.000930938520468771, 0.000727841863408685, 0.000609279086347669, 0.000624307838734239, 0.000679036253131926, 0.00156895420514047, 0.000665835628751665, 0.00136220722924918, 0.000708668550942093, 0.000630763999652117, 0.000746739038731903, 0.000697974523063749, 0.000682127603795379, 0.000631306727882475, 0.00155586272012442, 0.000655034615192562, 0.000932967290282249, 0.000654501898679882, 0.00222979579120874, 0.00065228040330112, 0.000596682133618742, 0.0011704625794664, 0.00168834417127073, 0.000553234713152051, 0.000760071387048811, 0.000985305057838559, 0.000647257955279201, 0.00132498936727643, 0.000544701120816171, 0.00105283327866346, 0.000669420231133699, 0.000773175153881311, 0.00084164907457307, 0.000663373037241399, 0.000646081636659801, 0.000685979728586972, 0.000942443730309606, 0.000814944563899189, 0.000876883859746158, 0.000715359346941113, 0.000712500826921314, 0.00120431208051741, 0.000657601107377559, 0.000651288370136172, 0.000889104790985584, 0.00100665481295437, 0.000599185528699309, 0.00105380034074187, 0.000676827447023243, 0.0014085698639974, 0.00062165065901354, 0.000642428640276194, 0.000606773537583649, 0.000647493463475257, 0.000729502819012851, 0.000692327914293855, 0.000588443188462406, 0.000750214036088437, 0.000624448352027684, 0.00227342848666012, 0.00110811658669263, 0.000671078683808446, 0.00109404698014259, 0.00240697083063424, 0.000552600657101721, 0.000896471436135471, 0.00104557070881128, 0.000734527595341206, 0.000820885936263949, 0.000514297920744866, 0.000645962078124285, 0.000514970917720348, 0.000655644631478935, 0.00106966285966337, 0.000607508118264377, 0.000675194023642689, 0.000680305238347501, 0.000730324711184949, 0.000678815762512386, 0.0012996835866943, 0.000791048747487366, 0.000636241282336414, 0.000654079369269311, 0.000693580775987357, 0.000614837452303618, 0.000703677709680051, 0.00221375888213515, 0.000988134648650885, 0.000725500634871423, 0.00136781495530158, 0.000790816033259034, 0.000693816167768091, 0.000732642249204218, 0.000589126837439835, 0.000802986731287092, 0.00063373806187883, 0.000810933590400964, 0.000655870186164975, 0.000550879980437458, 0.0013639786047861, 0.00131311547011137, 0.000655398878734559, 0.00135885225608945, 0.000843047047965229, 0.000598677259404212, 0.000590088486205786, 0.000787658733315766, 0.00124300108291209, 0.000766881566960365, 0.000638512894511223, 0.000549429270904511, 0.00112897565122694, 0.000628289533779025, 0.00134784437250346, 0.000768388155847788, 0.000848199590109289, 0.00054499413818121, 0.00152567541226745, 0.00059227179735899, 0.000837389205116779, 0.000761624483857304, 0.000656605698168278, 0.000743256940040737, 0.000609733280725777, 0.00076390040339902, 0.000762971001677215, 0.000554022786673158, 0.000526266056112945, 0.0015561010222882, 0.00136026542168111, 0.000594029028434306, 0.00140380999073386, 0.000713163230102509, 0.000772991217672825, 0.000897835125215352, 0.00264219311065972, 0.000833534577395767, 0.000598777492996305, 0.00104339024983346, 0.000763282354455441, 0.00071088393451646, 0.00110834615770727, 0.000546267838217318, 0.000647007371298969, 0.000673550297506154, 0.00125887896865606, 0.00154681864660233, 0.000648232875391841, 0.000706858700141311, 0.00163158145733178, 0.00117492338176817, 0.000886827823705971, 0.000849761301651597, 0.000673115777317435, 0.000618954189121723, 0.00262947590090334, 0.00073602085467428, 0.000667820393573493, 0.00176251400262117, 0.00115815491881222, 0.000777703244239092, 0.000569084950257093, 0.00123131577856839, 0.000763096089940518, 0.000678864133078605, 0.000908963847905397, 0.000751709856558591, 0.000725438294466585, 0.000694260117597878, 0.00164421054068953, 0.00212249369360507, 0.00107363134156913, 0.000762988172937185, 0.000611136842053384, 0.000663533166516572, 0.00115547317545861, 0.000833404250442982, 0.000666366424411535, 0.000685012666508555, 0.000744968885555863, 0.000703083816915751, 0.000757648842409253, 0.00057181908050552, 0.000899510050658137, 0.00132200215011835, 0.00129156839102507, 0.00112732383422554, 0.00106549658812582, 0.00159685290418565, 0.00131197494920343, 0.000581387605052441, 0.000580260355491191, 0.000676275347359478, 0.000887860311195254, 0.00061857100808993, 0.000923257844988257, 0.00239710346795619, 0.000759904680307955, 0.000606548972427845, 0.000649568217340857, 0.000741020834539086, 0.000622160790953785, 0.000677292875479907, 0.000539380358532071, 0.000771013903431594, 0.00140260509215295, 0.00212416285648942 };
  static const int16_t buff_info_Conv2D_150_weights_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Gemm_159_bias_quant_scale[] = { 2.07217385650438e-06, 3.60389321940602e-06, 1.97451504391211e-06, 2.294669911862e-06, 2.42699707087013e-06, 2.79831192528945e-06, 2.06572985916864e-06, 2.6485945454624e-06, 2.05858077606536e-06, 1.9654680727399e-06, 2.67442237600335e-06, 2.18762511394743e-06, 2.13916428037919e-06, 2.43450131165446e-06, 1.98944235307863e-06, 2.34408275900932e-06, 2.67777431872673e-06, 2.06988806894515e-06, 3.38333575200522e-06, 2.25880535253964e-06, 2.52959921454021e-06, 2.56455882663431e-06, 2.16931653085339e-06, 2.13950329452928e-06, 2.33858600040548e-06, 2.51702590503555e-06, 2.37029098570929e-06, 2.56351131611154e-06, 2.45712499236106e-06, 2.81149846159678e-06, 2.72451211458247e-06, 2.21141090150923e-06, 2.2120907487988e-06, 2.51994447353354e-06, 2.10952316592738e-06, 2.15182399188052e-06, 2.48014566750498e-06, 3.02578769151296e-06, 2.54559108725516e-06, 2.02082946998416e-06, 1.95119332602189e-06, 2.3836739728722e-06, 2.79347204923397e-06, 2.56718794844346e-06, 2.85719602288736e-06, 1.97571353055537e-06, 2.19525327338488e-06, 2.18317245526123e-06, 2.83367353404174e-06, 2.66156916950422e-06, 2.32368097385915e-06, 2.04991215468908e-06, 2.76346895589086e-06, 2.25281064558658e-06, 2.35114475799492e-06, 2.25885469262721e-06, 2.21615323425794e-06, 3.27418047163519e-06, 2.37481458498223e-06, 2.08337678486714e-06, 2.15032309824892e-06, 2.40759459302353e-06, 2.43996873905417e-06, 2.17254500967101e-06, 2.52679092227481e-06, 2.18002219298796e-06, 1.79995527105348e-06, 3.09606070914015e-06, 2.2341996555042e-06, 2.35496759160014e-06, 2.56869520853797e-06, 2.32320940085629e-06, 1.7821733990786e-06, 2.58959653365309e-06, 2.24965060624527e-06, 2.74370540864766e-06, 3.11906592287414e-06, 2.19587923311337e-06, 2.33204855248914e-06, 2.3898164727143e-06, 2.11441670217027e-06, 2.20758329305681e-06, 2.22818835027283e-06, 2.3387747205561e-06, 2.34681942856696e-06, 2.14038095691649e-06, 1.98408110918535e-06, 1.9715096186701e-06, 2.17938463720202e-06, 2.52355835073104e-06, 2.56550538324518e-06, 2.25606140702439e-06, 2.70745636044012e-06, 3.29516319652612e-06, 2.4214270979428e-06, 2.0711079287139e-06, 2.56060934589186e-06, 2.22547078010393e-06, 3.54296412297117e-06, 2.33451805797813e-06, 2.33988407671859e-06, 2.83848476101412e-06, 1.96268069885264e-06, 2.31799003813649e-06, 2.09204290513298e-06, 2.15589193430787e-06, 2.28607314056717e-06, 2.07013795261446e-06, 2.21122377297434e-06, 2.91155083687045e-06, 2.4337916784134e-06, 2.65384551312309e-06, 3.00910369332996e-06, 1.83843076229095e-06, 2.26261363422964e-06, 2.16194030144834e-06, 1.83986162483052e-06, 2.11625683732564e-06, 2.63710580838961e-06, 2.35639458878723e-06, 2.31185549637303e-06, 1.9937508568546e-06, 2.47476009462844e-06, 2.80531298813003e-06, 2.39288146985928e-06, 2.34423123401939e-06, 2.11300266528269e-06, 2.14885790228436e-06 };
  static const int16_t buff_info_Gemm_159_bias_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_128_50176_1_1[] = { 128, 1, 1, 50176 };
  static const uint32_t buff_info__mem_shape_F_128_50176_1_1[] = { 128, 50176, 1, 1 };
  static const float buff_info_Gemm_159_weights_transposed_3_quant_scale[] = { 0.000612252566497773, 0.00106482033152133, 0.00058339792303741, 0.00067799212411046, 0.000717090035323054, 0.000826800183858722, 0.000610348593909293, 0.000782564107794315, 0.000608236296102405, 0.000580724852625281, 0.000790195248555392, 0.000646364234853536, 0.000632045848760754, 0.000719307281542569, 0.000587808433920145, 0.000692591827828437, 0.000791185651905835, 0.000611577183008194, 0.000999653595499694, 0.000667395419441164, 0.000747405283618718, 0.000757734582293779, 0.000640954705886543, 0.000632146024145186, 0.00069096777588129, 0.000743690296076238, 0.000700335425790399, 0.000757425033953041, 0.00072599173290655, 0.000830696313641965, 0.000804994953796268, 0.000653392111416906, 0.000653592986054718, 0.000744552642572671, 0.000623287924099714, 0.000635786331258714, 0.000732793530914932, 0.00089401105651632, 0.000752130290493369, 0.0005970821948722, 0.000576507183723152, 0.000704289588611573, 0.0008253701380454, 0.000758511363528669, 0.000844198279082775, 0.000583752000238746, 0.000648618093691766, 0.000645048683509231, 0.000837248226162046, 0.000786397606134415, 0.000686563842464238, 0.00060567504260689, 0.000816505344118923, 0.000665624218527228, 0.000694678397849202, 0.000667410029564053, 0.000654793286230415, 0.000967402127571404, 0.000701671990100294, 0.000615562661550939, 0.000635342847090214, 0.000711357279215008, 0.000720922718755901, 0.000641908613033593, 0.000746575475204736, 0.000644117884803563, 0.000531821802724153, 0.000914774194825441, 0.000660125340800732, 0.000695807917509228, 0.000758956710342318, 0.000686424493324012, 0.000526567921042442, 0.000765132310334593, 0.000664690567646176, 0.000810665893368423, 0.000921571394428611, 0.000648803077638149, 0.000689036154653877, 0.000706104503478855, 0.000624733802396804, 0.000652261194773018, 0.00065834925044328, 0.000691023480612785, 0.000693400448653847, 0.000632405281066895, 0.000586224370636046, 0.000582509906962514, 0.000643929466605186, 0.000745620403904468, 0.000758014211896807, 0.000666584703139961, 0.000799955625552684, 0.000973601767327636, 0.000715444330126047, 0.00061193760484457, 0.000756567635107785, 0.000657546275760978, 0.00104681798256934, 0.000689765845891088, 0.000691351306159049, 0.000838669773656875, 0.000579901272431016, 0.000684882397763431, 0.000618123158346862, 0.000636988261248916, 0.000675452116411179, 0.000611651048529893, 0.000653336814139038, 0.000860258180182427, 0.000719097559340298, 0.000784115574788302, 0.000889081507921219, 0.000543189933523536, 0.000668520631734282, 0.000638775352854282, 0.00054361269576475, 0.000625277520157397, 0.000779169553425163, 0.000696229515597224, 0.000683069869410247, 0.000589081377256662, 0.00073120224988088, 0.000828868709504604, 0.000707010098267347, 0.000692635716404766, 0.000624315987806767, 0.000634909956716001 };
  static const int16_t buff_info_Gemm_159_weights_transposed_3_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_1024_1_7[] = { 1, 1, 7, 1024 };
  static const uint32_t buff_info__mem_shape_M_1024_1_7[] = { 64, 1, 7, 16 };
  static const uint32_t buff_info__shape_32_8_3_3[] = { 32, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_32_8_3_3[] = { 32, 3, 3, 8 };
  static const float buff_info_Conv2D_13_weights_inflated_250_quant_scale[] = { 0.0084187826141715, 0.00171337556093931, 0.00223928946070373, 0.00649577239528298, 0.00314375618472695, 0.00315218674950302, 0.00149795867037028, 0.00495331548154354, 0.00453639822080731, 0.00236677238717675, 0.00327946851029992, 0.00277593918144703, 0.00481718266382813, 0.00562189845368266, 0.00501558510586619, 0.00633022375404835, 0.00844208057969809, 0.00569234369322658, 0.00478866929188371, 0.00542011065408587, 0.00242192181758583, 0.00189550290815532, 0.00149195757694542, 0.00296138669364154, 0.00356150255538523, 0.00746333552524447, 0.00592440087348223, 0.00722011690959334, 0.00538161071017385, 0.00281590595841408, 0.00351833063177764, 0.00648644473403692 };
  static const int16_t buff_info_Conv2D_13_weights_inflated_250_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_64_8_3_3[] = { 64, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_64_8_3_3[] = { 64, 3, 3, 8 };
  static const float buff_info_Conv2D_24_weights_inflated_252_quant_scale[] = { 0.00193802628200501, 0.0020397836342454, 0.0022460448089987, 0.00178677320946008, 0.00252328044734895, 0.00222877389751375, 0.00181316141970456, 0.00139783532358706, 0.00173119525425136, 0.00238262419588864, 0.00123027479276061, 0.00220703822560608, 0.00172812736127526, 0.00270514818839729, 0.00228199292905629, 0.00157497567124665, 0.00232366984710097, 0.00199844944290817, 0.00158180471044034, 0.00181127816904336, 0.00321940379217267, 0.00293879373930395, 0.00192704307846725, 0.00255168764851987, 0.00165785220451653, 0.00284887664020061, 0.00208369549363852, 0.00125975836999714, 0.00245867995545268, 0.00221730396151543, 0.00167574977967888, 0.0034472884144634, 0.00152912130579352, 0.00242975633591413, 0.00232861842960119, 0.00118073052726686, 0.00253354921005666, 0.00193706760182977, 0.00303029920905828, 0.00166751677170396, 0.00141219317447394, 0.00220578256994486, 0.00130738993175328, 0.00207239366136491, 0.00204852293245494, 0.00168836978264153, 0.00240666535682976, 0.00308061391115189, 0.00222248025238514, 0.00182940461672843, 0.00312868482433259, 0.00132227048743516, 0.00194772030226886, 0.00213669054210186, 0.0034042177721858, 0.00209956639446318, 0.0030890298075974, 0.00189340941142291, 0.0027461068239063, 0.00157454842701554, 0.001874283188954, 0.00288659217767417, 0.00301743601448834, 0.00198711734265089 };
  static const int16_t buff_info_Conv2D_24_weights_inflated_252_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_128_8_3_3[] = { 128, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_128_8_3_3[] = { 128, 3, 3, 8 };
  static const float buff_info_Conv2D_35_weights_inflated_254_quant_scale[] = { 0.00295581691898406, 0.00198080972768366, 0.00231899530626833, 0.00674410257488489, 0.00206191372126341, 0.00409835297614336, 0.00703004375100136, 0.00201296363957226, 0.00256816693581641, 0.00212492840364575, 0.00474104238674045, 0.00342050474137068, 0.00645166309550405, 0.00276703876443207, 0.00353090185672045, 0.0018426903989166, 0.00292628724128008, 0.00294462824240327, 0.00200147461146116, 0.00272972811944783, 0.00266715255565941, 0.00474099023267627, 0.003358373651281, 0.00363350776024163, 0.00318209081888199, 0.0050549590960145, 0.0072167026810348, 0.00185209943447262, 0.00620792992413044, 0.00395604269579053, 0.0026476162020117, 0.00268840999342501, 0.00285096908919513, 0.00288118445314467, 0.0025397592689842, 0.00352337071672082, 0.0017202531453222, 0.00442789355292916, 0.00695755798369646, 0.00288411881774664, 0.00433526979759336, 0.00362792564556003, 0.00247754179872572, 0.0043574646115303, 0.00220205332152545, 0.00295896478928626, 0.00283222738653421, 0.00845924392342567, 0.00264502200298011, 0.00234107463620603, 0.00207288260571659, 0.00183178775478154, 0.00611734483391047, 0.00349513371475041, 0.00245078932493925, 0.00698544876649976, 0.00516296736896038, 0.00241724518127739, 0.00222402228973806, 0.00320865330286324, 0.00271129445172846, 0.0057487366721034, 0.00209597637876868, 0.00285439309664071, 0.0020907826256007, 0.0020875227637589, 0.00244470010511577, 0.0029609187040478, 0.00143002206459641, 0.00659507140517235, 0.00437884731218219, 0.00357141718268394, 0.00275131152011454, 0.0059390994720161, 0.00555154774338007, 0.00205262540839612, 0.00407942058518529, 0.00214859843254089, 0.00229802867397666, 0.00300152390263975, 0.00218682177364826, 0.00169280974660069, 0.00234391796402633, 0.0079372227191925, 0.00163373001851141, 0.00651902938261628, 0.00218894868157804, 0.0024097952991724, 0.00286445952951908, 0.00230710138566792, 0.00267284037545323, 0.00415562558919191, 0.00271529355086386, 0.00612750416621566, 0.00184183800593019, 0.00235510640777647, 0.0055224597454071, 0.00601162761449814, 0.00237067486159503, 0.00256828614510596, 0.00245809741318226, 0.00257832370698452, 0.00250777904875576, 0.00368545879609883, 0.00426747743040323, 0.00299239251762629, 0.00185814767610282, 0.00287382816895843, 0.00438322965055704, 0.00176559004466981, 0.00235549476929009, 0.00231917970813811, 0.00140539079438895, 0.00200617825612426, 0.00112507282756269, 0.00284891715273261, 0.00268161413259804, 0.0032090216409415, 0.00235756160691381, 0.00262081390246749, 0.00207906495779753, 0.00213415967300534, 0.00184848601929843, 0.00357765331864357, 0.00166653271298856, 0.00696725724264979, 0.00280454894527793, 0.00169487099628896 };
  static const int16_t buff_info_Conv2D_35_weights_inflated_254_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_46_weights_inflated_256_quant_scale[] = { 0.00103212869726121, 0.00186841224785894, 0.0016988015267998, 0.00174400059040636, 0.00179579458199441, 0.00188331864774227, 0.00181976309977472, 0.00140509242191911, 0.00142349221277982, 0.00170985714066774, 0.00134207343216985, 0.00137388694565743, 0.00152873271144927, 0.00124499015510082, 0.00135791848879308, 0.0016835363348946, 0.00176405510865152, 0.00169281708076596, 0.0016066808020696, 0.00159502704627812, 0.00233685784041882, 0.00179127580486238, 0.00164951430633664, 0.00163065083324909, 0.00136640609707683, 0.00125656265299767, 0.00172975345049053, 0.00170529307797551, 0.00220588990487158, 0.00165585079230368, 0.00194511003792286, 0.00138794688973576, 0.00201531616039574, 0.00187391764484346, 0.00144077965524048, 0.00169533095322549, 0.00182017497718334, 0.00153717515058815, 0.00188832252752036, 0.00157834636047482, 0.00156443589366972, 0.00166565296240151, 0.00201052706688643, 0.00244470965117216, 0.00149015057832003, 0.00120183965191245, 0.00135027221404016, 0.00180120929144323, 0.0019433336565271, 0.00119136250577867, 0.00152159412391484, 0.00162100535817444, 0.00163718103431165, 0.00173302355688065, 0.00108172034379095, 0.00126670754980296, 0.00173546804580837, 0.00149815808981657, 0.00166066980455071, 0.00185842008795589, 0.00189986289478838, 0.00159710098523647, 0.00198796926997602, 0.00181941827759147, 0.00188570539467037, 0.00184042402543128, 0.00135234370827675, 0.00164410402067006, 0.00130670366343111, 0.00145893159788102, 0.00138598319608718, 0.00178309786133468, 0.00147688074503094, 0.00166578148491681, 0.00137385562993586, 0.00166749896015972, 0.0019917597528547, 0.0014778469922021, 0.00176667049527168, 0.00228880625218153, 0.00178170658182353, 0.00149934692308307, 0.00117807905189693, 0.00133411935530603, 0.00122617615852505, 0.00196056650020182, 0.00165469665080309, 0.00149802165105939, 0.00155778171028942, 0.00156598316971213, 0.00157278135884553, 0.00175664259586483, 0.00143439730163664, 0.00198939372785389, 0.00146602664608508, 0.00151235493831336, 0.00151188753079623, 0.00144630717113614, 0.00188270106445998, 0.00133604323491454, 0.00261555728502572, 0.00146118458360434, 0.00117196759674698, 0.00179710739757866, 0.0014734243741259, 0.00152057176455855, 0.00164672639220953, 0.00183584343176335, 0.00162033352535218, 0.00108994101174176, 0.00206642737612128, 0.00178538158070296, 0.00207215338014066, 0.00138738704845309, 0.00190653814934194, 0.00120149936992675, 0.000905617489479482, 0.00163040438201278, 0.00111086678225547, 0.00294735468924046, 0.00159891147632152, 0.00180418277159333, 0.00191912264563143, 0.00164763687644154, 0.00182768574450165, 0.00131750828586519, 0.00130648224148899, 0.00143734051380306 };
  static const int16_t buff_info_Conv2D_46_weights_inflated_256_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_256_8_3_3[] = { 256, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_256_8_3_3[] = { 256, 3, 3, 8 };
  static const float buff_info_Conv2D_57_weights_inflated_258_quant_scale[] = { 0.00319199962541461, 0.00282893958501518, 0.00210055499337614, 0.00440941099077463, 0.00285166571848094, 0.00229078833945096, 0.00220531737431884, 0.00294656050391495, 0.00198136828839779, 0.00178544316440821, 0.00300186639651656, 0.00213754293508828, 0.00281536811962724, 0.00224037631414831, 0.00260251690633595, 0.00252184504643083, 0.00260399235412478, 0.00376640376634896, 0.00101225799880922, 0.00190581229981035, 0.00346524128690362, 0.00198411848396063, 0.00211760820820928, 0.00382616533897817, 0.00359851913526654, 0.0027045349124819, 0.00280903000384569, 0.00241723726503551, 0.00269383960403502, 0.00230512395501137, 0.00204784143716097, 0.00208712997846305, 0.0026885021943599, 0.00173501798417419, 0.00207586330361664, 0.00367896328680217, 0.00290603539906442, 0.00261419895105064, 0.00413146056234837, 0.00353242177516222, 0.00266682263463736, 0.0024631095584482, 0.00340901268646121, 0.00482502626255155, 0.00473635504022241, 0.00226726778782904, 0.00321658235043287, 0.00217147218063474, 0.00190824316814542, 0.00216603768058121, 0.00131730269640684, 0.00380854774266481, 0.00173094961792231, 0.00280629796907306, 0.0028128947596997, 0.00338654173538089, 0.00319511024281383, 0.00239339587278664, 0.00353011954575777, 0.00521398149430752, 0.00215719663538039, 0.00252915895543993, 0.00307128392159939, 0.00210233684629202, 0.00206939596682787, 0.00239534303545952, 0.00122961238957942, 0.0030923169106245, 0.00270000239834189, 0.00113735895138234, 0.00232705892995, 0.00237848702818155, 0.00287885707803071, 0.00303793512284756, 0.00187944946810603, 0.00181357725523412, 0.00171577639412135, 0.00231582694686949, 0.00237524975091219, 0.00237711239606142, 0.00375521671958268, 0.00300756609067321, 0.00205482309684157, 0.00252151442691684, 0.00423008622601628, 0.00263048359192908, 0.00272183585911989, 0.00378190958872437, 0.00169437727890909, 0.00484450673684478, 0.00205666804686189, 0.0022809284273535, 0.00243910448625684, 0.00263242144137621, 0.00198703678324819, 0.00283548980951309, 0.00230148457922041, 0.00273503037169576, 0.00326256174594164, 0.00186776067130268, 0.00236615655012429, 0.00324884313158691, 0.002328229136765, 0.00304509769193828, 0.00358056672848761, 0.00282818777486682, 0.00177381152752787, 0.00207910127937794, 0.00270906905643642, 0.00218805088661611, 0.00322895823046565, 0.00245867762714624, 0.00490388181060553, 0.00450444500893354, 0.00216465326957405, 0.00203832983970642, 0.00246399478055537, 0.00312232156284153, 0.002102539408952, 0.00285033509135246, 0.00220760237425566, 0.00227937521412969, 0.00365096610039473, 0.00290888478048146, 0.00114487123209983, 0.00227218912914395, 0.00264725740998983, 0.00249861017800868, 0.00174112862441689, 0.0037398359272629, 0.00221298099495471, 0.00199566688388586, 0.0022854192648083, 0.00234636734239757, 0.00399619853124022, 0.00266169407404959, 0.00255770888179541, 0.0023280035238713, 0.00283121434040368, 0.002636345801875, 0.003951795399189, 0.00306359631940722, 0.00404358562082052, 0.00187299936078489, 0.00311244139447808, 0.00465877959504724, 0.00213868776336312, 0.00146068527828902, 0.00378628936596215, 0.00158218841534108, 0.00217695883475244, 0.00272422120906413, 0.00136950914748013, 0.00185237673576921, 0.00509302970021963, 0.00165199674665928, 0.00240756222046912, 0.00236709089949727, 0.00180625100620091, 0.0017115012742579, 0.00460573984310031, 0.00276555749587715, 0.0030875806696713, 0.00242250575684011, 0.00274199014529586, 0.00428542355075479, 0.00255054072476923, 0.00227604899555445, 0.00288418587297201, 0.00209664739668369, 0.00291594048030674, 0.00484721641987562, 0.0029823889490217, 0.00181614106986672, 0.00302603165619075, 0.00206533912569284, 0.00221917126327753, 0.00353547278791666, 0.00307583017274737, 0.00209934660233557, 0.00296661723405123, 0.00257871299982071, 0.00214347429573536, 0.00187402334995568, 0.00235338532365859, 0.00163871678523719, 0.00193716830108315, 0.00229751248843968, 0.00276908045634627, 0.00210090866312385, 0.00238553434610367, 0.00431663496419787, 0.0020981952548027, 0.0025306127499789, 0.00112389633432031, 0.00206157402135432, 0.00203427392989397, 0.00421573827043176, 0.00165500584989786, 0.00266729039140046, 0.00226129032671452, 0.00170959101524204, 0.000833521422464401, 0.00225549307651818, 0.00372633314691484, 0.00320402043871582, 0.00260937167331576, 0.00208919704891741, 0.00178100634366274, 0.00144387362524867, 0.0031237758230418, 0.00211864989250898, 0.00209669512696564, 0.00214311177842319, 0.00201130658388138, 0.00311540905386209, 0.00292212818749249, 0.00205993023701012, 0.00238739186897874, 0.00226163631305099, 0.00253873760811985, 0.00393496872857213, 0.00451917480677366, 0.00270101637579501, 0.00239905179478228, 0.00391143653541803, 0.00539476796984673, 0.00400116480886936, 0.00344539224170148, 0.00265238853171468, 0.00230835448019207, 0.00403790082782507, 0.00282482104375958, 0.00220935791730881, 0.00257676281034946, 0.0020757825113833, 0.00168915023095906, 0.00266847503371537, 0.00385841960087419, 0.00190693535842001, 0.0021161010954529, 0.00306744128465652, 0.00344828376546502, 0.00237692706286907, 0.00263665127567947, 0.00272552925162017, 0.00181333592627198, 0.0044916314072907, 0.00831635668873787, 0.0021547032520175, 0.00325645739212632, 0.00335580878891051, 0.00455845193937421, 0.0016003018245101, 0.00389914400875568, 0.00132169958669692 };
  static const int16_t buff_info_Conv2D_57_weights_inflated_258_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_68_weights_inflated_260_quant_scale[] = { 0.00188507710117847, 0.00155452569015324, 0.00172785867471248, 0.00222993060015142, 0.0018986074719578, 0.00211204425431788, 0.00187058292794973, 0.00218297261744738, 0.00153470237273723, 0.00261379987932742, 0.000775550724938512, 0.00249391538091004, 0.00167491263709962, 0.0014415216865018, 0.00161993864458054, 0.00194804044440389, 0.00232098763808608, 0.00150243449024856, 0.00144226558040828, 0.00251955073326826, 0.00135386188048869, 0.00142792123369873, 0.00219561508856714, 0.00154550140723586, 0.00231522042304277, 0.00123445817735046, 0.00145348359365016, 0.00144939962774515, 0.00124719028826803, 0.00200305762700737, 0.0015020405407995, 0.00149173883255571, 0.00209958851337433, 0.00269723031669855, 0.00280311820097268, 0.0023041982203722, 0.00213441182859242, 0.0025119713973254, 0.00130171794444323, 0.00195416365750134, 0.00164125335868448, 0.0016082531074062, 0.00126130785793066, 0.00186201452743262, 0.00191015563905239, 0.00133053062018007, 0.00144017580896616, 0.00174007622990757, 0.00143920199479908, 0.0018309555016458, 0.00157549371942878, 0.00156141817569733, 0.00229357811622322, 0.00147485092747957, 0.00166246527805924, 0.0024356737267226, 0.00197418429888785, 0.000915610115043819, 0.00122277706395835, 0.00152054370846599, 0.00206430861726403, 0.0022088207770139, 0.00156832998618484, 0.0017370714340359, 0.00148215796798468, 0.00142560387030244, 0.00180289952550083, 0.00170410505961627, 0.00128566531930119, 0.00228888681158423, 0.00311545934528112, 0.00156950205564499, 0.00109551486093551, 0.00177510408684611, 0.00156446686014533, 0.00187674362678081, 0.00133136799558997, 0.00156114250421524, 0.00139437476173043, 0.0016806103521958, 0.00137789524160326, 0.0015452221268788, 0.00208766851574183, 0.00134366005659103, 0.00136184401344508, 0.00137020542751998, 0.00204299087636173, 0.00220382725819945, 0.00122081721201539, 0.00150715583004057, 0.00147729890886694, 0.00205681752413511, 0.00135629530996084, 0.00150180503260344, 0.00171249033883214, 0.00143281754571944, 0.00130564591381699, 0.00167238072026521, 0.0015259615611285, 0.00140006048604846, 0.00151175493374467, 0.0015306567074731, 0.00148057600017637, 0.00172847765497863, 0.00282990490086377, 0.00166310998611152, 0.00161332823336124, 0.00176922220271081, 0.00303065939806402, 0.00157719664275646, 0.00129893061239272, 0.00144033203832805, 0.00116775929927826, 0.0020443859975785, 0.00162498420104384, 0.00174432015046477, 0.00144345324952155, 0.00144122820347548, 0.00128200207836926, 0.00242598168551922, 0.00158571195788682, 0.00187879404984415, 0.00198870780877769, 0.00184965529479086, 0.0029470594599843, 0.00140679627656937, 0.00166440370958298, 0.00131150207016617, 0.00122981995809823, 0.00118647050112486, 0.00181894237175584, 0.00160072080325335, 0.0022693753708154, 0.00136840681079775, 0.00156440655700862, 0.00240902602672577, 0.00148371385876089, 0.00233055907301605, 0.00136625044979155, 0.00191900040954351, 0.00145739584695548, 0.00154860259499401, 0.00129312044009566, 0.00109635503031313, 0.00155129609629512, 0.0011845245026052, 0.0011680576717481, 0.00133023853413761, 0.00138830405194312, 0.00161471567116678, 0.00158239458687603, 0.00251836562529206, 0.00234009278938174, 0.00146777695044875, 0.00137439789250493, 0.0015485278563574, 0.0015371753834188, 0.00170539401005954, 0.00164666841737926, 0.00138541031628847, 0.00180069438647479, 0.00152605003677309, 0.00222334638237953, 0.00124327128287405, 0.002169419080019, 0.00176971114706248, 0.00149023113772273, 0.00149400031659752, 0.00171195948496461, 0.00137251394335181, 0.00181140191853046, 0.00147117127198726, 0.00142737478017807, 0.00133593950886279, 0.00208445056341588, 0.00192195700947195, 0.00156335858628154, 0.00212307693436742, 0.00227648206055164, 0.00131698523182422, 0.00147470342926681, 0.00103526096791029, 0.00240334263071418, 0.00176795886363834, 0.00133105262648314, 0.00151191127952188, 0.00292193936184049, 0.0013896634336561, 0.00103906984440982, 0.0013308305060491, 0.00133874290622771, 0.00204017828218639, 0.00192392873577774, 0.00159559864550829, 0.00157479976769537, 0.00102592876646668, 0.00115767982788384, 0.0011639358708635, 0.00209735170938075, 0.0029532490298152, 0.00150449073407799, 0.00182144099380821, 0.00169508194085211, 0.00178693851921707, 0.00150875572580844, 0.00202821823768318, 0.00177873752545565, 0.0019231530604884, 0.0014984862646088, 0.00147832895163447, 0.00133728876244277, 0.00119189010001719, 0.00148446101229638, 0.00174068077467382, 0.00147655233740807, 0.00146895775105804, 0.00148942938540131, 0.00169217807706445, 0.00144134438596666, 0.00195514387451112, 0.00328520848415792, 0.00126147957053035, 0.00155414128676057, 0.00125985138583928, 0.00198362534865737, 0.00118889939039946, 0.00178788171615452, 0.00143294536974281, 0.00127647758927196, 0.00163257506210357, 0.00229769549332559, 0.00153716572094709, 0.00123114814050496, 0.00150652159936726, 0.0012476253323257, 0.00182570272590965, 0.00179285800550133, 0.00109398318454623, 0.00140292628202587, 0.00138107093516737, 0.00152849918231368, 0.00109335500746965, 0.0020983072463423, 0.00185844965744764, 0.00114485539961606, 0.00104436976835132, 0.00116598489694297, 0.00154463003855199, 0.000928577443119138, 0.00162958563305438, 0.00144124182406813, 0.00153347360901535, 0.00175394560210407, 0.0015570861287415, 0.00123496726155281, 0.00141909625381231 };
  static const int16_t buff_info_Conv2D_68_weights_inflated_260_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_512_8_3_3[] = { 512, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_512_8_3_3[] = { 512, 3, 3, 8 };
  static const float buff_info_Conv2D_79_weights_inflated_262_quant_scale[] = { 0.00198690989054739, 0.00244212849065661, 0.00241997325792909, 0.00377644714899361, 0.00226999539881945, 0.00262408936396241, 0.00172865705098957, 0.00325540965422988, 0.00137097970582545, 0.00171100534498692, 0.00307894893921912, 0.00148178392555565, 0.0019725754391402, 0.00185120175592601, 0.00229547289200127, 0.0026672116946429, 0.00284319813363254, 0.00273116235621274, 0.00122422666754574, 0.00219754036515951, 0.00262131681665778, 0.00331367435865104, 0.00290165049955249, 0.00235491106286645, 0.00361176021397114, 0.00316987652331591, 0.00184381485451013, 0.00265840068459511, 0.00527447508648038, 0.00192606763448566, 0.00177775975316763, 0.00334349926561117, 0.00210103369317949, 0.00202755536884069, 0.00323125650174916, 0.000887149362824857, 0.00317949359305203, 0.00115969288162887, 0.00219520088285208, 0.00232301210053265, 0.00239544827491045, 0.00264437333680689, 0.0023441263474524, 0.00240129977464676, 0.00242095580324531, 0.00226632412523031, 0.0016101251821965, 0.00257140095345676, 0.00225178548134863, 0.00295652216300368, 0.00121220946311951, 0.00223413086496294, 0.00253842608071864, 0.00156175473239273, 0.00230575678870082, 0.0020744944922626, 0.00164582033175975, 0.00215059029869735, 0.00245091202668846, 0.00309207173995674, 0.00132093415595591, 0.00202769297175109, 0.00202386686578393, 0.00226428639143705, 0.00229471386410296, 0.00357758020982146, 0.00264234165661037, 0.00162781495600939, 0.00224705110304058, 0.00222528679296374, 0.00229894299991429, 0.00345828267745674, 0.00327325263060629, 0.00452457787469029, 0.00277535780332983, 0.0031266629230231, 0.00159292831085622, 0.00279707624576986, 0.00127907528076321, 0.00192063744179904, 0.000999464537017047, 0.00210234615951777, 0.00132402672898024, 0.00231899786740541, 0.00168271048460156, 0.00409621512517333, 0.00268731289543211, 0.0011600780999288, 0.00305206468328834, 0.00119905709289014, 0.00184846634510905, 0.00269519607536495, 0.00140052160713822, 0.00260457466356456, 0.00225683720782399, 0.00199067429639399, 0.00233184453099966, 0.00293551990762353, 0.00294604734517634, 0.00276311952620745, 0.00178841769229621, 0.00395313836634159, 0.00238434690982103, 0.00236015720292926, 0.00157674902584404, 0.00168692611623555, 0.00180709117557853, 0.00187159795314074, 0.00292322086170316, 0.00199958891607821, 0.00144351634662598, 0.00311576016247272, 0.00254805735312402, 0.00243713660165668, 0.00137274002190679, 0.001733397715725, 0.000910882547032088, 0.00351618835702538, 0.00299278344027698, 0.00255596032366157, 0.00578447850421071, 0.00268560042604804, 0.00142674858216196, 0.00232952600345016, 0.00197085202671587, 0.00204799114726484, 0.00247333641164005, 0.00238099717535079, 0.00242742872796953, 0.00276824273169041, 0.00160029227845371, 0.0013791685923934, 0.00184783316217363, 0.00128837116062641, 0.00279723224230111, 0.00166003999765962, 0.00206944136880338, 0.00171141256578267, 0.00152307515963912, 0.00313920713961124, 0.00287600257433951, 0.00246260175481439, 0.00205096323043108, 0.00264516100287437, 0.00241235853172839, 0.00121718621812761, 0.0023224912583828, 0.00227814679965377, 0.0011606477200985, 0.00218457472510636, 0.00188239652197808, 0.000915753073059022, 0.00219307211227715, 0.00169964379165322, 0.00182366569060832, 0.00186887045856565, 0.00268666539341211, 0.00139974313788116, 0.00230642175301909, 0.001342533971183, 0.00192564411554486, 0.00190761813428253, 0.00314317177981138, 0.00180633179843426, 0.0015132020926103, 0.00250397506169975, 0.00101859378628433, 0.00257667084224522, 0.00270053371787071, 0.00185093912295997, 0.00242552463896573, 0.00231381854973733, 0.00195362092927098, 0.0025571018923074, 0.00230721267871559, 0.00317570893093944, 0.0016939373454079, 0.0015174854779616, 0.00244682445190847, 0.00209784880280495, 0.00121295708231628, 0.00258193630725145, 0.00414616428315639, 0.000868405972141773, 0.00146162847522646, 0.00178460753522813, 0.00240843207575381, 0.00233960500918329, 0.00190547353122383, 0.00311024370603263, 0.0014390533324331, 0.000781717186328024, 0.0022287757601589, 0.0026465430855751, 0.00487753376364708, 0.00293330755084753, 0.00211332505568862, 0.00153004424646497, 0.00232478533871472, 0.00147864769678563, 0.00228991988115013, 0.00328509183600545, 0.00256105442531407, 0.00276542804203928, 0.00339433783665299, 0.0024942469317466, 0.00233047921210527, 0.00298316427506506, 0.00156099838204682, 0.00259531196206808, 0.00266145612113178, 0.00316097331233323, 0.00137064117006958, 0.00350800948217511, 0.000833441445138305, 0.00185635953675956, 0.00316787441261113, 0.00238016596995294, 0.00154815020505339, 0.00348542095161974, 0.00194754404947162, 0.00163786776829511, 0.00295012048445642, 0.00203963555395603, 0.0026464678812772, 0.00468689668923616, 0.00350579968653619, 0.00234356988221407, 0.00263580260798335, 0.00287086446769536, 0.00175977684557438, 0.0035450158175081, 0.00135196070186794, 0.00232080835849047, 0.00202378048561513, 0.00173733045812696, 0.00259887217544019, 0.00199835421517491, 0.00270152906887233, 0.00108841434121132, 0.00213958485983312, 0.00223878980614245, 0.00321893044747412, 0.00125966558698565, 0.00121298886369914, 0.000924955820664763, 0.00437059672549367, 0.000885340035893023, 0.0023099398240447, 0.00314296851865947, 0.00113992590922862, 0.00308880629017949, 0.00154219870455563, 0.00185598398093134, 0.0015823730500415, 0.00316935055889189, 0.00218908325769007, 0.00204734480939806, 0.00281386915594339, 0.00131783820688725, 0.00297644012607634, 0.00268572382628918, 0.00122672994621098, 0.00233790930360556, 0.00289406650699675, 0.00204751268029213, 0.001718835439533, 0.00345520698465407, 0.00249305833131075, 0.00110062852036208, 0.00178151798900217, 0.00293826847337186, 0.00109093822538853, 0.00231493543833494, 0.00557052250951529, 0.00207562651485205, 0.00231437617912889, 0.00195746682584286, 0.00283109629526734, 0.0036532434169203, 0.00220917444676161, 0.00317066838033497, 0.00273871561512351, 0.00201998790726066, 0.00257148966193199, 0.00246960204094648, 0.00163118599448353, 0.00285952817648649, 0.00127201247960329, 0.00154743797611445, 0.00154647545423359, 0.00184142333455384, 0.00184670311864465, 0.001810148707591, 0.00142352073453367, 0.00239353883080184, 0.00322857149876654, 0.00247960188426077, 0.00264683971181512, 0.00296323583461344, 0.00313838105648756, 0.00259982305578887, 0.00301435170695186, 0.00255637685768306, 0.00261200475506485, 0.00199555861763656, 0.00224099867045879, 0.00177257822360843, 0.00138828449416906, 0.00212690513581038, 0.00259220786392689, 0.00204360811039805, 0.00352796725928783, 0.00375077803619206, 0.00181516539305449, 0.00266541144810617, 0.00122111337259412, 0.00117509893607348, 0.00257669249549508, 0.00202997354790568, 0.00326757575385273, 0.00185502995736897, 0.00250554922968149, 0.0032870820723474, 0.00327909970656037, 0.000826859672088176, 0.00249606277793646, 0.00347479153424501, 0.00195037736557424, 0.00249181175604463, 0.00235766870900989, 0.00135496025905013, 0.00143183884210885, 0.00237983535043895, 0.00122536381240934, 0.0019447150407359, 0.00213687680661678, 0.0021757900249213, 0.00213962071575224, 0.00284139183349907, 0.00131561979651451, 0.00148021115455776, 0.00333909294568002, 0.00172102765645832, 0.00284853880293667, 0.00209436239674687, 0.0026807386893779, 0.00418033124879003, 0.00672594085335732, 0.00201020110398531, 0.00176305277273059, 0.00137225457001477, 0.00121163472067565, 0.00231553171761334, 0.00313775218091905, 0.00252524018287659, 0.00226612365804613, 0.00303897983394563, 0.00191822671331465, 0.00281436601653695, 0.0021447294857353, 0.00225021806545556, 0.00229952484369278, 0.0023128273896873, 0.00263256719335914, 0.00286883371882141, 0.00195181788876653, 0.0018713817698881, 0.00136443169321865, 0.00295975408516824, 0.00383044080808759, 0.00227862596511841, 0.00282652140595019, 0.00171999097801745, 0.00299373059533536, 0.00240932893939316, 0.00288894842378795, 0.00379763939417899, 0.00139284518081695, 0.00374835287220776, 0.00328577868640423, 0.00257922895252705, 0.00309934746474028, 0.00123394699767232, 0.00217060605064034, 0.0023860945366323, 0.00240154657512903, 0.0025708144530654, 0.00203996174968779, 0.00283109722658992, 0.00253304908983409, 0.00261470535770059, 0.00183490640483797, 0.00144587201066315, 0.00174218241591007, 0.00179318350274116, 0.00251979567110538, 0.00225632381625473, 0.00190183112863451, 0.00177761085797101, 0.00494170095771551, 0.00257233832962811, 0.00491085136309266, 0.00169526401441544, 0.00133981369435787, 0.00254335696808994, 0.00397218251600862, 0.00157274375669658, 0.00266671669669449, 0.00248678307980299, 0.00146711349952966, 0.0025758701376617, 0.00261555425822735, 0.00202244962565601, 0.00278902566060424, 0.00298203807324171, 0.00205280119553208, 0.00164486735593528, 0.0032017023768276, 0.00138713489286602, 0.00263813836500049, 0.00243474007584155, 0.0034389877691865, 0.00268468097783625, 0.00155858835205436, 0.00252201920375228, 0.00167711370158941, 0.0034074611030519, 0.00372370192781091, 0.00350774871185422, 0.00172770442441106, 0.00314319948665798, 0.00369101157411933, 0.00130716734565794, 0.00237754825502634, 0.0036219519097358, 0.00455873133614659, 0.00355916167609394, 0.00196707248687744, 0.00219878018833697, 0.00113953079562634, 0.00106853223405778, 0.00158829404972494, 0.00210460624657571, 0.00193271052557975, 0.00171509617939591, 0.00410086428746581, 0.00345060229301453, 0.00249875918962061, 0.00372493057511747, 0.00152106210589409, 0.00286361365579069, 0.00196791556663811, 0.00162044854369015, 0.00304140732623637, 0.0029046693816781, 0.00240147835575044, 0.0021888162009418, 0.00222236849367619, 0.00132403522729874, 0.00229483679868281, 0.00207858649082482, 0.00219839275814593, 0.00235833856277168, 0.00121770577970892, 0.00269016297534108, 0.00222186278551817, 0.00284486543387175, 0.00193055102135986, 0.0013167301658541, 0.000764809781685472, 0.00301273469813168, 0.00118735258001834, 0.00428807828575373, 0.00210642674937844, 0.0037055304273963, 0.00331727741286159, 0.00564605137333274, 0.00215604924596846, 0.00200001313351095, 0.0018965577473864, 0.00408971589058638, 0.00251582032069564, 0.0017420151270926, 0.00215871236287057, 0.00144579389598221, 0.00432777311652899, 0.00242274953052402, 0.00100227433722466, 0.00220924546010792, 0.00381616735830903, 0.00295177870430052, 0.00208244379609823, 0.00243353797122836, 0.00125755893532187, 0.00323793035931885, 0.00113663694355637, 0.00257711973972619, 0.00157882692292333, 0.0019176930654794, 0.00249778153374791, 0.00224212254397571, 0.00226311688311398, 0.00170029338914901, 0.00181058770976961, 0.0014973261859268, 0.00185487396083772, 0.0030813415069133, 0.00161997275426984, 0.00207820301875472, 0.00188873265869915, 0.00181255326606333 };
  static const int16_t buff_info_Conv2D_79_weights_inflated_262_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_90_weights_inflated_264_quant_scale[] = { 0.00190627528354526, 0.00161507935263216, 0.00190281728282571, 0.00234193471260369, 0.00207631709054112, 0.000805074698291719, 0.00257204682566226, 0.00182583369314671, 0.00171549629885703, 0.00174472504295409, 0.0023295171558857, 0.0033000442199409, 0.00183357461355627, 0.00158630835358053, 0.0020533197093755, 0.00330333667807281, 0.00351289217360318, 0.00217907386831939, 0.0027279385831207, 0.00298696733079851, 0.00207529333420098, 0.00291629880666733, 0.00360209308564663, 0.00107150303665549, 0.0020413005258888, 0.00104737572837621, 0.00173742207698524, 0.00243391701951623, 0.00117473804857582, 0.00176585791632533, 0.00248568551614881, 0.00163315387908369, 0.00273252534680068, 0.00138512079138309, 0.00206734915263951, 0.00266091269440949, 0.00172111310530454, 0.00160628766752779, 0.00160328845959157, 0.00229662121273577, 0.00319575681351125, 0.00142728234641254, 0.00183793355245143, 0.00214984826743603, 0.00214255321770906, 0.00291578914038837, 0.00224841665476561, 0.00291596469469368, 0.0027684485539794, 0.00100319541525096, 0.00311869289726019, 0.00165029615163803, 0.00227466784417629, 0.00180046528112143, 0.00224081473425031, 0.00201287679374218, 0.00170899974182248, 0.00219082017429173, 0.00227491836994886, 0.00223710783757269, 0.00123562361113727, 0.00280564487911761, 0.00278215063735843, 0.00199492112733424, 0.00123102066572756, 0.00319113791920245, 0.00274714757688344, 0.00162568618543446, 0.00113139511086047, 0.00145755393896252, 0.00252838339656591, 0.00305982097052038, 0.00250014360062778, 0.00161037896759808, 0.000855190795846283, 0.00191235367674381, 0.00181571883149445, 0.0010702203726396, 0.00277905003167689, 0.00237394077703357, 0.00131706718821079, 0.00161220924928784, 0.000930774491280317, 0.00224265968427062, 0.0023607884068042, 0.00133421435020864, 0.00381330354139209, 0.00192074524238706, 0.00284855929203331, 0.00140446389559656, 0.00120431580580771, 0.00151391664985567, 0.00236689089797437, 0.00178412871900946, 0.00202475697733462, 0.00176766363438219, 0.00130381528288126, 0.00254968879744411, 0.00235773297026753, 0.00241984520107508, 0.00195723329670727, 0.00194853788707405, 0.00251985713839531, 0.00270489556714892, 0.00127958424855024, 0.00321366637945175, 0.00273872306570411, 0.00137928500771523, 0.00195898301899433, 0.00286113424226642, 0.00262184860184789, 0.00175212358590215, 0.00197715475223958, 0.00171202269848436, 0.0032055638730526, 0.00293044093996286, 0.00356224505230784, 0.0018591278931126, 0.00238023372367024, 0.00252017704769969, 0.00175579183269292, 0.00137592293322086, 0.00111142895184457, 0.00184470100793988, 0.00169348367489874, 0.00276327272877097, 0.00327159045264125, 0.00154430884867907, 0.00146749767009169, 0.00257936306297779, 0.00291938381269574, 0.00256590079516172, 0.00161802116781473, 0.00131822726689279, 0.00151957396883518, 0.00278046983294189, 0.00185336999129504, 0.000761649454943836, 0.0027000829577446, 0.00125805148854852, 0.00339630269445479, 0.00249743880704045, 0.00257164658978581, 0.0015855252277106, 0.00200257613323629, 0.0031914550345391, 0.00126395432744175, 0.00264930073171854, 0.00127070839516819, 0.00202544266358018, 0.00172256713267416, 0.00320662627927959, 0.0035440749488771, 0.00196665665134788, 0.00230993214063346, 0.00251634698361158, 0.00287695834413171, 0.0018619765760377, 0.00216752733103931, 0.00349771976470947, 0.00343015417456627, 0.00199675047770143, 0.00190849520731717, 0.00218923995271325, 0.00277031818404794, 0.00154837314039469, 0.00239734188653529, 0.0021544168703258, 0.00242683081887662, 0.00165370630566031, 0.00241596228443086, 0.00106564024463296, 0.00280351075343788, 0.00275366869755089, 0.00258765299804509, 0.00297965481877327, 0.00317771057598293, 0.00210246909409761, 0.00161307153757662, 0.00263236369937658, 0.00167430331930518, 0.00283994153141975, 0.00159182527568191, 0.00141720776446164, 0.00170793663710356, 0.00247368705458939, 0.00131182395853102, 0.00255722040310502, 0.00357605679892004, 0.00385376182384789, 0.00146974506787956, 0.00193966261576861, 0.00201316736638546, 0.00294082937762141, 0.00378256198018789, 0.00183390616439283, 0.00169142265804112, 0.00269012292847037, 0.00191121082752943, 0.00367130292579532, 0.00137219263706356, 0.00357287796214223, 0.00267392070963979, 0.00301472819410264, 0.00172285549342632, 0.00130594230722636, 0.00258012372069061, 0.00113021139986813, 0.00100537959951907, 0.00286423461511731, 0.00435321033000946, 0.00250049796886742, 0.00452432502061129, 0.00183973670937121, 0.00293977349065244, 0.00130489852745086, 0.00219830335117877, 0.00195241998881102, 0.0024276536423713, 0.00192865415010601, 0.00229369616135955, 0.00171571352984756, 0.00177737534977496, 0.00366990827023983, 0.00151790655218065, 0.00189865683205426, 0.00235100113786757, 0.00142652646172792, 0.00166769069619477, 0.00340169086121023, 0.00166135199833661, 0.00144154415465891, 0.00161223823670298, 0.00226595578715205, 0.00259967125020921, 0.00167873990722001, 0.00291124382056296, 0.00299519742839038, 0.00318161305040121, 0.00245912186801434, 0.0026972945779562, 0.002619398990646, 0.0024377207737416, 0.00225286581553519, 0.00276936730369925, 0.00250469567254186, 0.00137725344393402, 0.00174238660838455, 0.00235488079488277, 0.00165156624279916, 0.00144941743928939, 0.00170548050664365, 0.00222621927969158, 0.00264564016833901, 0.00184532289858907, 0.00231083249673247, 0.00271914107725024, 0.00207170820795, 0.00231029186397791, 0.00334019749425352, 0.00222483207471669, 0.00254181097261608, 0.00307210045866668, 0.00221164105460048, 0.00485870148986578, 0.0014827725244686, 0.00271915178745985, 0.00195224524941295, 0.00178240414243191, 0.00182405696250498, 0.00270253303460777, 0.00239640683867037, 0.00169967487454414, 0.00380386179313064, 0.00136472843587399, 0.00364603195339441, 0.00143169693183154, 0.0017251247772947, 0.00247005419805646, 0.00168921367730945, 0.00182002177461982, 0.00142041337676346, 0.00297174579463899, 0.00262421206571162, 0.00104344298597425, 0.00468385778367519, 0.00295442366041243, 0.0024468288756907, 0.00201214430853724, 0.0021121974568814, 0.00240474590100348, 0.00147364276926965, 0.00311406259424984, 0.00276866718195379, 0.00327269267290831, 0.00142649933695793, 0.00271608540788293, 0.00244496972300112, 0.00166204664856195, 0.00217736302874982, 0.00115355150774121, 0.00153571518603712, 0.00240848772227764, 0.00219370611011982, 0.00336253503337502, 0.00281827710568905, 0.00204979674890637, 0.00156059430446476, 0.00224373699165881, 0.0010459553450346, 0.00168880599085242, 0.00190494395792484, 0.00180794089101255, 0.001817308482714, 0.00175272801425308, 0.00198364583775401, 0.00235760491341352, 0.00214851461350918, 0.00250749220140278, 0.00158805272076279, 0.00247596274130046, 0.00213124393485487, 0.00243148556910455, 0.00216827541589737, 0.00214693555608392, 0.00234802672639489, 0.00187509530223906, 0.00517394812777638, 0.00348561489954591, 0.00259707914665341, 0.0026953334454447, 0.00133636849932373, 0.00266546406783164, 0.00375297083519399, 0.0033659387845546, 0.00240733358077705, 0.00218739942647517, 0.00290698208846152, 0.00148681458085775, 0.00240025762468576, 0.00234544486738741, 0.00158680253662169, 0.00227742921561003, 0.00246139522641897, 0.00283981836400926, 0.0039992444217205, 0.00140526751056314, 0.00302925077266991, 0.00419789459556341, 0.0020993622019887, 0.00203549908474088, 0.00247987196780741, 0.00203148438595235, 0.00277014169842005, 0.0023730993270874, 0.00125990796368569, 0.00150778295937926, 0.00261283968575299, 0.00253037572838366, 0.00222296034917235, 0.00170088047161698, 0.0023118294775486, 0.0016985279507935, 0.00249224645085633, 0.00333621422760189, 0.00170651450753212, 0.000832818157505244, 0.00208546104840934, 0.0018815875519067, 0.00256161391735077, 0.00321363052353263, 0.0025982060469687, 0.00352214509621263, 0.00176277908030897, 0.00159834185615182, 0.00165797816589475, 0.00170648598577827, 0.00161353684961796, 0.00268341018818319, 0.00213027722202241, 0.00213978579267859, 0.00296232104301453, 0.00247361417859793, 0.00151912414003164, 0.00152113207150251, 0.00124421704094857, 0.00196963967755437, 0.00205700192600489, 0.00227591535076499, 0.00242905225604773, 0.00216471822932363, 0.00282989791594446, 0.00361380469985306, 0.000697723706252873, 0.00126432825345546, 0.00136185809969902, 0.00295856967568398, 0.00214907573536038, 0.00209597172215581, 0.00367188663221896, 0.00223070033825934, 0.001972371712327, 0.00201610033400357, 0.00349835678935051, 0.00208363169804215, 0.00133000314235687, 0.00318993045948446, 0.00127868400886655, 0.00107946258503944, 0.00154505565296859, 0.00140761723741889, 0.00249850703403354, 0.00205797771923244, 0.00121028849389404, 0.00235676974989474, 0.00233092647977173, 0.00177060032729059, 0.00212654029019177, 0.00291770440526307, 0.00337816448882222, 0.00162781064864248, 0.00141053239349276, 0.00232972274534404, 0.00231938972137868, 0.00190799927804619, 0.00320475338958204, 0.00302555807866156, 0.00211543287150562, 0.00115071993786842, 0.00169075781013817, 0.0021320297382772, 0.00609549507498741, 0.00225698878057301, 0.00218203291296959, 0.00275417859666049, 0.00277373241260648, 0.0018263760721311, 0.0029067718423903, 0.0014696279540658, 0.00228394917212427, 0.00169464061036706, 0.00312779261730611, 0.00466977013275027, 0.00210657482966781, 0.00138341297861189, 0.00191796210128814, 0.00104833696968853, 0.00135921617038548, 0.00213867798447609, 0.00171293364837766, 0.0016448589740321, 0.00176001968793571, 0.00145100150257349, 0.0041345888748765, 0.00568118365481496, 0.00227134465239942, 0.00325648998841643, 0.00191141117829829, 0.00221426575444639, 0.00213075545616448, 0.00250420602969825, 0.00301249208860099, 0.00232582562603056, 0.00160760735161602, 0.0025172159075737, 0.00178473675623536, 0.0011174421524629, 0.00196805852465332, 0.00154933671001345, 0.00251181190833449, 0.0019095204770565, 0.002109803725034, 0.00244587589986622, 0.00237419432960451, 0.00247149309143424, 0.00240257661789656, 0.00224804668687284, 0.00230562314391136, 0.00156007171608508, 0.00247841817326844, 0.00331265991553664, 0.00238444679416716, 0.00484068505465984, 0.00276293023489416, 0.00452493969351053, 0.00204880256205797, 0.00258662714622915, 0.00248564570210874, 0.00261697382666171, 0.00277383578941226, 0.00103170273359865, 0.00173580378759652, 0.00245922058820724, 0.00363573501817882, 0.00168917025439441, 0.000987466191872954, 0.00271883234381676, 0.00317986216396093, 0.00151043583173305, 0.00191985804121941, 0.00213475502096117, 0.00354153313674033, 0.00238250265829265, 0.00152463128324598, 0.00288768159225583, 0.00245162681676447, 0.00499007385224104, 0.00140307785477489, 0.00211832928471267, 0.00238451012410223, 0.00188656209502369, 0.0032181607093662 };
  static const int16_t buff_info_Conv2D_90_weights_inflated_264_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_101_weights_inflated_266_quant_scale[] = { 0.00258590839803219, 0.00173869996797293, 0.00241805636323988, 0.00200583855621517, 0.00191657745745033, 0.00103048386517912, 0.00160697021055967, 0.00323081598617136, 0.0015490249497816, 0.00125581270549446, 0.00236938847228885, 0.00424532918259501, 0.00240225857123733, 0.0020280652679503, 0.00286342878825963, 0.00357844983227551, 0.0024633607827127, 0.00251550693064928, 0.0012932390673086, 0.00103004323318601, 0.00169200485106558, 0.00210247049108148, 0.00302959559485316, 0.00171275238972157, 0.00119818549137563, 0.0017353700241074, 0.00304110930301249, 0.00288093951530755, 0.0015263285022229, 0.00278725987300277, 0.00203894078731537, 0.0026200725696981, 0.00252548465505242, 0.00266808993183076, 0.00240498641505837, 0.00252355961129069, 0.00144546129740775, 0.00227245781570673, 0.00145719537977129, 0.00254502263851464, 0.00211399723775685, 0.00155953678768128, 0.00127012562006712, 0.0028878862503916, 0.00279112812131643, 0.00156504940241575, 0.00120356329716742, 0.002652378519997, 0.00174341828096658, 0.00281488709151745, 0.00313382456079125, 0.00242980453185737, 0.00198577251285315, 0.00276839407160878, 0.00188109360169619, 0.00207853759638965, 0.00263076671399176, 0.00361385918222368, 0.00249757780693471, 0.00261312979273498, 0.00080695771612227, 0.00246945978142321, 0.00203616754151881, 0.00202687038108706, 0.00313848815858364, 0.00460899481549859, 0.00346212624572217, 0.00347125646658242, 0.00264520756900311, 0.00300522102043033, 0.0024251970462501, 0.00202298094518483, 0.00161650602240115, 0.000986431608907878, 0.00243563787080348, 0.0026578342076391, 0.00263402238488197, 0.00262322369962931, 0.00183335377369076, 0.00263484357856214, 0.00241439812816679, 0.000993892434053123, 0.00193755922373384, 0.00333589012734592, 0.00146711128763855, 0.00159733451437205, 0.00181230180896819, 0.0032899749930948, 0.00114480592310429, 0.00158310797996819, 0.00174219626933336, 0.00279435981065035, 0.00184408109635115, 0.00259699020534754, 0.00302737602032721, 0.00107379059772938, 0.00255817989818752, 0.00262374244630337, 0.00206686765886843, 0.00283013307489455, 0.00139747036155313, 0.00156474055256695, 0.00182100408710539, 0.00160710047930479, 0.00168912543449551, 0.000939023680984974, 0.00251162727363408, 0.00116067787166685, 0.0025814997497946, 0.00215021031908691, 0.00246033119037747, 0.00226826826110482, 0.00155740301124752, 0.00170155509840697, 0.0018949881196022, 0.00263036042451859, 0.00243766163475811, 0.00367785221897066, 0.00206389091908932, 0.00142971659079194, 0.00210815854370594, 0.00260883010923862, 0.00136905955150723, 0.00202623568475246, 0.00243406509980559, 0.00258602667599916, 0.0023943493142724, 0.00267824740149081, 0.00390533776953816, 0.00362213794142008, 0.0014289862010628, 0.00230645900592208, 0.00265849521383643, 0.00240399269387126, 0.00191180815454572, 0.00207708799280226, 0.00127588212490082, 0.00150633719749749, 0.00348490243777633, 0.00227200402878225, 0.00220402097329497, 0.00135912024416029, 0.00119031185749918, 0.00132034800481051, 0.00158858532086015, 0.00269560934975743, 0.0035249690990895, 0.00129144475795329, 0.00246615661308169, 0.00147999438922852, 0.00150071387179196, 0.00236316816881299, 0.00289953500032425, 0.00123507203534245, 0.00157857174053788, 0.00255433516576886, 0.0020263975020498, 0.00287867290899158, 0.00289163854904473, 0.00269581098109484, 0.0023486188147217, 0.00174957199487835, 0.0020262086763978, 0.00236209854483604, 0.00193774700164795, 0.00179974630009383, 0.00270471209660172, 0.00313852471299469, 0.001684166258201, 0.00137558160349727, 0.00152854190673679, 0.00205566128715873, 0.0020999382250011, 0.00247331452555954, 0.00224242825061083, 0.00157895870506763, 0.0013199980603531, 0.00237277918495238, 0.00193378934636712, 0.00218371814116836, 0.00177189207170159, 0.00315230526030064, 0.00109279691241682, 0.00254721287637949, 0.0024345547426492, 0.00176422507502139, 0.00400120532140136, 0.00236046500504017, 0.00234666187316179, 0.00125258113257587, 0.00159432273358107, 0.00324133690446615, 0.00289965281262994, 0.00175429473165423, 0.00189143174793571, 0.00205847970210016, 0.00249428930692375, 0.00294413766823709, 0.0025766605976969, 0.00143201381433755, 0.00333180255256593, 0.00189204467460513, 0.00304683949798346, 0.00236443569883704, 0.00143264723010361, 0.00300034694373608, 0.00341723510064185, 0.00311500346288085, 0.00227893888950348, 0.00219007791019976, 0.00276590813882649, 0.00226390291936696, 0.00138438958674669, 0.00269077019765973, 0.00227907462976873, 0.00264464621432126, 0.00115174660459161, 0.00121787411626428, 0.00278306612744927, 0.00201516691595316, 0.00230292230844498, 0.00122010568156838, 0.00196477747522295, 0.00139865430537611, 0.00190958415623754, 0.00271841837093234, 0.0017486687283963, 0.00237002340145409, 0.00331173627637327, 0.00319346040487289, 0.0024771667085588, 0.0023445887491107, 0.00231785699725151, 0.00249364110641181, 0.00209594378247857, 0.00176118779927492, 0.00120733864605427, 0.00250662281177938, 0.00133310223463923, 0.00282095395959914, 0.00239578424952924, 0.00180142349563539, 0.0015792699996382, 0.00264792703092098, 0.00191203597933054, 0.00271805562078953, 0.00189729419071227, 0.00174905720632523, 0.00384890707209706, 0.00179131550248712, 0.00475772749632597, 0.0027516158297658, 0.00193690205924213, 0.00147884478792548, 0.00446277158334851, 0.00149307621177286, 0.00262136897072196, 0.00213470216840506, 0.00310526113025844, 0.0020015244372189, 0.00371533236466348, 0.00107335776556283, 0.00108218961395323, 0.00115815224125981, 0.00201801559887826, 0.00232259114272892, 0.00225816294550896, 0.00242073344998062, 0.00254975445568562, 0.00282045872882009, 0.00254735816270113, 0.00224266480654478, 0.00189506681635976, 0.00268939649686217, 0.0016568269347772, 0.00496877124533057, 0.0024057615082711, 0.00184335245285183, 0.00345958699472249, 0.00278409477323294, 0.00199340097606182, 0.00267269881442189, 0.00241097691468894, 0.00101002713199705, 0.0026610151398927, 0.00120919139590114, 0.00229440117254853, 0.00227270903997123, 0.00111800176091492, 0.00273223500698805, 0.00123632769100368, 0.00238840002566576, 0.00355114252306521, 0.00310139660723507, 0.00221503991633654, 0.00235931458882987, 0.00244964705780149, 0.00668942322954535, 0.00259119365364313, 0.00219973502680659, 0.00229777861386538, 0.00255582225508988, 0.00224779173731804, 0.00392962154000998, 0.00253272033296525, 0.00305523793213069, 0.002400869037956, 0.0018951635574922, 0.00212591816671193, 0.00103296816814691, 0.000682789599522948, 0.00266619841568172, 0.00263239117339253, 0.00292141805402935, 0.00170642952434719, 0.00323046813718975, 0.00221478077583015, 0.00120554864406586, 0.00616274680942297, 0.00273213325999677, 0.0024743617977947, 0.00188395474106073, 0.00245993095450103, 0.00194688956253231, 0.00227705389261246, 0.00270239007659256, 0.00210567004978657, 0.00319541618227959, 0.00241425167769194, 0.00163309124764055, 0.00308427377603948, 0.00176464812830091, 0.00228972057811916, 0.00159949902445078, 0.00192566006444395, 0.00207592360675335, 0.00177872052881867, 0.00389145989902318, 0.00267176632769406, 0.00313695077784359, 0.00374801410362124, 0.00294604455120862, 0.00340605550445616, 0.00225075636990368, 0.00232255505397916, 0.0028351373039186, 0.0025870525278151, 0.00551694864407182, 0.00333884358406067, 0.00273220846429467, 0.00238896603696048, 0.00276122288778424, 0.00138982234057039, 0.00270417029969394, 0.00314000574871898, 0.00351529638282955, 0.00285039586015046, 0.00254190480336547, 0.00220038206316531, 0.00212669977918267, 0.00238453177735209, 0.00281389756128192, 0.00170750997494906, 0.00262906542047858, 0.00102395878639072, 0.00288317212834954, 0.0022988913115114, 0.00266282563097775, 0.00387185160070658, 0.00223115179687738, 0.00223376974463463, 0.00149505154695362, 0.00197439058683813, 0.0033714952878654, 0.00267509580589831, 0.00206334353424609, 0.00301119592040777, 0.00133837817702442, 0.00201292568817735, 0.00294316839426756, 0.00212344294413924, 0.00267813797108829, 0.00375796924345195, 0.0024758989457041, 0.00185127649456263, 0.000932317110709846, 0.00151790014933795, 0.0017927719745785, 0.00261161150410771, 0.00137251091655344, 0.00357376085594296, 0.00241912109777331, 0.00330213410779834, 0.00189476378727704, 0.00312227103859186, 0.00147532904520631, 0.00164567318279296, 0.0020599418785423, 0.00279738521203399, 0.00219311518594623, 0.00235268985852599, 0.0041882237419486, 0.00213827542029321, 0.00186927162576467, 0.00228419713675976, 0.00223242817446589, 0.00132968532852829, 0.00285931932739913, 0.0012004739837721, 0.00221809954382479, 0.00247381837107241, 0.00270772422663867, 0.00134367030113935, 0.00267102685756981, 0.00156594056170434, 0.0030528821516782, 0.00345373013988137, 0.00125055620446801, 0.00147092516999692, 0.00149275572039187, 0.00287472899071872, 0.00123265001457185, 0.00250680488534272, 0.00324337929487228, 0.0030722850933671, 0.00335179641842842, 0.00136186776217073, 0.003211525734514, 0.00274948985315859, 0.00256402068771422, 0.00100262253545225, 0.00144286511931568, 0.00353139266371727, 0.00298665883019567, 0.00241257017478347, 0.00266710296273232, 0.00171358208172023, 0.00190326734445989, 0.00150538433808833, 0.00240791123360395, 0.00321637187153101, 0.00196711695753038, 0.00176866527181119, 0.00225323578342795, 0.0020532098133117, 0.00265729706734419, 0.00255904672667384, 0.0024985324125737, 0.00195737043395638, 0.00182289176154882, 0.00111703854054213, 0.00284790969453752, 0.00227619265206158, 0.00194495683535933, 0.00267611443996429, 0.00192635669372976, 0.00169021333567798, 0.00118282169569284, 0.00355296698398888, 0.00530776707455516, 0.001186408335343, 0.0027858919929713, 0.00358430203050375, 0.00298590399324894, 0.00243512890301645, 0.00540193729102612, 0.00233277026563883, 0.00151454040315002, 0.00261470954865217, 0.00163051078561693, 0.00340937147848308, 0.00182314007543027, 0.00147756771184504, 0.00145373959094286, 0.00198079855181277, 0.00251074647530913, 0.00146434491034597, 0.00182895013131201, 0.00294574978761375, 0.00231117405928671, 0.00250368472188711, 0.00336499488912523, 0.00290772970765829, 0.00268372008576989, 0.00217534811235964, 0.00123633933253586, 0.0034761365968734, 0.00145628792233765, 0.00193480134475976, 0.00175254244823009, 0.00283500971272588, 0.00175492640119046, 0.00116036168765277, 0.00293209892697632, 0.00176442146766931, 0.00212917267344892, 0.00159667176194489, 0.00237382878549397, 0.00256310356780887, 0.00173517130315304, 0.0021172424312681, 0.0018380907131359, 0.00177153630647808, 0.00176035519689322, 0.00283262622542679, 0.00262657483108342, 0.00221563572995365, 0.00210648542270064, 0.00200347928330302, 0.0019443950150162, 0.0033197661396116, 0.00255686673335731 };
  static const int16_t buff_info_Conv2D_101_weights_inflated_266_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_112_weights_inflated_268_quant_scale[] = { 0.00206956011243165, 0.00156774919014424, 0.00233227480202913, 0.00308687379583716, 0.00161308073438704, 0.00190992851275951, 0.00111330463550985, 0.00323620368726552, 0.00231573102064431, 0.00324699794873595, 0.00274271704256535, 0.0017993455985561, 0.00126792583614588, 0.00202930625528097, 0.000998942065052688, 0.00236188736744225, 0.00220606848597527, 0.0027290575671941, 0.00184917112346739, 0.00203229929320514, 0.00249498034827411, 0.00296585750766098, 0.00202073203399777, 0.00116733822505921, 0.00270863296464086, 0.00216112402267754, 0.00296575669199228, 0.00277152215130627, 0.00298280571587384, 0.00197017984464765, 0.00291537423618138, 0.00180754193570465, 0.00148903962690383, 0.00226624286733568, 0.001154616009444, 0.00238831923343241, 0.000804199313279241, 0.00251327315345407, 0.00285090808756649, 0.00144506630022079, 0.00152875890489668, 0.00246850750409067, 0.00298731937073171, 0.00367918214760721, 0.00104372762143612, 0.00230761035345495, 0.00229351664893329, 0.00318021862767637, 0.00354895368218422, 0.00212100567296147, 0.00203764415346086, 0.0011856984347105, 0.00307629606686532, 0.00242423289455473, 0.00214808969758451, 0.00241915485821664, 0.00312710949219763, 0.00166670011822134, 0.00198436691425741, 0.00266722240485251, 0.0022253121715039, 0.00253773550502956, 0.00276372022926807, 0.00198849127627909, 0.00276533840224147, 0.00153091829270124, 0.00143561908043921, 0.00157237541861832, 0.000836331862956285, 0.00229009101167321, 0.0016412662807852, 0.00317003671079874, 0.00314962491393089, 0.00293578300625086, 0.00123369006905705, 0.00370906060561538, 0.00263980845920742, 0.00284488662146032, 0.00297451321966946, 0.00263994582928717, 0.00214065285399556, 0.00128715275786817, 0.00259470636956394, 0.00267827836796641, 0.00317745911888778, 0.00209374469704926, 0.00304549327120185, 0.00240866490639746, 0.00264103151857853, 0.00237212516367435, 0.00312252785079181, 0.00273455237038434, 0.00258917477913201, 0.0014004820259288, 0.00154685589950532, 0.00170192180667073, 0.00130993966013193, 0.00115954887587577, 0.0022988140117377, 0.00279764295555651, 0.00099243619479239, 0.00187243055552244, 0.00141144182998687, 0.00350518198683858, 0.00358940684236586, 0.00141861627344042, 0.00154702109284699, 0.00111563643440604, 0.00268113659694791, 0.00121881696395576, 0.00160071859136224, 0.00298677710816264, 0.0013375012204051, 0.000922470004297793, 0.0024348683655262, 0.00266751088202, 0.00387440784834325, 0.00376823730766773, 0.00226229848340154, 0.00266122119501233, 0.00247200787998736, 0.00198458647355437, 0.00288467085920274, 0.00232850783504546, 0.00137512828223407, 0.0018389115575701, 0.00301012932322919, 0.0015185367083177, 0.0014830615837127, 0.00117947440594435, 0.00380465248599648, 0.00238049146719277, 0.00227998103946447, 0.00292191538028419, 0.00281438371166587, 0.00169756286777556, 0.00280198454856873, 0.00293944706209004, 0.00328593235462904, 0.00370153808034956, 0.00162118324078619, 0.00151383818592876, 0.00352349691092968, 0.00241453782655299, 0.00263602728955448, 0.00277312286198139, 0.00302492245100439, 0.00217774510383606, 0.00223685242235661, 0.00260943872854114, 0.00156757666263729, 0.00267546786926687, 0.00246381852775812, 0.00306553533300757, 0.00223974697291851, 0.00263944850303233, 0.00171792891342193, 0.00198730500414968, 0.00218358845449984, 0.00251191179268062, 0.00281993160024285, 0.00262868730351329, 0.00154070334974676, 0.00268869590945542, 0.00151786068454385, 0.00238519441336393, 0.00195467402227223, 0.000799975066911429, 0.00218155584298074, 0.00279021356254816, 0.00292345858179033, 0.00134069018531591, 0.00283331633545458, 0.00265048118308187, 0.00117717788089067, 0.00127302831970155, 0.00170505628921092, 0.00140980561263859, 0.00157602271065116, 0.00182308163493872, 0.00158055021893233, 0.00245427084155381, 0.00329595268703997, 0.00256448215804994, 0.00354638183489442, 0.00240559643134475, 0.00202545570209622, 0.00525286328047514, 0.0027460812125355, 0.00214146333746612, 0.00200099148787558, 0.00113579584285617, 0.0019366416381672, 0.00231474125757813, 0.00213951477780938, 0.00112964317668229, 0.00100839603692293, 0.00155032787006348, 0.00284469989128411, 0.00299025001004338, 0.0011254606069997, 0.00222991174086928, 0.00239159911870956, 0.0018746757414192, 0.00164747412782162, 0.000986427301540971, 0.00238727428950369, 0.00185150047764182, 0.00233538821339607, 0.00227222591638565, 0.00185276055708528, 0.00357106886804104, 0.00283968658186495, 0.00138382252771407, 0.00217570434324443, 0.00305527099408209, 0.00158026104327291, 0.0034601460210979, 0.00475722970440984, 0.00282124779187143, 0.00253531010821462, 0.00234796572476625, 0.00348291941918433, 0.00245252135209739, 0.00141287595033646, 0.0024622417986393, 0.0028269502799958, 0.00278668547980487, 0.00343313300982118, 0.00288337445817888, 0.00112589134369045, 0.00211481540463865, 0.00279282056726515, 0.00129233242478222, 0.00155951897613704, 0.00253689219243824, 0.00286570284515619, 0.00169252150226384, 0.00268171820789576, 0.003037124639377, 0.00284516462124884, 0.00287768011912704, 0.00086325011216104, 0.00106369878631085, 0.00304563692770898, 0.0023243660107255, 0.00307655148208141, 0.0012427014298737, 0.0021523900795728, 0.00292758899740875, 0.00330409128218889, 0.00108402664773166, 0.00299779721535742, 0.00155627960339189, 0.00144798413384706, 0.00179899274371564, 0.00151410955004394, 0.00340224290266633, 0.00337984459474683, 0.00190983328502625, 0.002113499911502, 0.00232806103304029, 0.00242989999242127, 0.00177205028012395, 0.00209256121888757, 0.000860364059917629, 0.00165631889831275, 0.0018378101522103, 0.00108828162774444, 0.00113999284803867, 0.00193957088049501, 0.00268569891341031, 0.00240916782058775, 0.0028378393035382, 0.0027656052261591, 0.00263878400437534, 0.00245165103115141, 0.00323898368515074, 0.00295410631224513, 0.00160111126024276, 0.00250699720345438, 0.00244761048816144, 0.00253595062531531, 0.00153848878107965, 0.00291916029527783, 0.00238965707831085, 0.00255752331577241, 0.00438123289495707, 0.00259715598076582, 0.00210618390701711, 0.000828330928925425, 0.00347342155873775, 0.00286385463550687, 0.00290827755816281, 0.00335584604181349, 0.00181284674908966, 0.00254806946031749, 0.00281592411920428, 0.00193656026385725, 0.00311714340932667, 0.00186926533933729, 0.00117477658204734, 0.00227741152048111, 0.00218493863940239, 0.00258273188956082, 0.00248590623959899, 0.00214125355705619, 0.00179463264066726, 0.000856327766086906, 0.00195419159717858, 0.00243402668274939, 0.0021350544411689, 0.00160697754472494, 0.00356403505429626, 0.00253989198245108, 0.00322582316584885, 0.00104299187660217, 0.00305682164616883, 0.00198288192041218, 0.000923178449738771, 0.0041663683950901, 0.00345169496722519, 0.00200203084386885, 0.00144257175270468, 0.00242800428532064, 0.00221604760736227, 0.00143981329165399, 0.00234140176326036, 0.00215997919440269, 0.00144155498128384, 0.00160724809393287, 0.00311483629047871, 0.00228382856585085, 0.00104796653613448, 0.00217025214806199, 0.00180330756120384, 0.0013574839103967, 0.00321373948827386, 0.0029708375222981, 0.00167079735547304, 0.00212525716051459, 0.00186648988164961, 0.00344060431234539, 0.00247947708703578, 0.0029039029031992, 0.00278111756779253, 0.00217887782491744, 0.00153301167301834, 0.00185154657810926, 0.00140678917523474, 0.00285552674904466, 0.00283458130434155, 0.00284667732194066, 0.00260346289724112, 0.00145064073149115, 0.00283149047754705, 0.00227314536459744, 0.0012820108095184, 0.00179696583654732, 0.00189897883683443, 0.00249621900729835, 0.00184731534682214, 0.000637817720416933, 0.00298173958435655, 0.00252180499956012, 0.00228448584675789, 0.00138399831485003, 0.00217942078597844, 0.00215339218266308, 0.0030253769364208, 0.0018438003025949, 0.0025301028508693, 0.00303394859656692, 0.0019168967846781, 0.00216094870120287, 0.00202472321689129, 0.00322780641727149, 0.00201500719413161, 0.00236420799046755, 0.00208106264472008, 0.00202187732793391, 0.00119308428838849, 0.00161353230942041, 0.00151698861736804, 0.00250006117857993, 0.0028106493409723, 0.0027983586769551, 0.000981535646133125, 0.00177126447670162, 0.00199437164701521, 0.00257365056313574, 0.00280903605744243, 0.0012988387607038, 0.00245929439552128, 0.00303886947222054, 0.00231448351405561, 0.00311081716790795, 0.00268013612367213, 0.00334434420801699, 0.00241871713660657, 0.00154596799984574, 0.00203257892280817, 0.00271762046031654, 0.00290119997225702, 0.00233587925322354, 0.00209735217504203, 0.00255742971785367, 0.00178847799543291, 0.000949794426560402, 0.0021892711520195, 0.00179032876621932, 0.0017618112033233, 0.00228815176524222, 0.00251996633596718, 0.00162267405539751, 0.00253230868838727, 0.00288420845754445, 0.00353080872446299, 0.00374708021990955, 0.00212696404196322, 0.00124957552179694, 0.00278519513085485, 0.00264196400530636, 0.0025535833556205, 0.00287725427187979, 0.00252983928658068, 0.00231549888849258, 0.00303158792667091, 0.00262758764438331, 0.00202515441924334, 0.0023478027433157, 0.00241277297027409, 0.00190719414968044, 0.00130009604617953, 0.00470417551696301, 0.00164591451175511, 0.00276074954308569, 0.00293745007365942, 0.0020165634341538, 0.00210006046108902, 0.00339672155678272, 0.00111413863487542, 0.00261488137766719, 0.00209216633811593, 0.00300897238776088, 0.00179719051811844, 0.00246565346606076, 0.00184903258923441, 0.0031332834623754, 0.00125756685156375, 0.00238609011285007, 0.00277020083740354, 0.00170909066218883, 0.00234321155585349, 0.00286619015969336, 0.00285693653859198, 0.00112088047899306, 0.00322866276837885, 0.0020913437474519, 0.000875979254487902, 0.00344369979575276, 0.00130909297149628, 0.00145995384082198, 0.00148818804882467, 0.0022516124881804, 0.00270896614529192, 0.00251356372609735, 0.00235616066493094, 0.00182112271431834, 0.00127710215747356, 0.00096890825079754, 0.00298336939886212, 0.000829659751616418, 0.00248010898940265, 0.00265582860447466, 0.00295663881115615, 0.00302612106315792, 0.00220537791028619, 0.00273265852592885, 0.00337848369963467, 0.00401082914322615, 0.00194611796177924, 0.0014303638599813, 0.00238443724811077, 0.00283210841007531, 0.00247612572275102, 0.00130005751270801, 0.00245739636011422, 0.00153425452299416, 0.00304871005937457, 0.00239498098380864, 0.000809493474662304, 0.00100150925572962, 0.00178185850381851, 0.00240389560349286, 0.00120564992539585, 0.002608188893646, 0.00221221381798387, 0.00257619214244187, 0.00234011234715581, 0.00277895876206458, 0.00225020945072174, 0.00145197554957122, 0.000993128982372582, 0.000843570043798536, 0.00178721954580396, 0.00243617407977581, 0.000968520122114569, 0.00257072341628373, 0.00240389071404934, 0.00323324487544596, 0.00283971917815506 };
  static const int16_t buff_info_Conv2D_112_weights_inflated_268_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_123_weights_inflated_270_quant_scale[] = { 0.00223983288742602, 0.00140887394081801, 0.000879973697010428, 0.00177893473301083, 0.00096333835972473, 0.00150784349534661, 0.00185280037112534, 0.00422481074929237, 0.0011661461321637, 0.00228901486843824, 0.0021869488991797, 0.00194913090672344, 0.00112170446664095, 0.000680459488648921, 0.00231536221690476, 0.00168438814580441, 0.000989392981864512, 0.00210614083334804, 0.00251940917223692, 0.000874706893227994, 0.00186111347284168, 0.00278575182892382, 0.00232418859377503, 0.00223197089508176, 0.000850966258440167, 0.00203513354063034, 0.00189192977268249, 0.00265817553736269, 0.000902576663065702, 0.00150075182318687, 0.00164383254013956, 0.00220649992115796, 0.00224465900100768, 0.00160524062812328, 0.00232076155953109, 0.00239562639035285, 0.00121475593186915, 0.00296707823872566, 0.0011725650401786, 0.00127590517513454, 0.00240141339600086, 0.00215528765693307, 0.00156869238708168, 0.00251301215030253, 0.0023623863235116, 0.000699978263583034, 0.00166225852444768, 0.00253395945765078, 0.00136794743593782, 0.00409447494894266, 0.00115749647375196, 0.00105052441358566, 0.00146717939060181, 0.00200076936744153, 0.000926456646993756, 0.00196322705596685, 0.00209451513364911, 0.00175908429082483, 0.00256348727270961, 0.00131672574207187, 0.00259712059050798, 0.00118495197966695, 0.00271032564342022, 0.00161139015108347, 0.00144636339973658, 0.00246149231679738, 0.0023690618108958, 0.00212702713906765, 0.00209151534363627, 0.00144766841549426, 0.00276278634555638, 0.00152180262375623, 0.00245815585367382, 0.00141334254294634, 0.000932825438212603, 0.00260564149357378, 0.00214777840301394, 0.00228010001592338, 0.00237180083058774, 0.00141836295370013, 0.00112370355054736, 0.000889157352503389, 0.00240171467885375, 0.00179134646896273, 0.00262254220433533, 0.00259900582022965, 0.000587015063501894, 0.00229826918803155, 0.00119628908578306, 0.000891899166163057, 0.00123427470680326, 0.00242803920991719, 0.00164200866129249, 0.00189041928388178, 0.00229060417041183, 0.0029764084611088, 0.00192324374802411, 0.00285173719748855, 0.00202801264822483, 0.00214960146695375, 0.00198993948288262, 0.00155291683040559, 0.00266808154992759, 0.00233459682203829, 0.00190844410099089, 0.00229575647972524, 0.00232315808534622, 0.00137877208180726, 0.00265028281137347, 0.00173181854188442, 0.00102551409509033, 0.00187725794967264, 0.00193615444004536, 0.00205858028493822, 0.00284988898783922, 0.00242548668757081, 0.00237489654682577, 0.00147973909042776, 0.00220640283077955, 0.000984012731350958, 0.00230228318832815, 0.00212428043596447, 0.00167531450279057, 0.00237123970873654, 0.00162187800742686, 0.00128948828205466, 0.00193195196334273, 0.00221010134555399, 0.00278054247610271, 0.00234648818150163, 0.00146634632255882, 0.00188034388702363, 0.001571019529365, 0.00210926332511008, 0.00318025215528905, 0.00201070751063526, 0.000849026546347886, 0.00196568365208805, 0.000929872854612768, 0.00121638609562069, 0.0017096980009228, 0.00124673335812986, 0.00192933052312583, 0.00197172351181507, 0.0023659139405936, 0.00238174409605563, 0.00182855199091136, 0.00225788936950266, 0.00218519452027977, 0.00280473474413157, 0.00146921130362898, 0.00140867044683546, 0.00122608814854175, 0.00226379535160959, 0.00151888665277511, 0.000766341225244105, 0.00171090418007225, 0.00168972904793918, 0.00217601284384727, 0.00270905788056552, 0.00239742151461542, 0.00210310937836766, 0.00170917622745037, 0.00192462862469256, 0.00231924862600863, 0.00316667696461082, 0.00145431479904801, 0.00143166992347687, 0.00234125810675323, 0.00155498087406158, 0.00180649105459452, 0.00199924153275788, 0.00268612173385918, 0.00156887236516923, 0.00136765534989536, 0.0011365202954039, 0.00202073086984456, 0.00134512421209365, 0.00217822636477649, 0.00104440655559301, 0.00229586660861969, 0.000848265830427408, 0.00112874957267195, 0.00186446553561836, 0.00293189845979214, 0.0021480715367943, 0.00106099853292108, 0.000827330397441983, 0.00170783372595906, 0.00160333758685738, 0.00246744183823466, 0.00192625017371029, 0.00101248547434807, 0.00135554338339716, 0.00185199698898941, 0.000932721712160856, 0.00273335911333561, 0.00170997076202184, 0.00267577334307134, 0.00162901252042502, 0.00150957214646041, 0.00243595358915627, 0.0016403867630288, 0.00283580040559173, 0.000965208979323506, 0.00210170773789287, 0.00140545354224741, 0.00292535591870546, 0.00200178055092692, 0.00128201441839337, 0.000811121019069105, 0.00223226891830564, 0.00231562159024179, 0.00132154277525842, 0.00185321609023958, 0.00198401580564678, 0.00154285319149494, 0.0018619813490659, 0.0016777009004727, 0.00271059875376523, 0.00285548833198845, 0.00141263438854367, 0.00193946121726185, 0.00226346985436976, 0.00210763560608029, 0.00142104097176343, 0.00176715676207095, 0.00216650427319109, 0.00403552642092109, 0.0020446665585041, 0.00297922687605023, 0.00267779640853405, 0.0021892508957535, 0.00260705268010497, 0.00221261498518288, 0.00222915154881775, 0.00101645232643932, 0.00262879207730293, 0.00286258058622479, 0.00189224863424897, 0.00151055597234517, 0.0011532319476828, 0.00183619640301913, 0.00115719519089907, 0.00117811001837254, 0.00257374066859484, 0.00220203609205782, 0.00128624029457569, 0.00277815037406981, 0.00194309209473431, 0.00189410976599902, 0.00236614374443889, 0.00131499022245407, 0.00146383896935731, 0.00241496367380023, 0.00106075243093073, 0.00118338025640696, 0.0016535067697987, 0.00171392550691962, 0.000991567969322205, 0.00144361204002053, 0.00306653254665434, 0.00239058281295002, 0.00226424285210669, 0.00148525240365416, 0.00252941157668829, 0.00127000128850341, 0.00197116099298, 0.00232157297432423, 0.00171111419331282, 0.00214139558374882, 0.0017666359199211, 0.00309505849145353, 0.0018664023373276, 0.00142481084913015, 0.00172696029767394, 0.00186377973295748, 0.00227550277486444, 0.00252816267311573, 0.00285222148522735, 0.00225223344750702, 0.00129091332200915, 0.00196933094412088, 0.00255034351721406, 0.00199508550576866, 0.0011667791986838, 0.00174276193138212, 0.00167660298757255, 0.0019063544459641, 0.00260884803719819, 0.00257926038466394, 0.00290206959471107, 0.00203029904514551, 0.00157722446601838, 0.00240731611847878, 0.00286596012301743, 0.00102804705966264, 0.00145834079012275, 0.00181493500713259, 0.00203408976085484, 0.00176326592918485, 0.00155877147335559, 0.000828063581138849, 0.00279860571026802, 0.00171065726317465, 0.00256279273889959, 0.000861497304867953, 0.00187408935744315, 0.00213688798248768, 0.00267148483544588, 0.000771746854297817, 0.00202789762988687, 0.0024800777900964, 0.00218903506174684, 0.00257517443969846, 0.0013149012811482, 0.00254196091555059, 0.00191929575521499, 0.00198795832693577, 0.00218024337664247, 0.00103277713060379, 0.00181144999805838, 0.00310442503541708, 0.00213613850064576, 0.0025659806560725, 0.00149688648525625, 0.00218703737482429, 0.00188216008245945, 0.00176719320006669, 0.0017024694243446, 0.00185753649566323, 0.00230158772319555, 0.00223237415775657, 0.00260342541150749, 0.00217053876258433, 0.00155767914839089, 0.00219331192784011, 0.00161806598771363, 0.00151474308222532, 0.00133208907209337, 0.0015623337822035, 0.00178738730028272, 0.0018452707445249, 0.000858505605719984, 0.00167250330559909, 0.00216417596675456, 0.00189526355825365, 0.0024612876586616, 0.000996510148979723, 0.0015136303845793, 0.00137333618476987, 0.00211729411967099, 0.00199832860380411, 0.00140018749516457, 0.00163485133089125, 0.00210957042872906, 0.00170316337607801, 0.00216170097701252, 0.00195544492453337, 0.00127771880943328, 0.00156854325905442, 0.00245854095555842, 0.00218873820267618, 0.00190936122089624, 0.00224345549941063, 0.00150537514127791, 0.00153947493527085, 0.00181983294896781, 0.00105178356170654, 0.00140543060842901, 0.00132399145513773, 0.00194954115431756, 0.0010881134076044, 0.00244009867310524, 0.00140290451236069, 0.000982365920208395, 0.00263188523240387, 0.0016651350306347, 0.00153549888636917, 0.00186927651520818, 0.00278913229703903, 0.00337921408936381, 0.00207578507252038, 0.00187314639333636, 0.00168976769782603, 0.00137586344499141, 0.00106695806607604, 0.00174167694058269, 0.00115873548202217, 0.00175161426886916, 0.00136014434974641, 0.00191674730740488, 0.000848860712721944, 0.00346496305428445, 0.0017559768166393, 0.00216832989826798, 0.00290402676910162, 0.00171361269894987, 0.00163334852550179, 0.00109012972097844, 0.0030567164067179, 0.00185513391625136, 0.00183651875704527, 0.0013586429413408, 0.00120213825721294, 0.00228491309098899, 0.00211832066997886, 0.00106704619247466, 0.00247590849176049, 0.00181468471419066, 0.00146168610081077, 0.00252269674092531, 0.00128960120491683, 0.00243262550793588, 0.00245913700200617, 0.00234308792278171, 0.00103593617677689, 0.00194676232058555, 0.00188138068187982, 0.00225224788300693, 0.00247711571864784, 0.00129947438836098, 0.00156108220107853, 0.0024574319832027, 0.00219820928759873, 0.00299011962488294, 0.00275062560103834, 0.00181003648322076, 0.00158710556570441, 0.000630809518042952, 0.00192867906298488, 0.00192796415649354, 0.00154472899157554, 0.00178026291541755, 0.0020394183229655, 0.00188103097025305, 0.00248012971132994, 0.00281158555299044, 0.00230515515431762, 0.00215327087789774, 0.00231437548063695, 0.0022858704905957, 0.00282990280538797, 0.00205437210388482, 0.00180031568743289, 0.00234146253205836, 0.00247908476740122, 0.00173337059095502, 0.00173126952722669, 0.00258034118451178, 0.00231340317986906, 0.00236298679374158, 0.00156988331582397, 0.00205543870106339, 0.00255337753333151, 0.00314761814661324, 0.0020665256306529, 0.00325445760972798, 0.00133206427562982, 0.00157917372416705, 0.00290181743912399, 0.0028180624358356, 0.000748603080864996, 0.0017871914897114, 0.00185239315032959, 0.00211601960472763, 0.00152316328603774, 0.00230154441669583, 0.00173072400502861, 0.00206268858164549, 0.0016133354511112, 0.00100004649721086, 0.00187604851089418, 0.0022830085363239, 0.0014799649361521, 0.00262392265722156, 0.00234850565902889, 0.000882139545865357, 0.00268456735648215, 0.00231240084394813, 0.000869620649609715, 0.00124363624490798, 0.0029846194665879, 0.00156776595395058, 0.00209477101452649, 0.00258947326801717, 0.00256831967271864, 0.00160781282465905, 0.00256439857184887, 0.00160169892478734, 0.00177291920408607, 0.00174102885648608, 0.00199930043891072, 0.0022299624979496, 0.00240788678638637, 0.00265089562162757, 0.00158866005949676, 0.00278021092526615, 0.00222654058597982, 0.00224113347940147, 0.00289011071436107, 0.00264122593216598, 0.00165484368335456, 0.00103661662433296, 0.00203714310191572, 0.00148755265399814, 0.00102861621417105, 0.00220603216439486, 0.00210218271240592, 0.00129115779418498, 0.00126120494678617, 0.00120718684047461 };
  static const int16_t buff_info_Conv2D_123_weights_inflated_270_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const float buff_info_Conv2D_134_weights_inflated_272_quant_scale[] = { 0.00113815732765943, 0.00143835402559489, 0.0017128448234871, 0.00160153198521584, 0.00171113363467157, 0.000769846665207297, 0.00129944481886923, 0.00108077737968415, 0.00174705754034221, 0.00104159920010716, 0.0010863624047488, 0.00124352134298533, 0.0012377155944705, 0.00109018548391759, 0.0014109939802438, 0.00126632500905544, 0.00123559683561325, 0.0016478574834764, 0.00136289186775684, 0.00153654825408012, 0.00122680026106536, 0.00101726653520018, 0.00148930959403515, 0.0013895824085921, 0.0012128195958212, 0.00139495264738798, 0.00103468669112772, 0.00129188259597868, 0.00137111358344555, 0.00127602263819426, 0.00129540078341961, 0.00123689800966531, 0.00164728180970997, 0.00143191730603576, 0.00125662679784, 0.00154665263835341, 0.00132014660630375, 0.00150361750274897, 0.00135582790244371, 0.00137095514219254, 0.00126895483117551, 0.00104318873491138, 0.0013030250556767, 0.00137824984267354, 0.00101288408041, 0.00125026854220778, 0.00182773522101343, 0.00151732319500297, 0.0013568711001426, 0.00146309938281775, 0.00131261628121138, 0.00122279091738164, 0.00142407161183655, 0.00127802381757647, 0.00148955103941262, 0.00136859191115946, 0.000955642724875361, 0.00126046850346029, 0.00135995051823556, 0.00122272141743451, 0.00131858186796308, 0.00116343353874981, 0.00151574693154544, 0.000881487445440143, 0.00138163683004677, 0.00115128699690104, 0.00156211119610816, 0.000944311439525336, 0.00145719316788018, 0.00120061310008168, 0.00126568891573697, 0.00168633402790874, 0.0011933739297092, 0.00152995577082038, 0.00120240938849747, 0.00128866184968501, 0.00153022329322994, 0.00123157259076834, 0.00135978392791003, 0.00138783000875264, 0.00110599107574672, 0.00149871781468391, 0.001123160822317, 0.00114931957796216, 0.00157719559501857, 0.00129911967087537, 0.00109976821113378, 0.00138494907878339, 0.0013768570497632, 0.00155215489212424, 0.00110884383320808, 0.0011631689267233, 0.00141675898339599, 0.00126271334011108, 0.00128954497631639, 0.00148279592394829, 0.00132411299273372, 0.00127924990374595, 0.00153316371142864, 0.00172635656781495, 0.00161360495258123, 0.00112524873111397, 0.00157721887808293, 0.00139229488559067, 0.00152822537347674, 0.00163899385370314, 0.00133727584034204, 0.00138614815659821, 0.00124180980492383, 0.00153604079969227, 0.00124692765530199, 0.00163389171939343, 0.00141530344262719, 0.00122715206816792, 0.00144507153891027, 0.00131036294624209, 0.0013707154430449, 0.00145680690184236, 0.00118255068082362, 0.00116190779954195, 0.00130192725919187, 0.00150484393816441, 0.00125710910651833, 0.00133299198932946, 0.00134783692192286, 0.00116168812382966, 0.00142592890188098, 0.00116667302791029, 0.00111223931889981, 0.00120706867892295, 0.00136770727112889, 0.00125977711286396, 0.00130549899768084, 0.00132954854052514, 0.00144214520696551, 0.00135900627356023, 0.00136120989918709, 0.00128198182210326, 0.00108176190406084, 0.00124163052532822, 0.00128214352298528, 0.00126566353719682, 0.00125198974274099, 0.00134275248274207, 0.00136495393235236, 0.00138745713047683, 0.00134224572684616, 0.00115972210187465, 0.00132069666869938, 0.00139621610287577, 0.00146630348172039, 0.00137167191132903, 0.0013638666132465, 0.00147451355587691, 0.00155990978237242, 0.00152376829646528, 0.0014153509400785, 0.00156484369654208, 0.00135403219610453, 0.00144438585266471, 0.00106895051430911, 0.00140015163924545, 0.00132231414318085, 0.00126123067457229, 0.00151578953955323, 0.00141057383734733, 0.00124259642325342, 0.00139568408485502, 0.000999446609057486, 0.00136610143817961, 0.00118339201435447, 0.00143262022174895, 0.00132863176986575, 0.00136496045161039, 0.00121629005298018, 0.0014706258662045, 0.00128953543026, 0.00115285604260862, 0.000979180447757244, 0.00117490894626826, 0.00111152720637619, 0.00150111329276115, 0.00122948689386249, 0.0013324631145224, 0.00163256353698671, 0.00112985819578171, 0.00115531089249998, 0.00130204705055803, 0.00122838874813169, 0.00153151107951999, 0.00135350460186601, 0.00135906389914453, 0.00121677049901336, 0.00161871989257634, 0.00111139577347785, 0.00139303784817457, 0.00099011417478323, 0.00131146039348096, 0.00150727457366884, 0.00123022322077304, 0.00115093716885895, 0.00111223571002483, 0.00129812373779714, 0.00131489790510386, 0.00138229131698608, 0.00165132735855877, 0.00142921099904925, 0.00126986973918974, 0.00145586929284036, 0.00146325281821191, 0.00130366231314838, 0.00125862599816173, 0.00146718847099692, 0.00115048908628523, 0.00155125616583973, 0.00107684463728219, 0.0016278208931908, 0.001046177232638, 0.00142508361022919, 0.00139048765413463, 0.00138854212127626, 0.00107888551428914, 0.00114430778194219, 0.00117163686081767, 0.00118096545338631, 0.00112859613727778, 0.00140076014213264, 0.00155934190843254, 0.00127256789710373, 0.00137817091308534, 0.00134075805544853, 0.00149894668720663, 0.000938903074711561, 0.00126246735453606, 0.00126186630222946, 0.00152944622095674, 0.00129967648535967, 0.00148866034578532, 0.00166126957628876, 0.00149692117702216, 0.00129306560847908, 0.00123054883442819, 0.00159305438864976, 0.00148714787792414, 0.00114928011316806, 0.001284014666453, 0.00140360207296908, 0.0013509604614228, 0.00121113087516278, 0.00146584550384432, 0.00131616822909564, 0.00132073869463056, 0.00134804472327232, 0.00165342830587178, 0.00144987995736301, 0.00142901833169162, 0.00110434833914042, 0.00116314017213881, 0.00128718768246472, 0.00150736013893038, 0.00150999368634075, 0.00122941075824201, 0.00146043999120593, 0.00143593095708638, 0.00108238938264549, 0.00133572577033192, 0.0015069890068844, 0.000858310668263584, 0.00137211685068905, 0.00123715738300234, 0.00123684480786324, 0.00142671831417829, 0.00136088998988271, 0.00155984063167125, 0.0012525636702776, 0.00136874162126333, 0.00136343541089445, 0.00123704958241433, 0.00113748875446618, 0.00154236098751426, 0.00105394143611193, 0.00119216542225331, 0.00149487459566444, 0.00121800205670297, 0.00112924259155989, 0.00100309355184436, 0.0015672518638894, 0.00174475042149425, 0.00134075956884772, 0.00156329479068518, 0.00134013453498483, 0.00100014242343605, 0.00160743063315749, 0.00117306155152619, 0.000803744012955576, 0.00127849914133549, 0.00162527663633227, 0.00135484093334526, 0.00132597121410072, 0.00140730140265077, 0.00116031977813691, 0.000972702109720558, 0.00143817416392267, 0.00103933829814196, 0.00161505315918475, 0.00139359361492097, 0.00137640908360481, 0.00127367919776589, 0.00136954465415329, 0.00146825483534485, 0.00129129597917199, 0.00142169848550111, 0.00152116804383695, 0.00153267453424633, 0.00146223697811365, 0.00172953028231859, 0.00156227103434503, 0.0016050353879109, 0.00130263320170343, 0.00143149262294173, 0.00151715730316937, 0.00146970548667014, 0.00129482836928219, 0.00143107958137989, 0.00127823557704687, 0.00131157145369798, 0.00115684734191746, 0.00118369096890092, 0.00136834837030619, 0.00155433849431574, 0.00133626512251794, 0.00134915707167238, 0.00132205837871879, 0.00107090314850211, 0.00133468012791127, 0.00140733923763037, 0.00104502122849226, 0.00132679904345423, 0.00147467153146863, 0.00124941428657621, 0.00125044188462198, 0.00131226051598787, 0.00167733943089843, 0.00167866982519627, 0.00179414253216237, 0.00124483928084373, 0.00132354779634625, 0.00108172034379095, 0.00122995942365378, 0.00103953550569713, 0.00115326012019068, 0.0012859144480899, 0.00132862105965614, 0.00109447748400271, 0.00128737394697964, 0.00136478617787361, 0.00145329756196588, 0.00113367079757154, 0.0013729517813772, 0.0015173387946561, 0.00142449629493058, 0.00139549269806594, 0.00140184164047241, 0.00116651388816535, 0.00129073520656675, 0.00150829355698079, 0.00105723121669143, 0.00121133914217353, 0.00130752613767982, 0.00133295450359583, 0.00132854713592678, 0.00131045968737453, 0.00141520495526493, 0.00108415086288005, 0.00143334828317165, 0.00135032250545919, 0.00141185487154871, 0.00149279867764562, 0.00139540573582053, 0.000936859229113907, 0.00118232576642185, 0.000944224535487592, 0.00144355476368219, 0.0016922194045037, 0.00157448346726596, 0.00105500128120184, 0.00122986536007375, 0.00107698049396276, 0.00130122376140207, 0.00153240852523595, 0.00154248520266265, 0.0010935451136902, 0.00145363074261695, 0.00139562273398042, 0.00120318471454084, 0.00129843922331929, 0.00152017979416996, 0.00136309524532408, 0.00126595096662641, 0.00117305072490126, 0.00120918767061085, 0.00140772946178913, 0.00145288417115808, 0.00134085537865758, 0.00151881610509008, 0.00128193537238985, 0.00157479452900589, 0.000999833922833204, 0.00112510670442134, 0.00107295624911785, 0.00109916110523045, 0.00120864016935229, 0.000872981792781502, 0.00115964992437512, 0.00131497182883322, 0.00125906465109438, 0.00137285178061575, 0.00122388068120927, 0.00139475311152637, 0.00122520071454346, 0.0013385993661359, 0.00107853265944868, 0.00126712687779218, 0.00162477884441614, 0.00107643322553486, 0.00136478804051876, 0.00131622096523643, 0.0012802203418687, 0.00128737313207239, 0.00120238680392504, 0.00156645500101149, 0.00136191304773092, 0.00100929965265095, 0.00129463442135602, 0.00122096412815154, 0.00124550994951278, 0.00148263643495739, 0.00180441827978939, 0.00133259024005383, 0.00167900056112558, 0.0012723597465083, 0.0012382841669023, 0.00101767329033464, 0.00121057976502925, 0.00140382768586278, 0.00127028417773545, 0.00123127060942352, 0.00144829996861517, 0.00118828646373004, 0.00141719728708267, 0.00158127944450825, 0.00164081086404622, 0.0013200236717239, 0.00153397780377418, 0.00141656643245369, 0.00139062874950469, 0.00118069315794855, 0.00125046400353312, 0.00126481219194829, 0.00139672961086035, 0.00135398330166936, 0.00139838084578514, 0.00116450118366629, 0.0011438624933362, 0.00125028379261494, 0.00133908679708838, 0.00120575760956854, 0.00109346967656165, 0.00126710836775601, 0.00100478332024068, 0.0013806524220854, 0.00134378578513861, 0.00137445761356503, 0.00114458333700895, 0.00146082160063088, 0.00130535045173019, 0.00149699649773538, 0.00135082751512527, 0.00140177889261395, 0.00151566928252578, 0.00115023425314575, 0.00150562892667949, 0.00101009430363774, 0.00139384914655238, 0.00153407815378159, 0.00130946177523583, 0.00113067624624819, 0.00131714600138366, 0.0014093522913754, 0.0013760143192485, 0.00134695135056973, 0.00104447931516916, 0.00142891425639391, 0.00136213365476578, 0.00131200021132827, 0.00169892830308527, 0.00167416676413268, 0.00101555837318301, 0.00141738879028708, 0.00168189068790525, 0.00157495157327503, 0.00134498788975179, 0.00142886955291033, 0.00115589785855263, 0.00145043095108122, 0.00142689840868115, 0.00135816133115441, 0.00111078727059066, 0.000874979072250426, 0.00107565685175359, 0.00129310064949095, 0.00131023093126714 };
  static const int16_t buff_info_Conv2D_134_weights_inflated_272_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_1024_8_3_3[] = { 1024, 3, 3, 8 };
  static const uint32_t buff_info__mem_shape_L_1024_8_3_3[] = { 1024, 3, 3, 8 };
  static const float buff_info_Conv2D_145_weights_inflated_274_quant_scale[] = { 0.00114799092989415, 0.000765500590205193, 0.00149209063965827, 0.000962051912210882, 0.00171852554194629, 0.0012115971185267, 0.00150486151687801, 0.00145488639827818, 0.00086747802561149, 0.00121192540973425, 0.00177986558992416, 0.00156285800039768, 0.00124681950546801, 0.000741616589948535, 0.000827265612315387, 0.00149155396502465, 0.000893277698196471, 0.0016675564693287, 0.00104235974140465, 0.00125898059923202, 0.00159133435226977, 0.00114665424916893, 0.00185910367872566, 0.00198191683739424, 0.000826854491606355, 0.000740138988476247, 0.00146223860792816, 0.00162822543643415, 0.00184740917757154, 0.00125744927208871, 0.00184966693632305, 0.00107604684308171, 0.000578093226067722, 0.00109202635940164, 0.00156287825666368, 0.000782421266194433, 0.00172918406315148, 0.00128855486400425, 0.00169340299908072, 0.000749174854718149, 0.00171513494569808, 0.00100338482297957, 0.000868149509187788, 0.00184938951861113, 0.000930849637370557, 0.00113076309207827, 0.00119032606016845, 0.000845694565214217, 0.00171403493732214, 0.00160624855197966, 0.00164037104696035, 0.0013841912150383, 0.00123260426335037, 0.00113950041122735, 0.000846720184199512, 0.00143040658440441, 0.000906129891518503, 0.00141036487184465, 0.00176217744592577, 0.000833795114886016, 0.00125717418268323, 0.00118062878027558, 0.00145374378189445, 0.00097321585053578, 0.00201310031116009, 0.00161391356959939, 0.00116451492067426, 0.00148606626316905, 0.00177073350641876, 0.00157545390538871, 0.00184946728404611, 0.00154304166790098, 0.000627064029686153, 0.000931159709580243, 0.00105237099342048, 0.000912660791072994, 0.0017308727838099, 0.0016544972313568, 0.00137210660614073, 0.00143216759897768, 0.00126773095689714, 0.000824043119791895, 0.00133643497247249, 0.00116458989214152, 0.001127022318542, 0.000961004523560405, 0.000866688846144825, 0.001470553339459, 0.00116767385043204, 0.00137615704443306, 0.00189017434604466, 0.000914502248633653, 0.000502181064803153, 0.0018483349122107, 0.00161433359608054, 0.000973449321463704, 0.00139280396979302, 0.00155817694030702, 0.00135713524650782, 0.000924313964787871, 0.00138946867082268, 0.00105685333255678, 0.00180786533746868, 0.000697351526468992, 0.00099609512835741, 0.00202725338749588, 0.00157433084677905, 0.00154632073827088, 0.00112460891250521, 0.00136859086342156, 0.00143015256617218, 0.00113029452040792, 0.001059933565557, 0.000934650306589901, 0.00134409044403583, 0.00170512718614191, 0.00124296674039215, 0.00131382723338902, 0.00187306373845786, 0.00145399011671543, 0.000846599112264812, 0.00131109927315265, 0.00134442618582398, 0.00155590428039432, 0.0015324738342315, 0.00176249165087938, 0.00156353262718767, 0.00160568731371313, 0.00106765504460782, 0.00126354349777102, 0.000803195987828076, 0.00104714173357934, 0.00180531886871904, 0.00140831072349101, 0.00138681929092854, 0.00134581699967384, 0.00156936817802489, 0.00121304241474718, 0.0016563058597967, 0.00114918209146708, 0.00100512977223843, 0.00114468904212117, 0.00111000810284168, 0.00168247986584902, 0.00107638724148273, 0.00110886665061116, 0.00115443568211049, 0.00093244737945497, 0.00177054281812161, 0.00153413345105946, 0.00117810489609838, 0.000742687960155308, 0.000907773210201412, 0.00181784352753311, 0.00137774622999132, 0.000735236913897097, 0.00163643795531243, 0.001663240022026, 0.0013213399797678, 0.00115720857866108, 0.00160884810611606, 0.0016085107345134, 0.00109910487663001, 0.00190610927529633, 0.00129995809402317, 0.00130559678655118, 0.00109526247251779, 0.00094668148085475, 0.00183067854959518, 0.000591149495448917, 0.00114791980013251, 0.0011779764899984, 0.000833194411825389, 0.00114570918958634, 0.00168769108131528, 0.00091799785150215, 0.0014843475073576, 0.000731636129785329, 0.00109732733108103, 0.00148750678636134, 0.000822622911073267, 0.0016180215170607, 0.0017202477902174, 0.0016477620229125, 0.00167582393623888, 0.00142538524232805, 0.00179765780922025, 0.000721259799320251, 0.00113377650268376, 0.00179542659316212, 0.000985637889243662, 0.00171910901553929, 0.000951664871536195, 0.00147845770698041, 0.000827721320092678, 0.00130092666950077, 0.00086760992417112, 0.00137483677826822, 0.00184906262438744, 0.00159519293811172, 0.00160822388716042, 0.00162261363584548, 0.00115945294965059, 0.00173484918195754, 0.00160883937496692, 0.00116881658323109, 0.000866265967488289, 0.00132800696883351, 0.00128049275372177, 0.000982199562713504, 0.00102878548204899, 0.000882917898707092, 0.000905616790987551, 0.00110131443943828, 0.00131817033980042, 0.00105325889308006, 0.00176338560413569, 0.000764907628763467, 0.000951815396547318, 0.00114691432099789, 0.000858680519741029, 0.000877513142768294, 0.000826891337055713, 0.0012724221451208, 0.00164595944806933, 0.000994657981209457, 0.00138752430211753, 0.0014612297527492, 0.00170748645905405, 0.00088188739027828, 0.00125593156553805, 0.000842295179609209, 0.00174123211763799, 0.00151166541036218, 0.000900010869372636, 0.0013169243466109, 0.00130970485042781, 0.00135201937519014, 0.00130389363039285, 0.000605880049988627, 0.00144784047733992, 0.00146689172834158, 0.00176941603422165, 0.000656422227621078, 0.00138838845305145, 0.000911523588001728, 0.00109513860661536, 0.00161595898680389, 0.00191996421199292, 0.00153138337191194, 0.00166019843891263, 0.00160327949561179, 0.000778641435317695, 0.0011296181473881, 0.00100297573953867, 0.000634737138170749, 0.00170964747667313, 0.00121713848784566, 0.00113671377766877, 0.00165099208243191, 0.000815223902463913, 0.00137207447551191, 0.00175870081875473, 0.00206440314650536, 0.00147737853694707, 0.00156074354890734, 0.0014093155041337, 0.00161677470896393, 0.00111056049354374, 0.00131162849720567, 0.00101783918216825, 0.000859978143125772, 0.00109805003739893, 0.000824577931780368, 0.00171683495864272, 0.0011106546735391, 0.00164569681510329, 0.000932950992137194, 0.000878429389558733, 0.000837219122331589, 0.0013968936400488, 0.00120724167209119, 0.000903761363588274, 0.0013623577542603, 0.000973417714703828, 0.00101321691181511, 0.000945584266446531, 0.000879730854649097, 0.00145304843317717, 0.00133840413764119, 0.00158912630286068, 0.000855452206451446, 0.00170384801458567, 0.00135714490897954, 0.00177979713771492, 0.00157178251538426, 0.00116080755833536, 0.0014065601862967, 0.00162852765060961, 0.00135465722996742, 0.00173917575739324, 0.00137887254822999, 0.00184668332803994, 0.00165725266560912, 0.0014357091858983, 0.00156565243378282, 0.00141783163417131, 0.0018219401827082, 0.000965188373811543, 0.00159510597586632, 0.00146268145181239, 0.00149265269283205, 0.00149918696843088, 0.0010892212158069, 0.00188030558638275, 0.00142119312658906, 0.00123267935123295, 0.00125259254127741, 0.00164131098426878, 0.00171746232081205, 0.00147411215584725, 0.00184788357000798, 0.00159528292715549, 0.00130170490592718, 0.000724512035958469, 0.000997482100501657, 0.0014545040903613, 0.00144084275234491, 0.00122186739463359, 0.00158556690439582, 0.00127129419706762, 0.00182055577170104, 0.00134684809017926, 0.00116844335570931, 0.00127844698727131, 0.00113672134466469, 0.00138008885551244, 0.00088223087368533, 0.00156767631415278, 0.00148185377474874, 0.00172200181987137, 0.00147808960173279, 0.00152366259135306, 0.00159029150381684, 0.00146771664731205, 0.00167813361622393, 0.001675418112427, 0.00162769190501422, 0.00154778535943478, 0.00154145190026611, 0.00112169759813696, 0.000641853897832334, 0.00107589783146977, 0.00124553160276264, 0.000894940749276429, 0.0014993044314906, 0.00119842903222889, 0.00166261859703809, 0.00108122231904417, 0.00138109584804624, 0.00122962531168014, 0.000856703263707459, 0.00116632063873112, 0.00110491889063269, 0.00154483143705875, 0.00164497166406363, 0.000629864807706326, 0.00182814570143819, 0.0007303292513825, 0.00122944475151598, 0.00148298777639866, 0.000762874260544777, 0.00152488611638546, 0.000991747016087174, 0.00174797500949353, 0.00127577758394182, 0.00108431419357657, 0.00183544296305627, 0.00118522741831839, 0.00127314927522093, 0.000750155828427523, 0.00112146337050945, 0.000749882601667196, 0.0012283124960959, 0.000768578553106636, 0.00109923467971385, 0.000933334929868579, 0.00148117740172893, 0.00158516876399517, 0.000615452881902456, 0.00106100703123957, 0.00177766510751098, 0.00107990473043174, 0.00166193826589733, 0.00108145957347006, 0.00105263374280185, 0.00166263245046139, 0.00174763600807637, 0.00199497258290648, 0.000923073326703161, 0.00201035616919398, 0.00103593873791397, 0.00114196795038879, 0.00170012738090008, 0.00101459852885455, 0.00152230355888605, 0.0015926860505715, 0.000745594326872379, 0.00187689857557416, 0.000918278645258397, 0.00135034846607596, 0.0014396938495338, 0.00113826675806195, 0.00159348314628005, 0.00159654940944165, 0.0016813543625176, 0.00184310239274055, 0.000988033250905573, 0.00112638983409852, 0.00127051724120975, 0.00112116162199527, 0.0016823640326038, 0.00145533820614219, 0.00103580742143095, 0.00164775550365448, 0.00090302067110315, 0.000593273667618632, 0.000780304661020637, 0.00085168716032058, 0.00166884576901793, 0.000862543354742229, 0.00166324246674776, 0.000629800138995051, 0.000699522730428725, 0.00126179947983474, 0.000960997072979808, 0.00167527026496828, 0.00179137720260769, 0.00112638133578002, 0.00148475484456867, 0.00117753376252949, 0.000990806729532778, 0.0019084265222773, 0.00138577015604824, 0.00190280273091048, 0.00168986094649881, 0.00135504466015846, 0.00192444061394781, 0.00155802210792899, 0.00149352208245546, 0.00165814638603479, 0.00128386984579265, 0.00187466572970152, 0.00134617032017559, 0.00153797387611121, 0.00162512331735343, 0.00161242892500013, 0.00185921660158783, 0.000838471867609769, 0.000741715368349105, 0.00119648745749146, 0.0017180658178404, 0.00114445516373962, 0.00188489584252238, 0.000845237052999437, 0.00171334855258465, 0.0008909702883102, 0.00150338397361338, 0.00123699801042676, 0.00098970765247941, 0.0015089032240212, 0.00113237567711622, 0.00135200656950474, 0.00148425181396306, 0.00093216315144673, 0.000757561414502561, 0.00152560917194933, 0.00175073719583452, 0.00147392938379198, 0.00124474801123142, 0.0015080738812685, 0.00108245655428618, 0.00110947946086526, 0.00194958550855517, 0.00153283157851547, 0.00157889898400754, 0.000876062549650669, 0.000773051579017192, 0.00109013577457517, 0.00158492173068225, 0.00145029625855386, 0.00173321203328669, 0.00115847296547145, 0.00137625366915017, 0.00174798653461039, 0.00141053192783147, 0.0013610462192446, 0.00145542703103274, 0.000887501810211688, 0.00166196131613106, 0.00122122664470226, 0.00151690014172345, 0.00161330262199044, 0.00148279999848455, 0.00176789774559438, 0.00162365974392742, 0.00167747959494591, 0.00173301692120731, 0.00130821042694151, 0.00131340348161757, 0.00133988203015178, 0.00104682671371847, 0.00180690595880151, 0.00137555703986436, 0.00137485808227211, 0.00163573760073632, 0.00206342432647943, 0.000607938331086189, 0.00169030064716935, 0.00165674067102373, 0.00126957078464329, 0.00135497062001377, 0.00167106534354389, 0.000868472910951823, 0.00159611902199686, 0.000774429470766336, 0.00102457380853593, 0.00102543167304248, 0.00150296662468463, 0.00101126346271485, 0.000829426280688494, 0.00126361579168588, 0.000886111112777144, 0.00141364964656532, 0.00155684724450111, 0.00144813011866063, 0.0011793072335422, 0.000830202887300402, 0.00117301836144179, 0.000844467198476195, 0.00133728166110814, 0.00137028901372105, 0.0015538475709036, 0.0016093454323709, 0.00135762663558125, 0.00174509489443153, 0.00146217946894467, 0.00158454326447099, 0.000918555713724345, 0.00156015402171761, 0.00140066363383085, 0.00185429293196648, 0.00139997585210949, 0.00111342244781554, 0.00138836831320077, 0.000861479667946696, 0.001580371404998, 0.00134307518601418, 0.00113068264909089, 0.00144511146936566, 0.00133028347045183, 0.00118302146438509, 0.00167313090059906, 0.00127557350788265, 0.00113891367800534, 0.0016293223015964, 0.00117301184218377, 0.00153760937973857, 0.00149145373143256, 0.0011560347629711, 0.00134571688249707, 0.00142364273779094, 0.00159985921345651, 0.00135265628341585, 0.00163075979799032, 0.00154028821270913, 0.000704237027093768, 0.00143073999788612, 0.00132679077796638, 0.00116931961383671, 0.00208623195067048, 0.00171509804204106, 0.000909883878193796, 0.00165332586038858, 0.00183556589763612, 0.00142758316360414, 0.000655318202916533, 0.00126135488972068, 0.00157488358672708, 0.00114956055767834, 0.00199127709493041, 0.00175794423557818, 0.00103199121076614, 0.00180056248791516, 0.000952106667682528, 0.00132383650634438, 0.00116325949784368, 0.00105989852454513, 0.00178689497988671, 0.00110820913687348, 0.00117006746586412, 0.00133209745399654, 0.00135132018476725, 0.00162026658654213, 0.00108358054421842, 0.00139217579271644, 0.00199542194604874, 0.00121031003072858, 0.00162586313672364, 0.00107697781641036, 0.00156058289576322, 0.00143277610186487, 0.00164513161871582, 0.00157706474419683, 0.00106496899388731, 0.0011553232325241, 0.00137601490132511, 0.000659096869640052, 0.00168175809085369, 0.000982843106612563, 0.00186327041592449, 0.000972052628640085, 0.00169473036658019, 0.000737090012989938, 0.00146479241084307, 0.00117382220923901, 0.00179496523924172, 0.00213583698496222, 0.00160787836648524, 0.000843065499793738, 0.000731684849597514, 0.0009575339499861, 0.00199398119002581, 0.00112067349255085, 0.00129753607325256, 0.00117613410111517, 0.00155928696040064, 0.00145238288678229, 0.00131408823654056, 0.00121151970233768, 0.00134040706325322, 0.00161020667292178, 0.00138068897649646, 0.00184540636837482, 0.00153538270387799, 0.00136913673486561, 0.00174279895145446, 0.00133473833557218, 0.00137463957071304, 0.00133739330340177, 0.0010760739678517, 0.000663051090668887, 0.0017894059419632, 0.00101388606708497, 0.000804936396889389, 0.000842700945213437, 0.00124163285363466, 0.00164497282821685, 0.00140805612318218, 0.000828451476991177, 0.00144005252514035, 0.00114570232108235, 0.00085819698870182, 0.00146296271122992, 0.00139485497493297, 0.000828080344945192, 0.00095191557193175, 0.000912131683435291, 0.000848114956170321, 0.00129910674877465, 0.00163770571816713, 0.000993407680653036, 0.00162932300008833, 0.00170544674620032, 0.00115454534534365, 0.00143682793714106, 0.00172761280555278, 0.00184172473382205, 0.00166896695736796, 0.000771645514760166, 0.000994028407149017, 0.00182800088077784, 0.00114602583926171, 0.00109763292130083, 0.00120939954649657, 0.00104230968281627, 0.000706155900843441, 0.00102095364127308, 0.00137496658135206, 0.00209024478681386, 0.00176349561661482, 0.000931121467147022, 0.00148468255065382, 0.00199322635307908, 0.00174365332350135, 0.00106851034797728, 0.00152294372674078, 0.00144279398955405, 0.00104504381306469, 0.00167696829885244, 0.00152769696433097, 0.00157171336468309, 0.00107943860348314, 0.00157547544222325, 0.000861816806718707, 0.0015346483560279, 0.00100878637749702, 0.00147700763773173, 0.00101030245423317, 0.0012907509226352, 0.00102549744769931, 0.00141151458956301, 0.0009930485393852, 0.00138004391919822, 0.00175764062441885, 0.000928450026549399, 0.00178346934262663, 0.00176346208900213, 0.00172712479252368, 0.00177302188239992, 0.00164596759714186, 0.00170451623853296, 0.00132334954105318, 0.00111048470716923, 0.00156286032870412, 0.000695549824740738, 0.00168206600937992, 0.00174683309160173, 0.00129919836763293, 0.00185175682418048, 0.00146677927114069, 0.00148214981891215, 0.00162614171858877, 0.000875677855219692, 0.00149634690023959, 0.00117822550237179, 0.000815210689324886, 0.00156894489191473, 0.00114388519432396, 0.000753695785533637, 0.000725741847418249, 0.00144870369695127, 0.0010616387007758, 0.00127625232562423, 0.000893444404937327, 0.00148457055911422, 0.00180577638093382, 0.00128166447393596, 0.00137338507920504, 0.000767989666201174, 0.00147306476719677, 0.000773961597587913, 0.0014246879145503, 0.00183781306259334, 0.00160314841195941, 0.0019009035313502, 0.00147430167999119, 0.00171925232280046, 0.00156878493726254, 0.00121606630273163, 0.00120414642151445, 0.00148372375406325, 0.000820635934360325, 0.00122112839017063, 0.000765694188885391, 0.00213960534892976, 0.00146267202217132, 0.00211511296220124, 0.00164808321278542, 0.00135868496727198, 0.00159315054770559, 0.00177812203764915, 0.00120397773571312, 0.0016784209292382, 0.000902618921827525, 0.00150173075962812, 0.00196786830201745, 0.00172077387105674, 0.00165288429707289, 0.00180900911800563, 0.00097625789931044, 0.00103535922244191, 0.00166208785958588, 0.0010042751673609, 0.00073070265352726, 0.0017350158886984, 0.00126827554777265, 0.00170527922455221, 0.00136376882437617, 0.00145591364707798, 0.00138019782025367, 0.00118221354205161, 0.000657739175949246, 0.00127265485934913, 0.00133278022985905, 0.00102366495411843, 0.001289700740017, 0.00105581234674901, 0.00154353585094213, 0.00124801800120622, 0.00103926949668676, 0.000679009826853871, 0.00142128090374172, 0.00120030890684575, 0.00165653030853719, 0.00145486148539931, 0.000844719528686255, 0.0016569608123973, 0.00167203485034406, 0.000897398742381483, 0.00181057129520923, 0.00100032391492277, 0.00138985726516694, 0.000762591022066772, 0.00136237987317145, 0.00121246231719851, 0.0011302512139082, 0.00129625806584954, 0.000918357167392969, 0.000945503648836166, 0.00106186559423804, 0.0012255075853318, 0.00168738432694227, 0.00104311865288764, 0.0016514005837962, 0.0010429386748001, 0.00178132602013648, 0.00106524978764355, 0.00131677696481347, 0.00105002254713327, 0.00162988202646375, 0.000980154844000936, 0.000735216541215777, 0.00152488681487739, 0.00170810439158231, 0.00168629735708237, 0.00130711414385587, 0.000883828557562083, 0.00103603187017143, 0.00189095549285412, 0.00157774018589407, 0.00145918619818985, 0.00153243716340512, 0.000864191038999707, 0.00120399100705981, 0.0016915425658226, 0.00107859098352492, 0.00126024812925607, 0.000958779477514327, 0.00174481957219541, 0.000893580669071525, 0.00148337660357356, 0.00183644657954574, 0.00111461989581585, 0.00159351225011051, 0.000820624583866447, 0.00126297934912145, 0.00170614477247, 0.00177130405791104, 0.00127685524057597, 0.00172692548949271, 0.000633229908999056, 0.00211914256215096, 0.00126793072558939, 0.000671260757371783, 0.001508773653768, 0.00181946961674839, 0.0016416369471699, 0.00178848311770707, 0.00119360000826418, 0.00120297400280833, 0.000914674892555922, 0.000830126518849283, 0.00179208081681281, 0.001487607951276, 0.00149876612704247, 0.00179839739575982, 0.000788146804552525, 0.000744161370676011, 0.0017887509893626, 0.00159393972717226, 0.00112343241926283, 0.000933591858483851, 0.0013816487044096, 0.00192605296615511, 0.00178618263453245, 0.000756080495193601, 0.00170419155620039, 0.0010865272488445, 0.00133109570015222, 0.000914276693947613, 0.00134705484379083, 0.000688242784235626, 0.00140586344059557, 0.000941919744946063, 0.00184178713243455, 0.00174229999538511, 0.00198328029364347, 0.0015037611592561, 0.00101937947329134, 0.00146063591819257, 0.00150092446710914, 0.000843480520416051, 0.00166226550936699, 0.00125253386795521, 0.0012744382256642, 0.000663423270452768, 0.00106748228427023, 0.000845956557895988, 0.00163470127154142, 0.00100648775696754, 0.00077803572639823, 0.00135308410972357, 0.00112571287900209, 0.00177639373578131, 0.0017699928721413, 0.00102960073854774, 0.000742088188417256, 0.00144678261131048, 0.00135763792786747, 0.00152200914453715, 0.00146475480869412, 0.00113504205364734, 0.00157585460692644, 0.00191621342673898, 0.00147763092536479, 0.000802004942670465, 0.00137732992880046, 0.000818676373455673, 0.00182695884723216, 0.00215217540971935, 0.00179014238528907, 0.00123648578301072, 0.00124803406652063, 0.00142173259519041, 0.000871284457389265, 0.00140325643587857, 0.00155264209024608, 0.00152731745038182, 0.00179381016641855, 0.00175624119583517, 0.00138641975354403, 0.00108462991192937, 0.00118055718485266, 0.00167456734925508, 0.00117867533117533, 0.000637838442344218, 0.00165133061818779, 0.00149596633855253, 0.000557454186491668, 0.00177265692036599, 0.000929972040466964, 0.00152279459871352, 0.00181074067950249, 0.00129204953555018, 0.00111496530007571, 0.0017466873396188, 0.00076118012657389, 0.000956151227001101, 0.00154517556075007, 0.00107476639095694, 0.00169534655287862, 0.0010441419435665, 0.00161513988859951, 0.000910481438040733, 0.00101232610177249, 0.00135509774554521, 0.0012817558599636, 0.00131125934422016, 0.000973048794548959, 0.0017673420952633, 0.00157073687296361, 0.0016948317643255, 0.00124512147158384, 0.00119452108629048, 0.000929262605495751, 0.00154986314009875, 0.00167704350315034, 0.00173285801429302, 0.00165564683265984, 0.00132947077509016, 0.000946626416407526, 0.000986752333119512, 0.00131806335411966, 0.00111548218410462, 0.00142021453939378, 0.000849726609885693, 0.00097860989626497, 0.000968660518992692, 0.00155027804430574, 0.00137803377583623, 0.000891045201569796, 0.00167696014977992, 0.00127479154616594, 0.00135573663283139, 0.00174453889485449, 0.000853679666761309, 0.00192770571447909, 0.00153582531493157, 0.00155539938714355, 0.00156223913654685, 0.00140218588057905, 0.00173911103047431, 0.00153323786798865, 0.00194347428623587, 0.000872101169079542, 0.000839243992231786, 0.00142786256037652, 0.000614603806752712, 0.00137840560637414, 0.000700036296620965, 0.00074191769817844, 0.00182607932947576, 0.00183989922516048, 0.00169125781394541, 0.00136946409475058, 0.000965768354944885, 0.00162885535974056, 0.00159030442591757, 0.00125620816834271, 0.000889373186510056, 0.000867072260007262, 0.00163438136223704, 0.00101089512463659 };
  static const int16_t buff_info_Conv2D_145_weights_inflated_274_quant_offset[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
  static const uint32_t buff_info__shape_1024_1_1[] = { 1, 1, 1, 1024 };
  static const uint32_t buff_info__mem_shape_F_1024_1_1[] = { 1024, 1, 1 };
  static const uint32_t buff_info__shape_1_33_1_1[] = { 1, 1, 1, 33 };
  static const uint32_t buff_info__mem_shape_F_1_33_1_1[] = { 1, 33, 1, 1 };
  static const uint32_t buff_info__shape_1_65_1_1[] = { 1, 1, 1, 65 };
  static const uint32_t buff_info__mem_shape_F_1_65_1_1[] = { 1, 65, 1, 1 };
  static const uint32_t buff_info__shape_1_129_1_1[] = { 1, 1, 1, 129 };
  static const uint32_t buff_info__mem_shape_F_1_129_1_1[] = { 1, 129, 1, 1 };
  static const uint32_t buff_info__shape_1_257_1_1[] = { 1, 1, 1, 257 };
  static const uint32_t buff_info__mem_shape_F_1_257_1_1[] = { 1, 257, 1, 1 };
  static const uint32_t buff_info__shape_1_513_1_1[] = { 1, 1, 1, 513 };
  static const uint32_t buff_info__mem_shape_F_1_513_1_1[] = { 1, 513, 1, 1 };
  static const uint32_t buff_info__shape_1_1025_1_1[] = { 1, 1, 1, 1025 };
  static const uint32_t buff_info__mem_shape_F_1_1025_1_1[] = { 1, 1025, 1, 1 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_2_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 150528,
      .offset_limit = 150592,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_3_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_112_112,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Sub_3_param1",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194416,
      .offset_end = 10194420,
      .offset_limit = 10194488,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_7_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10175536,
      .offset_end = 10176400,
      .offset_limit = 10176464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_32_3_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_3_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_7_weights_quant_scale,
      .offset = buff_info_Conv2D_7_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_10_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10192784,
      .offset_end = 10192912,
      .offset_limit = 10192976,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_10_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10192912,
      .offset_end = 10193040,
      .offset_limit = 10193104,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_10_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193040,
      .offset_end = 10193168,
      .offset_limit = 10193232,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_10_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193168,
      .offset_end = 10193296,
      .offset_limit = 10193360,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_16_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193296,
      .offset_end = 10193424,
      .offset_limit = 10193488,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_16_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193424,
      .offset_end = 10193552,
      .offset_limit = 10193616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_16_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193552,
      .offset_end = 10193680,
      .offset_limit = 10193744,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "BatchNormalization_16_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193680,
      .offset_end = 10193808,
      .offset_limit = 10193872,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_18_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10152944,
      .offset_end = 10154992,
      .offset_limit = 10155056,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_64_32_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_64_32_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_18_weights_quant_scale,
      .offset = buff_info_Conv2D_18_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_21_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10190320,
      .offset_end = 10190576,
      .offset_limit = 10190640,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_21_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10190576,
      .offset_end = 10190832,
      .offset_limit = 10190896,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_21_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10190832,
      .offset_end = 10191088,
      .offset_limit = 10191152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_21_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10191088,
      .offset_end = 10191344,
      .offset_limit = 10191408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_27_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10191344,
      .offset_end = 10191600,
      .offset_limit = 10191664,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_27_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10191600,
      .offset_end = 10191856,
      .offset_limit = 10191920,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_27_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10191856,
      .offset_end = 10192112,
      .offset_limit = 10192176,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "BatchNormalization_27_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10192112,
      .offset_end = 10192368,
      .offset_limit = 10192432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_29_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9945088,
      .offset_end = 9953280,
      .offset_limit = 9953344,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_128_64_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_128_64_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_29_weights_quant_scale,
      .offset = buff_info_Conv2D_29_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_32_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10179536,
      .offset_end = 10180048,
      .offset_limit = 10180112,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_32_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10180048,
      .offset_end = 10180560,
      .offset_limit = 10180624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_32_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10180560,
      .offset_end = 10181072,
      .offset_limit = 10181136,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_32_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10181072,
      .offset_end = 10181584,
      .offset_limit = 10181648,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_38_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10181584,
      .offset_end = 10182096,
      .offset_limit = 10182160,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_38_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10182096,
      .offset_end = 10182608,
      .offset_limit = 10182672,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_38_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10182608,
      .offset_end = 10183120,
      .offset_limit = 10183184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_38_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10183120,
      .offset_end = 10183632,
      .offset_limit = 10183696,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_40_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9867264,
      .offset_end = 9883648,
      .offset_limit = 9883712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_128_128_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_128_128_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_40_weights_quant_scale,
      .offset = buff_info_Conv2D_40_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_43_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10183632,
      .offset_end = 10184144,
      .offset_limit = 10184208,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_43_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10184144,
      .offset_end = 10184656,
      .offset_limit = 10184720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_43_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10184656,
      .offset_end = 10185168,
      .offset_limit = 10185232,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_43_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10185168,
      .offset_end = 10185680,
      .offset_limit = 10185744,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_49_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10185680,
      .offset_end = 10186192,
      .offset_limit = 10186256,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_49_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10186192,
      .offset_end = 10186704,
      .offset_limit = 10186768,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_49_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10186704,
      .offset_end = 10187216,
      .offset_limit = 10187280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_49_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10187216,
      .offset_end = 10187728,
      .offset_limit = 10187792,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_51_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9797632,
      .offset_end = 9830400,
      .offset_limit = 9830464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_256_128_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_256_128_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_51_weights_quant_scale,
      .offset = buff_info_Conv2D_51_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_54_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10159152,
      .offset_end = 10160176,
      .offset_limit = 10160240,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_54_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10160176,
      .offset_end = 10161200,
      .offset_limit = 10161264,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_54_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10161200,
      .offset_end = 10162224,
      .offset_limit = 10162288,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_54_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10162224,
      .offset_end = 10163248,
      .offset_limit = 10163312,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_60_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10163248,
      .offset_end = 10164272,
      .offset_limit = 10164336,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_60_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10164272,
      .offset_end = 10165296,
      .offset_limit = 10165360,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_60_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10165296,
      .offset_end = 10166320,
      .offset_limit = 10166384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_60_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10166320,
      .offset_end = 10167344,
      .offset_limit = 10167408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_62_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9510912,
      .offset_end = 9576448,
      .offset_limit = 9576512,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_256_256_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_256_256_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_62_weights_quant_scale,
      .offset = buff_info_Conv2D_62_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_65_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10167344,
      .offset_end = 10168368,
      .offset_limit = 10168432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_65_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10168368,
      .offset_end = 10169392,
      .offset_limit = 10169456,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_65_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10169392,
      .offset_end = 10170416,
      .offset_limit = 10170480,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_65_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10170416,
      .offset_end = 10171440,
      .offset_limit = 10171504,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_71_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10171440,
      .offset_end = 10172464,
      .offset_limit = 10172528,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_71_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10172464,
      .offset_end = 10173488,
      .offset_limit = 10173552,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_71_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10173488,
      .offset_end = 10174512,
      .offset_limit = 10174576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "BatchNormalization_71_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10174512,
      .offset_end = 10175536,
      .offset_limit = 10175600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_73_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9306112,
      .offset_end = 9437184,
      .offset_limit = 9437248,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_512_256_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_256_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_73_weights_quant_scale,
      .offset = buff_info_Conv2D_73_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_76_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10054640,
      .offset_end = 10056688,
      .offset_limit = 10056752,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_76_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10056688,
      .offset_end = 10058736,
      .offset_limit = 10058800,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_76_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10058736,
      .offset_end = 10060784,
      .offset_limit = 10060848,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_76_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10060784,
      .offset_end = 10062832,
      .offset_limit = 10062896,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_82_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10062832,
      .offset_end = 10064880,
      .offset_limit = 10064944,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_82_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10064880,
      .offset_end = 10066928,
      .offset_limit = 10066992,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_82_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10066928,
      .offset_end = 10068976,
      .offset_limit = 10069040,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_82_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10068976,
      .offset_end = 10071024,
      .offset_limit = 10071088,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_84_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 7995392,
      .offset_end = 8257536,
      .offset_limit = 8257600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_512_512_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_512_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_84_weights_quant_scale,
      .offset = buff_info_Conv2D_84_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_87_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10071024,
      .offset_end = 10073072,
      .offset_limit = 10073136,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_87_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10073072,
      .offset_end = 10075120,
      .offset_limit = 10075184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_87_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10075120,
      .offset_end = 10077168,
      .offset_limit = 10077232,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_87_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10077168,
      .offset_end = 10079216,
      .offset_limit = 10079280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_93_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10079216,
      .offset_end = 10081264,
      .offset_limit = 10081328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_93_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10081264,
      .offset_end = 10083312,
      .offset_limit = 10083376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_93_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10083312,
      .offset_end = 10085360,
      .offset_limit = 10085424,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_93_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10085360,
      .offset_end = 10087408,
      .offset_limit = 10087472,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_95_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 8257536,
      .offset_end = 8519680,
      .offset_limit = 8519744,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_512_512_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_512_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_95_weights_quant_scale,
      .offset = buff_info_Conv2D_95_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_98_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10087408,
      .offset_end = 10089456,
      .offset_limit = 10089520,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_98_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10089456,
      .offset_end = 10091504,
      .offset_limit = 10091568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_98_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10091504,
      .offset_end = 10093552,
      .offset_limit = 10093616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_98_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10093552,
      .offset_end = 10095600,
      .offset_limit = 10095664,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_104_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10095600,
      .offset_end = 10097648,
      .offset_limit = 10097712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_104_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10097648,
      .offset_end = 10099696,
      .offset_limit = 10099760,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_104_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10099696,
      .offset_end = 10101744,
      .offset_limit = 10101808,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_104_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10101744,
      .offset_end = 10103792,
      .offset_limit = 10103856,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_106_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 8519680,
      .offset_end = 8781824,
      .offset_limit = 8781888,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_512_512_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_512_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_106_weights_quant_scale,
      .offset = buff_info_Conv2D_106_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_109_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10103792,
      .offset_end = 10105840,
      .offset_limit = 10105904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_109_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10105840,
      .offset_end = 10107888,
      .offset_limit = 10107952,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_109_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10107888,
      .offset_end = 10109936,
      .offset_limit = 10110000,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_109_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10109936,
      .offset_end = 10111984,
      .offset_limit = 10112048,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_115_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10111984,
      .offset_end = 10114032,
      .offset_limit = 10114096,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_115_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10114032,
      .offset_end = 10116080,
      .offset_limit = 10116144,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_115_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10116080,
      .offset_end = 10118128,
      .offset_limit = 10118192,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_115_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10118128,
      .offset_end = 10120176,
      .offset_limit = 10120240,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_117_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 8781824,
      .offset_end = 9043968,
      .offset_limit = 9044032,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_512_512_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_512_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_117_weights_quant_scale,
      .offset = buff_info_Conv2D_117_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_120_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10120176,
      .offset_end = 10122224,
      .offset_limit = 10122288,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_120_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10122224,
      .offset_end = 10124272,
      .offset_limit = 10124336,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_120_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10124272,
      .offset_end = 10126320,
      .offset_limit = 10126384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_120_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10126320,
      .offset_end = 10128368,
      .offset_limit = 10128432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_126_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10128368,
      .offset_end = 10130416,
      .offset_limit = 10130480,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_126_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10130416,
      .offset_end = 10132464,
      .offset_limit = 10132528,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_126_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10132464,
      .offset_end = 10134512,
      .offset_limit = 10134576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_126_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10134512,
      .offset_end = 10136560,
      .offset_limit = 10136624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_128_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9043968,
      .offset_end = 9306112,
      .offset_limit = 9306176,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_512_512_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_512_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_128_weights_quant_scale,
      .offset = buff_info_Conv2D_128_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_131_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10136560,
      .offset_end = 10138608,
      .offset_limit = 10138672,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_131_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10138608,
      .offset_end = 10140656,
      .offset_limit = 10140720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_131_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10140656,
      .offset_end = 10142704,
      .offset_limit = 10142768,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_131_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10142704,
      .offset_end = 10144752,
      .offset_limit = 10144816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_137_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10144752,
      .offset_end = 10146800,
      .offset_limit = 10146864,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_137_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10146800,
      .offset_end = 10148848,
      .offset_limit = 10148912,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_137_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10148848,
      .offset_end = 10150896,
      .offset_limit = 10150960,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "BatchNormalization_137_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10150896,
      .offset_end = 10152944,
      .offset_limit = 10153008,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_512,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_512,
    },
    {
      .name = "Conv2D_139_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 7471104,
      .offset_end = 7995392,
      .offset_limit = 7995456,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1024_512_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1024_512_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_139_weights_quant_scale,
      .offset = buff_info_Conv2D_139_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_142_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9970224,
      .offset_end = 9974320,
      .offset_limit = 9974384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_142_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9974320,
      .offset_end = 9978416,
      .offset_limit = 9978480,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_142_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9978416,
      .offset_end = 9982512,
      .offset_limit = 9982576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_142_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9982512,
      .offset_end = 9986608,
      .offset_limit = 9986672,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_148_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9986608,
      .offset_end = 9990704,
      .offset_limit = 9990768,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_148_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9990704,
      .offset_end = 9994800,
      .offset_limit = 9994864,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_148_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9994800,
      .offset_end = 9998896,
      .offset_limit = 9998960,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_148_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9998896,
      .offset_end = 10002992,
      .offset_limit = 10003056,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "Conv2D_150_weights",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 6422528,
      .offset_end = 7471104,
      .offset_limit = 7471168,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1024_1024_1_1,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1024_1024_1_1,
      .per_channel = 1,
      .scale = buff_info_Conv2D_150_weights_quant_scale,
      .offset = buff_info_Conv2D_150_weights_quant_offset,
    },
    {
      .name = "BatchNormalization_153_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10002992,
      .offset_end = 10007088,
      .offset_limit = 10007152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_153_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10007088,
      .offset_end = 10011184,
      .offset_limit = 10011248,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_153_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10011184,
      .offset_end = 10015280,
      .offset_limit = 10015344,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "BatchNormalization_153_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10015280,
      .offset_end = 10019376,
      .offset_limit = 10019440,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1024,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024,
    },
    {
      .name = "Quantize_156_y_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194640,
      .offset_end = 10194644,
      .offset_limit = 10194712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Quantize_156_y_zero_point",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195568,
      .offset_end = 10195569,
      .offset_limit = 10195640,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Gemm_159_bias",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10178512,
      .offset_end = 10179024,
      .offset_limit = 10179088,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT32,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
      .per_channel = 1,
      .scale = buff_info_Gemm_159_bias_quant_scale,
      .offset = buff_info_Gemm_159_bias_quant_offset,
    },
    {
      .name = "BatchNormalization_160_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10187728,
      .offset_end = 10188240,
      .offset_limit = 10188304,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_160_B",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10188240,
      .offset_end = 10188752,
      .offset_limit = 10188816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_160_input_mean",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10188752,
      .offset_end = 10189264,
      .offset_limit = 10189328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "BatchNormalization_160_input_var",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10189264,
      .offset_end = 10189776,
      .offset_limit = 10189840,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Gemm_159_weights_transposed_3",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 0,
      .offset_end = 6422528,
      .offset_limit = 6422592,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 50176,
      .mem_shape = buff_info__mem_shape_F_128_50176_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_128_50176_1_1,
      .per_channel = 1,
      .scale = buff_info_Gemm_159_weights_transposed_3_quant_scale,
      .offset = buff_info_Gemm_159_weights_transposed_3_quant_offset,
    },
    {
      .name = "Conv2D_139_mul_scale_227",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9883648,
      .offset_end = 9897984,
      .offset_limit = 9898048,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1024_1_7,
      .mem_ndims = 4,
      .chpos = CHPos_Mixed,
      .Qm = -13,
      .Qn = 28,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1024_1_7,
    },
    {
      .name = "Conv2D_145_mul_scale_236",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9897984,
      .offset_end = 9912320,
      .offset_limit = 9912384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 4,
      .mem_shape = buff_info__mem_shape_M_1024_1_7,
      .mem_ndims = 4,
      .chpos = CHPos_Mixed,
      .Qm = -15,
      .Qn = 30,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1024_1_7,
    },
    {
      .name = "Conv2D_150_mul_scale_245",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9912320,
      .offset_end = 9926656,
      .offset_limit = 9926720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1024_1_7,
      .mem_ndims = 4,
      .chpos = CHPos_Mixed,
      .Qm = -13,
      .Qn = 28,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1024_1_7,
    },
    {
      .name = "Conv2D_13_weights_inflated_250",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10027568,
      .offset_end = 10029872,
      .offset_limit = 10029936,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_32_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_32_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_13_weights_inflated_250_quant_scale,
      .offset = buff_info_Conv2D_13_weights_inflated_250_quant_offset,
    },
    {
      .name = "Conv2D_24_weights_inflated_252",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9953280,
      .offset_end = 9957888,
      .offset_limit = 9957952,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_64_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_64_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_24_weights_inflated_252_quant_scale,
      .offset = buff_info_Conv2D_24_weights_inflated_252_quant_offset,
    },
    {
      .name = "Conv2D_35_weights_inflated_254",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9926656,
      .offset_end = 9935872,
      .offset_limit = 9935936,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_128_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_128_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_35_weights_inflated_254_quant_scale,
      .offset = buff_info_Conv2D_35_weights_inflated_254_quant_offset,
    },
    {
      .name = "Conv2D_46_weights_inflated_256",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9935872,
      .offset_end = 9945088,
      .offset_limit = 9945152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_128_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_128_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_46_weights_inflated_256_quant_scale,
      .offset = buff_info_Conv2D_46_weights_inflated_256_quant_offset,
    },
    {
      .name = "Conv2D_57_weights_inflated_258",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9830400,
      .offset_end = 9848832,
      .offset_limit = 9848896,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_256_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_256_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_57_weights_inflated_258_quant_scale,
      .offset = buff_info_Conv2D_57_weights_inflated_258_quant_offset,
    },
    {
      .name = "Conv2D_68_weights_inflated_260",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9848832,
      .offset_end = 9867264,
      .offset_limit = 9867328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_256_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_256_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_68_weights_inflated_260_quant_scale,
      .offset = buff_info_Conv2D_68_weights_inflated_260_quant_offset,
    },
    {
      .name = "Conv2D_79_weights_inflated_262",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9576448,
      .offset_end = 9613312,
      .offset_limit = 9613376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_512_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_79_weights_inflated_262_quant_scale,
      .offset = buff_info_Conv2D_79_weights_inflated_262_quant_offset,
    },
    {
      .name = "Conv2D_90_weights_inflated_264",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9613312,
      .offset_end = 9650176,
      .offset_limit = 9650240,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_512_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_90_weights_inflated_264_quant_scale,
      .offset = buff_info_Conv2D_90_weights_inflated_264_quant_offset,
    },
    {
      .name = "Conv2D_101_weights_inflated_266",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9650176,
      .offset_end = 9687040,
      .offset_limit = 9687104,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_512_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_101_weights_inflated_266_quant_scale,
      .offset = buff_info_Conv2D_101_weights_inflated_266_quant_offset,
    },
    {
      .name = "Conv2D_112_weights_inflated_268",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9687040,
      .offset_end = 9723904,
      .offset_limit = 9723968,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_512_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_112_weights_inflated_268_quant_scale,
      .offset = buff_info_Conv2D_112_weights_inflated_268_quant_offset,
    },
    {
      .name = "Conv2D_123_weights_inflated_270",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9723904,
      .offset_end = 9760768,
      .offset_limit = 9760832,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_512_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_123_weights_inflated_270_quant_scale,
      .offset = buff_info_Conv2D_123_weights_inflated_270_quant_offset,
    },
    {
      .name = "Conv2D_134_weights_inflated_272",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9760768,
      .offset_end = 9797632,
      .offset_limit = 9797696,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_512_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_512_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_134_weights_inflated_272_quant_scale,
      .offset = buff_info_Conv2D_134_weights_inflated_272_quant_offset,
    },
    {
      .name = "Conv2D_145_weights_inflated_274",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9437184,
      .offset_end = 9510912,
      .offset_limit = 9510976,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 8,
      .mem_shape = buff_info__mem_shape_L_1024_8_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1024_8_3_3,
      .per_channel = 1,
      .scale = buff_info_Conv2D_145_weights_inflated_274_quant_scale,
      .offset = buff_info_Conv2D_145_weights_inflated_274_quant_offset,
    },
    {
      .name = "bn_alpha277",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10019376,
      .offset_end = 10023472,
      .offset_limit = 10023536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_F_1024_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024_1_1,
    },
    {
      .name = "bn_beta279",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10023472,
      .offset_end = 10027568,
      .offset_limit = 10027632,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_F_1024_1_1,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1024_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id608_700_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194704,
      .offset_end = 10194708,
      .offset_limit = 10194776,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id608_700_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195632,
      .offset_end = 10195633,
      .offset_limit = 10195704,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_7_off_bias_12_702_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193856,
      .offset_end = 10193860,
      .offset_limit = 10193928,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_7_off_bias_12_702_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194800,
      .offset_end = 10194801,
      .offset_limit = 10194872,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_11_704__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10192368,
      .offset_end = 10192500,
      .offset_limit = 10192568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 33,
      .mem_shape = buff_info__mem_shape_F_1_33_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_33_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id610_705_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194016,
      .offset_end = 10194020,
      .offset_limit = 10194088,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id610_705_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194960,
      .offset_end = 10194961,
      .offset_limit = 10195032,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_13_off_bias_21_707_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193872,
      .offset_end = 10193876,
      .offset_limit = 10193944,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_13_off_bias_21_707_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194816,
      .offset_end = 10194817,
      .offset_limit = 10194888,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_17_709__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10192512,
      .offset_end = 10192644,
      .offset_limit = 10192712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 33,
      .mem_shape = buff_info__mem_shape_F_1_33_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_33_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id612_710_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194032,
      .offset_end = 10194036,
      .offset_limit = 10194104,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id612_710_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194976,
      .offset_end = 10194977,
      .offset_limit = 10195048,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_18_off_bias_30_712_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193840,
      .offset_end = 10193844,
      .offset_limit = 10193912,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_18_off_bias_30_712_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194784,
      .offset_end = 10194785,
      .offset_limit = 10194856,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_22_714__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10189776,
      .offset_end = 10190036,
      .offset_limit = 10190104,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 65,
      .mem_shape = buff_info__mem_shape_F_1_65_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id615_715_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193936,
      .offset_end = 10193940,
      .offset_limit = 10194008,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id615_715_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194880,
      .offset_end = 10194881,
      .offset_limit = 10194952,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_24_off_bias_39_717_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193952,
      .offset_end = 10193956,
      .offset_limit = 10194024,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_24_off_bias_39_717_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194896,
      .offset_end = 10194897,
      .offset_limit = 10194968,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_28_719__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10190048,
      .offset_end = 10190308,
      .offset_limit = 10190376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 65,
      .mem_shape = buff_info__mem_shape_F_1_65_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id617_720_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194288,
      .offset_end = 10194292,
      .offset_limit = 10194360,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id617_720_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195232,
      .offset_end = 10195233,
      .offset_limit = 10195304,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_29_off_bias_48_722_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193888,
      .offset_end = 10193892,
      .offset_limit = 10193960,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_29_off_bias_48_722_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194832,
      .offset_end = 10194833,
      .offset_limit = 10194904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_33_724__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10176400,
      .offset_end = 10176916,
      .offset_limit = 10176984,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 129,
      .mem_shape = buff_info__mem_shape_F_1_129_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_129_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id622_725_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194048,
      .offset_end = 10194052,
      .offset_limit = 10194120,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id622_725_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194992,
      .offset_end = 10194993,
      .offset_limit = 10195064,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_35_off_bias_57_727_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193904,
      .offset_end = 10193908,
      .offset_limit = 10193976,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_35_off_bias_57_727_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194848,
      .offset_end = 10194849,
      .offset_limit = 10194920,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_39_729__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10176928,
      .offset_end = 10177444,
      .offset_limit = 10177512,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 129,
      .mem_shape = buff_info__mem_shape_F_1_129_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_129_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id624_730_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194064,
      .offset_end = 10194068,
      .offset_limit = 10194136,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id624_730_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195008,
      .offset_end = 10195009,
      .offset_limit = 10195080,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_40_off_bias_66_732_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193920,
      .offset_end = 10193924,
      .offset_limit = 10193992,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_40_off_bias_66_732_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194864,
      .offset_end = 10194865,
      .offset_limit = 10194936,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_44_734__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10177456,
      .offset_end = 10177972,
      .offset_limit = 10178040,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 129,
      .mem_shape = buff_info__mem_shape_F_1_129_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_129_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id629_735_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194080,
      .offset_end = 10194084,
      .offset_limit = 10194152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id629_735_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195024,
      .offset_end = 10195025,
      .offset_limit = 10195096,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_46_off_bias_75_737_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194096,
      .offset_end = 10194100,
      .offset_limit = 10194168,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_46_off_bias_75_737_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195040,
      .offset_end = 10195041,
      .offset_limit = 10195112,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_50_739__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10177984,
      .offset_end = 10178500,
      .offset_limit = 10178568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 129,
      .mem_shape = buff_info__mem_shape_F_1_129_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_129_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id631_740_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194432,
      .offset_end = 10194436,
      .offset_limit = 10194504,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id631_740_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195360,
      .offset_end = 10195361,
      .offset_limit = 10195432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_51_off_bias_84_743_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193968,
      .offset_end = 10193972,
      .offset_limit = 10194040,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_51_off_bias_84_743_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194912,
      .offset_end = 10194913,
      .offset_limit = 10194984,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_55_745__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10154992,
      .offset_end = 10156020,
      .offset_limit = 10156088,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id636_746_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194304,
      .offset_end = 10194308,
      .offset_limit = 10194376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id636_746_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195248,
      .offset_end = 10195249,
      .offset_limit = 10195320,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_57_off_bias_93_748_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193984,
      .offset_end = 10193988,
      .offset_limit = 10194056,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_57_off_bias_93_748_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194928,
      .offset_end = 10194929,
      .offset_limit = 10195000,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_61_750__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10156032,
      .offset_end = 10157060,
      .offset_limit = 10157128,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id638_751_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194320,
      .offset_end = 10194324,
      .offset_limit = 10194392,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id638_751_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195264,
      .offset_end = 10195265,
      .offset_limit = 10195336,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_62_off_bias_102_754_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194000,
      .offset_end = 10194004,
      .offset_limit = 10194072,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_62_off_bias_102_754_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194944,
      .offset_end = 10194945,
      .offset_limit = 10195016,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_66_756__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10157072,
      .offset_end = 10158100,
      .offset_limit = 10158168,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id643_757_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194336,
      .offset_end = 10194340,
      .offset_limit = 10194408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id643_757_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195280,
      .offset_end = 10195281,
      .offset_limit = 10195352,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_68_off_bias_111_759_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194352,
      .offset_end = 10194356,
      .offset_limit = 10194424,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_68_off_bias_111_759_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195296,
      .offset_end = 10195297,
      .offset_limit = 10195368,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_72_761__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10158112,
      .offset_end = 10159140,
      .offset_limit = 10159208,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 257,
      .mem_shape = buff_info__mem_shape_F_1_257_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_257_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id645_762_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194656,
      .offset_end = 10194660,
      .offset_limit = 10194728,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id645_762_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195584,
      .offset_end = 10195585,
      .offset_limit = 10195656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_73_off_bias_120_765_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194112,
      .offset_end = 10194116,
      .offset_limit = 10194184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_73_off_bias_120_765_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195056,
      .offset_end = 10195057,
      .offset_limit = 10195128,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_77_767__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10029872,
      .offset_end = 10031924,
      .offset_limit = 10031992,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id650_768_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194448,
      .offset_end = 10194452,
      .offset_limit = 10194520,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id650_768_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195376,
      .offset_end = 10195377,
      .offset_limit = 10195448,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_79_off_bias_129_770_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194128,
      .offset_end = 10194132,
      .offset_limit = 10194200,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_79_off_bias_129_770_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195072,
      .offset_end = 10195073,
      .offset_limit = 10195144,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_83_772__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10031936,
      .offset_end = 10033988,
      .offset_limit = 10034056,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id652_773_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194464,
      .offset_end = 10194468,
      .offset_limit = 10194536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id652_773_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195392,
      .offset_end = 10195393,
      .offset_limit = 10195464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_84_off_bias_138_776_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194144,
      .offset_end = 10194148,
      .offset_limit = 10194216,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_84_off_bias_138_776_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195088,
      .offset_end = 10195089,
      .offset_limit = 10195160,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_88_778__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10034000,
      .offset_end = 10036052,
      .offset_limit = 10036120,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id657_779_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194480,
      .offset_end = 10194484,
      .offset_limit = 10194552,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id657_779_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195408,
      .offset_end = 10195409,
      .offset_limit = 10195480,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_90_off_bias_147_781_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194160,
      .offset_end = 10194164,
      .offset_limit = 10194232,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_90_off_bias_147_781_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195104,
      .offset_end = 10195105,
      .offset_limit = 10195176,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_94_783__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10036064,
      .offset_end = 10038116,
      .offset_limit = 10038184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id659_784_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194496,
      .offset_end = 10194500,
      .offset_limit = 10194568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id659_784_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195424,
      .offset_end = 10195425,
      .offset_limit = 10195496,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_95_off_bias_156_787_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194176,
      .offset_end = 10194180,
      .offset_limit = 10194248,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_95_off_bias_156_787_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195120,
      .offset_end = 10195121,
      .offset_limit = 10195192,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_99_789__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10038128,
      .offset_end = 10040180,
      .offset_limit = 10040248,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id664_790_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194512,
      .offset_end = 10194516,
      .offset_limit = 10194584,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id664_790_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195440,
      .offset_end = 10195441,
      .offset_limit = 10195512,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_101_off_bias_165_792_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194192,
      .offset_end = 10194196,
      .offset_limit = 10194264,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_101_off_bias_165_792_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195136,
      .offset_end = 10195137,
      .offset_limit = 10195208,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_105_794__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10040192,
      .offset_end = 10042244,
      .offset_limit = 10042312,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id666_795_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194528,
      .offset_end = 10194532,
      .offset_limit = 10194600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id666_795_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195456,
      .offset_end = 10195457,
      .offset_limit = 10195528,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_106_off_bias_174_798_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194208,
      .offset_end = 10194212,
      .offset_limit = 10194280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_106_off_bias_174_798_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195152,
      .offset_end = 10195153,
      .offset_limit = 10195224,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_110_800__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10042256,
      .offset_end = 10044308,
      .offset_limit = 10044376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id671_801_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194544,
      .offset_end = 10194548,
      .offset_limit = 10194616,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id671_801_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195472,
      .offset_end = 10195473,
      .offset_limit = 10195544,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_112_off_bias_183_803_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194224,
      .offset_end = 10194228,
      .offset_limit = 10194296,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_112_off_bias_183_803_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195168,
      .offset_end = 10195169,
      .offset_limit = 10195240,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_116_805__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10044320,
      .offset_end = 10046372,
      .offset_limit = 10046440,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id673_806_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194560,
      .offset_end = 10194564,
      .offset_limit = 10194632,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id673_806_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195488,
      .offset_end = 10195489,
      .offset_limit = 10195560,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_117_off_bias_192_809_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194240,
      .offset_end = 10194244,
      .offset_limit = 10194312,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_117_off_bias_192_809_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195184,
      .offset_end = 10195185,
      .offset_limit = 10195256,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_121_811__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10046384,
      .offset_end = 10048436,
      .offset_limit = 10048504,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id678_812_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194576,
      .offset_end = 10194580,
      .offset_limit = 10194648,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id678_812_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195504,
      .offset_end = 10195505,
      .offset_limit = 10195576,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_123_off_bias_201_814_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194256,
      .offset_end = 10194260,
      .offset_limit = 10194328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_123_off_bias_201_814_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195200,
      .offset_end = 10195201,
      .offset_limit = 10195272,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_127_816__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10048448,
      .offset_end = 10050500,
      .offset_limit = 10050568,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id680_817_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194592,
      .offset_end = 10194596,
      .offset_limit = 10194664,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id680_817_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195520,
      .offset_end = 10195521,
      .offset_limit = 10195592,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_128_off_bias_210_820_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194272,
      .offset_end = 10194276,
      .offset_limit = 10194344,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_128_off_bias_210_820_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195216,
      .offset_end = 10195217,
      .offset_limit = 10195288,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_132_822__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10050512,
      .offset_end = 10052564,
      .offset_limit = 10052632,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id685_823_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194608,
      .offset_end = 10194612,
      .offset_limit = 10194680,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id685_823_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195536,
      .offset_end = 10195537,
      .offset_limit = 10195608,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_134_off_bias_219_825_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194624,
      .offset_end = 10194628,
      .offset_limit = 10194696,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_134_off_bias_219_825_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195552,
      .offset_end = 10195553,
      .offset_limit = 10195624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_138_827__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10052576,
      .offset_end = 10054628,
      .offset_limit = 10054696,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 513,
      .mem_shape = buff_info__mem_shape_F_1_513_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_513_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id687_828_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194720,
      .offset_end = 10194724,
      .offset_limit = 10194792,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id687_828_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195648,
      .offset_end = 10195649,
      .offset_limit = 10195720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_139_off_bias_228_831_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194368,
      .offset_end = 10194372,
      .offset_limit = 10194440,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_139_off_bias_228_831_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195312,
      .offset_end = 10195313,
      .offset_limit = 10195384,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_143_833__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9957888,
      .offset_end = 9961988,
      .offset_limit = 9962056,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1025,
      .mem_shape = buff_info__mem_shape_F_1_1025_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1025_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id692_834_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194672,
      .offset_end = 10194676,
      .offset_limit = 10194744,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id692_834_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195600,
      .offset_end = 10195601,
      .offset_limit = 10195672,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_145_off_bias_237_836_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194384,
      .offset_end = 10194388,
      .offset_limit = 10194456,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_145_off_bias_237_836_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195328,
      .offset_end = 10195329,
      .offset_limit = 10195400,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_149_838__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9962000,
      .offset_end = 9966100,
      .offset_limit = 9966168,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1025,
      .mem_shape = buff_info__mem_shape_F_1_1025_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1025_1_1,
    },
    {
      .name = "QuantizeLinear_inserted_id694_839_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194688,
      .offset_end = 10194692,
      .offset_limit = 10194760,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "QuantizeLinear_inserted_id694_839_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195616,
      .offset_end = 10195617,
      .offset_limit = 10195688,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_150_off_bias_246_842_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194400,
      .offset_end = 10194404,
      .offset_limit = 10194472,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_150_off_bias_246_842_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195344,
      .offset_end = 10195345,
      .offset_limit = 10195416,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "PReLU_154_844__slopes_",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 9966112,
      .offset_end = 9970212,
      .offset_limit = 9970280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1025,
      .mem_shape = buff_info__mem_shape_F_1_1025_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1025_1_1,
    },
    {
      .name = "Gemm_159_reshape_x_2_845_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193808,
      .offset_end = 10193812,
      .offset_limit = 10193880,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Gemm_159_reshape_x_2_845_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194752,
      .offset_end = 10194753,
      .offset_limit = 10194824,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Gemm_159_weights_transposed_3_847_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10179024,
      .offset_end = 10179536,
      .offset_limit = 10179600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Gemm_159_weights_transposed_3_847_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10192656,
      .offset_end = 10192784,
      .offset_limit = 10192848,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Gemm_159_conv_4_849_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10193824,
      .offset_end = 10193828,
      .offset_limit = 10193896,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Gemm_159_conv_4_849_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194768,
      .offset_end = 10194769,
      .offset_limit = 10194840,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Gemm_159_squeeze_y_5_851_atonn_internal_scale",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10194736,
      .offset_end = 10194740,
      .offset_limit = 10194808,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Gemm_159_squeeze_y_5_851_atonn_internal_offset",
      .addr_base = {(unsigned char *)(0x70380000UL) /* Equivalent hex address = 0x70380000UL */},
      .offset_start = 10195664,
      .offset_end = 10195665,
      .offset_limit = 10195736,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_face_recognition(void)
{
  static const uint32_t buff_info__shape_1_128[] = { 1, 1, 128, 1 };
  static const uint32_t buff_info__mem_shape_U_1_128[] = { 1, 128 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "BatchNormalization_160_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 512,
      .offset_limit = 576,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 156,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_128,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_face_recognition(void)
{
  static const uint32_t buff_info__shape_1_3_112_112[] = { 1, 112, 112, 3 };
  static const uint32_t buff_info__mem_shape_L_1_3_112_112[] = { 1, 112, 112, 3 };
  static const float buff_info_Sub_3_out_0_inserted_out608_quant_scale[] = { 0.5 };
  static const int16_t buff_info_Sub_3_out_0_inserted_out608_quant_offset[] = { 127 };
  static const float buff_info_Mul_4_out_0_cp_in_97_quant_scale[] = { 0.00390625 };
  static const int16_t buff_info_Mul_4_out_0_cp_in_97_quant_offset[] = { 127 };
  static const uint32_t buff_info__shape_1_32_112_112[] = { 1, 112, 112, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_112_112[] = { 1, 112, 112, 32 };
  static const float buff_info_Conv2D_7_off_bias_out_13_quant_scale[] = { 0.0124382572248578 };
  static const int16_t buff_info_Conv2D_7_off_bias_out_13_quant_offset[] = { 5 };
  static const float buff_info_PReLU_11_out_0_inserted_out610_quant_scale[] = { 0.0537846088409424 };
  static const int16_t buff_info_PReLU_11_out_0_inserted_out610_quant_offset[] = { -25 };
  static const float buff_info_Conv2D_13_off_bias_out_22_quant_scale[] = { 0.0277070757001638 };
  static const int16_t buff_info_Conv2D_13_off_bias_out_22_quant_offset[] = { 24 };
  static const float buff_info_PReLU_17_out_0_inserted_out612_quant_scale[] = { 0.057623103260994 };
  static const int16_t buff_info_PReLU_17_out_0_inserted_out612_quant_offset[] = { -21 };
  static const uint32_t buff_info__shape_1_64_112_112[] = { 1, 112, 112, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_112_112[] = { 1, 112, 112, 64 };
  static const float buff_info_Conv2D_18_off_bias_out_31_quant_scale[] = { 0.0422812700271606 };
  static const int16_t buff_info_Conv2D_18_off_bias_out_31_quant_offset[] = { 20 };
  static const float buff_info_PReLU_22_out_0_inserted_out615_quant_scale[] = { 0.0593999177217484 };
  static const int16_t buff_info_PReLU_22_out_0_inserted_out615_quant_offset[] = { -68 };
  static const uint32_t buff_info__shape_1_64_56_56[] = { 1, 56, 56, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_56_56[] = { 1, 56, 56, 64 };
  static const float buff_info_Conv2D_24_off_bias_out_40_quant_scale[] = { 0.0156908705830574 };
  static const int16_t buff_info_Conv2D_24_off_bias_out_40_quant_offset[] = { -20 };
  static const float buff_info_PReLU_28_out_0_inserted_out617_quant_scale[] = { 0.0552705451846123 };
  static const int16_t buff_info_PReLU_28_out_0_inserted_out617_quant_offset[] = { -82 };
  static const uint32_t buff_info__shape_1_128_56_56[] = { 1, 56, 56, 128 };
  static const uint32_t buff_info__mem_shape_L_1_128_56_56[] = { 1, 56, 56, 128 };
  static const float buff_info_Conv2D_29_off_bias_out_49_quant_scale[] = { 0.0220197346061468 };
  static const int16_t buff_info_Conv2D_29_off_bias_out_49_quant_offset[] = { 35 };
  static const float buff_info_PReLU_33_out_0_inserted_out622_quant_scale[] = { 0.0342308953404427 };
  static const int16_t buff_info_PReLU_33_out_0_inserted_out622_quant_offset[] = { -75 };
  static const float buff_info_Conv2D_35_off_bias_out_58_quant_scale[] = { 0.0135031631216407 };
  static const int16_t buff_info_Conv2D_35_off_bias_out_58_quant_offset[] = { -14 };
  static const float buff_info_PReLU_39_out_0_inserted_out624_quant_scale[] = { 0.0469064228236675 };
  static const int16_t buff_info_PReLU_39_out_0_inserted_out624_quant_offset[] = { -62 };
  static const uint32_t buff_info__mem_shape_M_1_128_56_56[] = { 1, 8, 56, 56, 16 };
  static const float buff_info_Conv2D_40_off_bias_out_67_quant_scale[] = { 0.027754332870245 };
  static const int16_t buff_info_Conv2D_40_off_bias_out_67_quant_offset[] = { 29 };
  static const float buff_info_PReLU_44_out_0_inserted_out629_quant_scale[] = { 0.0257888585329056 };
  static const int16_t buff_info_PReLU_44_out_0_inserted_out629_quant_offset[] = { -82 };
  static const uint32_t buff_info__shape_1_128_28_28[] = { 1, 28, 28, 128 };
  static const uint32_t buff_info__mem_shape_L_1_128_28_28[] = { 1, 28, 28, 128 };
  static const float buff_info_Conv2D_46_off_bias_out_76_quant_scale[] = { 0.00877392012625933 };
  static const int16_t buff_info_Conv2D_46_off_bias_out_76_quant_offset[] = { -5 };
  static const float buff_info_PReLU_50_out_0_inserted_out631_quant_scale[] = { 0.0400105267763138 };
  static const int16_t buff_info_PReLU_50_out_0_inserted_out631_quant_offset[] = { -43 };
  static const uint32_t buff_info__mem_shape_M_1_128_28_28[] = { 1, 8, 28, 28, 16 };
  static const float buff_info_PReLU_50_out_0_inserted_out631_cp_in_135_quant_scale[] = { 0.0400105267763138 };
  static const int16_t buff_info_PReLU_50_out_0_inserted_out631_cp_in_135_quant_offset[] = { -43 };
  static const uint32_t buff_info__shape_1_256_28_28[] = { 1, 28, 28, 256 };
  static const uint32_t buff_info__mem_shape_M_1_256_28_28[] = { 1, 16, 28, 28, 16 };
  static const uint32_t buff_info__mem_shape_L_1_256_28_28[] = { 1, 28, 28, 256 };
  static const float buff_info_Conv2D_51_off_bias_out_85_quant_scale[] = { 0.0190843399614096 };
  static const int16_t buff_info_Conv2D_51_off_bias_out_85_quant_offset[] = { 35 };
  static const float buff_info_PReLU_55_out_0_inserted_out636_quant_scale[] = { 0.0254904478788376 };
  static const int16_t buff_info_PReLU_55_out_0_inserted_out636_quant_offset[] = { -73 };
  static const float buff_info_Conv2D_57_off_bias_out_94_quant_scale[] = { 0.00704042101278901 };
  static const int16_t buff_info_Conv2D_57_off_bias_out_94_quant_offset[] = { -24 };
  static const float buff_info_PReLU_61_out_0_inserted_out638_quant_scale[] = { 0.0281442422419786 };
  static const int16_t buff_info_PReLU_61_out_0_inserted_out638_quant_offset[] = { -81 };
  static const float buff_info_PReLU_61_out_0_inserted_out638_cp_in_136_quant_scale[] = { 0.0281442422419786 };
  static const int16_t buff_info_PReLU_61_out_0_inserted_out638_cp_in_136_quant_offset[] = { -81 };
  static const float buff_info_Conv2D_62_off_bias_out_103_quant_scale[] = { 0.0217822138220072 };
  static const int16_t buff_info_Conv2D_62_off_bias_out_103_quant_offset[] = { 26 };
  static const float buff_info_PReLU_66_out_0_inserted_out643_quant_scale[] = { 0.0282704774290323 };
  static const int16_t buff_info_PReLU_66_out_0_inserted_out643_quant_offset[] = { -68 };
  static const uint32_t buff_info__shape_1_256_14_14[] = { 1, 14, 14, 256 };
  static const uint32_t buff_info__mem_shape_L_1_256_14_14[] = { 1, 14, 14, 256 };
  static const float buff_info_Conv2D_68_off_bias_out_112_quant_scale[] = { 0.00790206994861364 };
  static const int16_t buff_info_Conv2D_68_off_bias_out_112_quant_offset[] = { -21 };
  static const float buff_info_PReLU_72_out_0_inserted_out645_quant_scale[] = { 0.0296581704169512 };
  static const int16_t buff_info_PReLU_72_out_0_inserted_out645_quant_offset[] = { -52 };
  static const uint32_t buff_info__mem_shape_M_1_256_14_14[] = { 1, 16, 14, 14, 16 };
  static const float buff_info_PReLU_72_out_0_inserted_out645_cp_in_137_quant_scale[] = { 0.0296581704169512 };
  static const int16_t buff_info_PReLU_72_out_0_inserted_out645_cp_in_137_quant_offset[] = { -52 };
  static const uint32_t buff_info__shape_1_512_14_14[] = { 1, 14, 14, 512 };
  static const uint32_t buff_info__mem_shape_M_1_512_14_14[] = { 1, 32, 14, 14, 16 };
  static const uint32_t buff_info__mem_shape_L_1_512_14_14[] = { 1, 14, 14, 512 };
  static const float buff_info_Conv2D_73_off_bias_out_121_quant_scale[] = { 0.0167987793684006 };
  static const int16_t buff_info_Conv2D_73_off_bias_out_121_quant_offset[] = { 51 };
  static const float buff_info_PReLU_77_out_0_inserted_out650_quant_scale[] = { 0.0182438138872385 };
  static const int16_t buff_info_PReLU_77_out_0_inserted_out650_quant_offset[] = { -72 };
  static const float buff_info_Conv2D_79_off_bias_out_130_quant_scale[] = { 0.00463617267087102 };
  static const int16_t buff_info_Conv2D_79_off_bias_out_130_quant_offset[] = { -10 };
  static const float buff_info_PReLU_83_out_0_inserted_out652_quant_scale[] = { 0.0255309920758009 };
  static const int16_t buff_info_PReLU_83_out_0_inserted_out652_quant_offset[] = { -76 };
  static const float buff_info_PReLU_83_out_0_inserted_out652_cp_in_138_quant_scale[] = { 0.0255309920758009 };
  static const int16_t buff_info_PReLU_83_out_0_inserted_out652_cp_in_138_quant_offset[] = { -76 };
  static const float buff_info_Conv2D_84_off_bias_out_139_quant_scale[] = { 0.0100223934277892 };
  static const int16_t buff_info_Conv2D_84_off_bias_out_139_quant_offset[] = { -5 };
  static const float buff_info_PReLU_88_out_0_inserted_out657_quant_scale[] = { 0.0176423732191324 };
  static const int16_t buff_info_PReLU_88_out_0_inserted_out657_quant_offset[] = { -64 };
  static const float buff_info_Conv2D_90_off_bias_out_148_quant_scale[] = { 0.00543363159522414 };
  static const int16_t buff_info_Conv2D_90_off_bias_out_148_quant_offset[] = { -2 };
  static const float buff_info_PReLU_94_out_0_inserted_out659_quant_scale[] = { 0.0246124491095543 };
  static const int16_t buff_info_PReLU_94_out_0_inserted_out659_quant_offset[] = { -63 };
  static const float buff_info_PReLU_94_out_0_inserted_out659_cp_in_139_quant_scale[] = { 0.0246124491095543 };
  static const int16_t buff_info_PReLU_94_out_0_inserted_out659_cp_in_139_quant_offset[] = { -63 };
  static const float buff_info_Conv2D_95_off_bias_out_157_quant_scale[] = { 0.0105007672682405 };
  static const int16_t buff_info_Conv2D_95_off_bias_out_157_quant_offset[] = { 1 };
  static const float buff_info_PReLU_99_out_0_inserted_out664_quant_scale[] = { 0.0159733258187771 };
  static const int16_t buff_info_PReLU_99_out_0_inserted_out664_quant_offset[] = { -68 };
  static const float buff_info_Conv2D_101_off_bias_out_166_quant_scale[] = { 0.00520839262753725 };
  static const int16_t buff_info_Conv2D_101_off_bias_out_166_quant_offset[] = { -21 };
  static const float buff_info_PReLU_105_out_0_inserted_out666_quant_scale[] = { 0.0226547680795193 };
  static const int16_t buff_info_PReLU_105_out_0_inserted_out666_quant_offset[] = { -86 };
  static const float buff_info_PReLU_105_out_0_inserted_out666_cp_in_140_quant_scale[] = { 0.0226547680795193 };
  static const int16_t buff_info_PReLU_105_out_0_inserted_out666_cp_in_140_quant_offset[] = { -86 };
  static const float buff_info_Conv2D_106_off_bias_out_175_quant_scale[] = { 0.0136520965024829 };
  static const int16_t buff_info_Conv2D_106_off_bias_out_175_quant_offset[] = { 20 };
  static const float buff_info_PReLU_110_out_0_inserted_out671_quant_scale[] = { 0.0158433523029089 };
  static const int16_t buff_info_PReLU_110_out_0_inserted_out671_quant_offset[] = { -93 };
  static const float buff_info_Conv2D_112_off_bias_out_184_quant_scale[] = { 0.0038495606277138 };
  static const int16_t buff_info_Conv2D_112_off_bias_out_184_quant_offset[] = { 3 };
  static const float buff_info_PReLU_116_out_0_inserted_out673_quant_scale[] = { 0.0197210405021906 };
  static const int16_t buff_info_PReLU_116_out_0_inserted_out673_quant_offset[] = { -80 };
  static const float buff_info_PReLU_116_out_0_inserted_out673_cp_in_141_quant_scale[] = { 0.0197210405021906 };
  static const int16_t buff_info_PReLU_116_out_0_inserted_out673_cp_in_141_quant_offset[] = { -80 };
  static const float buff_info_Conv2D_117_off_bias_out_193_quant_scale[] = { 0.00951043330132961 };
  static const int16_t buff_info_Conv2D_117_off_bias_out_193_quant_offset[] = { 11 };
  static const float buff_info_PReLU_121_out_0_inserted_out678_quant_scale[] = { 0.0124743264168501 };
  static const int16_t buff_info_PReLU_121_out_0_inserted_out678_quant_offset[] = { -71 };
  static const float buff_info_Conv2D_123_off_bias_out_202_quant_scale[] = { 0.00365354074165225 };
  static const int16_t buff_info_Conv2D_123_off_bias_out_202_quant_offset[] = { -1 };
  static const float buff_info_PReLU_127_out_0_inserted_out680_quant_scale[] = { 0.0192501228302717 };
  static const int16_t buff_info_PReLU_127_out_0_inserted_out680_quant_offset[] = { -77 };
  static const float buff_info_PReLU_127_out_0_inserted_out680_cp_in_142_quant_scale[] = { 0.0192501228302717 };
  static const int16_t buff_info_PReLU_127_out_0_inserted_out680_cp_in_142_quant_offset[] = { -77 };
  static const float buff_info_Conv2D_128_off_bias_out_211_quant_scale[] = { 0.012047179043293 };
  static const int16_t buff_info_Conv2D_128_off_bias_out_211_quant_offset[] = { 8 };
  static const float buff_info_PReLU_132_out_0_inserted_out685_quant_scale[] = { 0.0104796029627323 };
  static const int16_t buff_info_PReLU_132_out_0_inserted_out685_quant_offset[] = { -87 };
  static const uint32_t buff_info__shape_1_512_7_7[] = { 1, 7, 7, 512 };
  static const uint32_t buff_info__mem_shape_L_1_512_7_7[] = { 1, 7, 7, 512 };
  static const float buff_info_Conv2D_134_off_bias_out_220_quant_scale[] = { 0.00439437432214618 };
  static const int16_t buff_info_Conv2D_134_off_bias_out_220_quant_offset[] = { -23 };
  static const float buff_info_PReLU_138_out_0_inserted_out687_quant_scale[] = { 0.0225423723459244 };
  static const int16_t buff_info_PReLU_138_out_0_inserted_out687_quant_offset[] = { -79 };
  static const uint32_t buff_info__mem_shape_M_1_512_7_7[] = { 1, 32, 7, 7, 16 };
  static const float buff_info_PReLU_138_out_0_inserted_out687_cp_in_143_quant_scale[] = { 0.0225423723459244 };
  static const int16_t buff_info_PReLU_138_out_0_inserted_out687_cp_in_143_quant_offset[] = { -79 };
  static const uint32_t buff_info__shape_1_1024_7_7[] = { 1, 7, 7, 1024 };
  static const uint32_t buff_info__mem_shape_M_1_1024_7_7[] = { 1, 64, 7, 7, 16 };
  static const uint32_t buff_info__mem_shape_L_1_1024_7_7[] = { 1, 7, 7, 1024 };
  static const float buff_info_Conv2D_139_off_bias_out_229_quant_scale[] = { 0.00919132400304079 };
  static const int16_t buff_info_Conv2D_139_off_bias_out_229_quant_offset[] = { 12 };
  static const float buff_info_PReLU_143_out_0_inserted_out692_quant_scale[] = { 0.0133142992854118 };
  static const int16_t buff_info_PReLU_143_out_0_inserted_out692_quant_offset[] = { -77 };
  static const float buff_info_Conv2D_145_off_bias_out_238_quant_scale[] = { 0.00388094549998641 };
  static const int16_t buff_info_Conv2D_145_off_bias_out_238_quant_offset[] = { 33 };
  static const float buff_info_PReLU_149_out_0_inserted_out694_quant_scale[] = { 0.0179469585418701 };
  static const int16_t buff_info_PReLU_149_out_0_inserted_out694_quant_offset[] = { -116 };
  static const float buff_info_PReLU_149_out_0_inserted_out694_cp_in_144_quant_scale[] = { 0.0179469585418701 };
  static const int16_t buff_info_PReLU_149_out_0_inserted_out694_cp_in_144_quant_offset[] = { -116 };
  static const float buff_info_Conv2D_150_off_bias_out_247_quant_scale[] = { 0.00952027272433043 };
  static const int16_t buff_info_Conv2D_150_off_bias_out_247_quant_offset[] = { 17 };
  static const float buff_info_Quantize_156_out_0_quant_scale[] = { 0.00338450819253922 };
  static const int16_t buff_info_Quantize_156_out_0_quant_offset[] = { 14 };
  static const uint32_t buff_info__shape_1_50176_1_1[] = { 1, 1, 1, 50176 };
  static const uint32_t buff_info__mem_shape_F_1_50176_1_1[] = { 1, 50176, 1, 1 };
  static const float buff_info_Gemm_159_reshape_x_2_quant_scale[] = { 0.00338450819253922 };
  static const int16_t buff_info_Gemm_159_reshape_x_2_quant_offset[] = { 14 };
  static const uint32_t buff_info__shape_1_128_1_1[] = { 1, 1, 1, 128 };
  static const uint32_t buff_info__mem_shape_F_1_128_1_1[] = { 1, 128, 1, 1 };
  static const float buff_info_Gemm_159_conv_4_quant_scale[] = { 0.00331049039959908 };
  static const int16_t buff_info_Gemm_159_conv_4_quant_offset[] = { 13 };
  static const uint32_t buff_info__shape_1_201731_1_1[] = { 1, 1, 1, 201731 };
  static const uint32_t buff_info__mem_shape_F_1_201731_1_1[] = { 1, 201731, 1, 1 };
  static const uint32_t buff_info__shape_1_128[] = { 1, 1, 128, 1 };
  static const uint32_t buff_info__mem_shape_U_1_128[] = { 1, 128 };
  static const float buff_info_Gemm_159_out_0_quant_scale[] = { 0.00331049039959908 };
  static const int16_t buff_info_Gemm_159_out_0_quant_offset[] = { 13 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_2_out_0_inserted_out604",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 150528,
      .offset_end = 301056,
      .offset_limit = 301120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_112_112,
    },
    {
      .name = "Sub_3_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 150528,
      .offset_limit = 150592,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_112_112,
    },
    {
      .name = "Sub_3_out_0_inserted_out608",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 150528,
      .offset_end = 188160,
      .offset_limit = 188224,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_3_112_112,
      .per_channel = 0,
      .scale = buff_info_Sub_3_out_0_inserted_out608_quant_scale,
      .offset = buff_info_Sub_3_out_0_inserted_out608_quant_offset,
    },
    {
      .name = "Mul_4_out_0_cp_in_97",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 439040,
      .offset_limit = 439104,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 4,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_3_112_112,
      .per_channel = 0,
      .scale = buff_info_Mul_4_out_0_cp_in_97_quant_scale,
      .offset = buff_info_Mul_4_out_0_cp_in_97_quant_offset,
    },
    {
      .name = "Conv2D_7_off_bias_out_13",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 5,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
      .per_channel = 0,
      .scale = buff_info_Conv2D_7_off_bias_out_13_quant_scale,
      .offset = buff_info_Conv2D_7_off_bias_out_13_quant_offset,
    },
    {
      .name = "Conv2D_7_off_bias_out_13_inserted_out609",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
    },
    {
      .name = "BatchNormalization_10_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 7,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
    },
    {
      .name = "PReLU_11_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1605632,
      .offset_end = 3211264,
      .offset_limit = 3211328,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
    },
    {
      .name = "PReLU_11_out_0_inserted_out610",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
      .per_channel = 0,
      .scale = buff_info_PReLU_11_out_0_inserted_out610_quant_scale,
      .offset = buff_info_PReLU_11_out_0_inserted_out610_quant_offset,
    },
    {
      .name = "Conv2D_13_off_bias_out_22",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 10,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
      .per_channel = 0,
      .scale = buff_info_Conv2D_13_off_bias_out_22_quant_scale,
      .offset = buff_info_Conv2D_13_off_bias_out_22_quant_offset,
    },
    {
      .name = "Conv2D_13_off_bias_out_22_inserted_out611",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
    },
    {
      .name = "BatchNormalization_16_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
    },
    {
      .name = "PReLU_17_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
    },
    {
      .name = "PReLU_17_out_0_inserted_out612",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_32_112_112,
      .per_channel = 0,
      .scale = buff_info_PReLU_17_out_0_inserted_out612_quant_scale,
      .offset = buff_info_PReLU_17_out_0_inserted_out612_quant_offset,
    },
    {
      .name = "Conv2D_18_off_bias_out_31",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 15,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_112_112,
      .per_channel = 0,
      .scale = buff_info_Conv2D_18_off_bias_out_31_quant_scale,
      .offset = buff_info_Conv2D_18_off_bias_out_31_quant_offset,
    },
    {
      .name = "Conv2D_18_off_bias_out_31_inserted_out614",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 3211264,
      .offset_limit = 3211328,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_112_112,
    },
    {
      .name = "BatchNormalization_21_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 3211264,
      .offset_end = 6422528,
      .offset_limit = 6422592,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 17,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_112_112,
    },
    {
      .name = "PReLU_22_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 3211264,
      .offset_limit = 3211328,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 18,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_112_112,
    },
    {
      .name = "PReLU_22_out_0_inserted_out615",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_112_112,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_112_112,
      .per_channel = 0,
      .scale = buff_info_PReLU_22_out_0_inserted_out615_quant_scale,
      .offset = buff_info_PReLU_22_out_0_inserted_out615_quant_offset,
    },
    {
      .name = "Conv2D_24_off_bias_out_40",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 20,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_56_56,
      .per_channel = 0,
      .scale = buff_info_Conv2D_24_off_bias_out_40_quant_scale,
      .offset = buff_info_Conv2D_24_off_bias_out_40_quant_offset,
    },
    {
      .name = "Conv2D_24_off_bias_out_40_inserted_out616",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_56_56,
    },
    {
      .name = "BatchNormalization_27_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 802816,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_56_56,
    },
    {
      .name = "PReLU_28_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_56_56,
    },
    {
      .name = "PReLU_28_out_0_inserted_out617",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 24,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_64_56_56,
      .per_channel = 0,
      .scale = buff_info_PReLU_28_out_0_inserted_out617_quant_scale,
      .offset = buff_info_PReLU_28_out_0_inserted_out617_quant_offset,
    },
    {
      .name = "Conv2D_29_off_bias_out_49",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 25,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
      .per_channel = 0,
      .scale = buff_info_Conv2D_29_off_bias_out_49_quant_scale,
      .offset = buff_info_Conv2D_29_off_bias_out_49_quant_offset,
    },
    {
      .name = "Conv2D_29_off_bias_out_49_inserted_out621",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 26,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "BatchNormalization_32_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 27,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "PReLU_33_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1605632,
      .offset_end = 3211264,
      .offset_limit = 3211328,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 28,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "PReLU_33_out_0_inserted_out622",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 29,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
      .per_channel = 0,
      .scale = buff_info_PReLU_33_out_0_inserted_out622_quant_scale,
      .offset = buff_info_PReLU_33_out_0_inserted_out622_quant_offset,
    },
    {
      .name = "Conv2D_35_off_bias_out_58",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 30,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
      .per_channel = 0,
      .scale = buff_info_Conv2D_35_off_bias_out_58_quant_scale,
      .offset = buff_info_Conv2D_35_off_bias_out_58_quant_offset,
    },
    {
      .name = "Conv2D_35_off_bias_out_58_inserted_out623",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 31,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "BatchNormalization_38_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 32,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "PReLU_39_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1605632,
      .offset_end = 3211264,
      .offset_limit = 3211328,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "PReLU_39_out_0_inserted_out624",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 34,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
      .per_channel = 0,
      .scale = buff_info_PReLU_39_out_0_inserted_out624_quant_scale,
      .offset = buff_info_PReLU_39_out_0_inserted_out624_quant_offset,
    },
    {
      .name = "Conv2D_40_out_0_cp_in_102_cp_in_103_cp_in_104",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 35,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_128_56_56,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "Conv2D_40_off_bias_out_67",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 35,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
      .per_channel = 0,
      .scale = buff_info_Conv2D_40_off_bias_out_67_quant_scale,
      .offset = buff_info_Conv2D_40_off_bias_out_67_quant_offset,
    },
    {
      .name = "Conv2D_40_off_bias_out_67_inserted_out628",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 36,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "BatchNormalization_43_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 37,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "PReLU_44_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 38,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
    },
    {
      .name = "PReLU_44_out_0_inserted_out629",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 39,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_56_56,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_56_56,
      .per_channel = 0,
      .scale = buff_info_PReLU_44_out_0_inserted_out629_quant_scale,
      .offset = buff_info_PReLU_44_out_0_inserted_out629_quant_offset,
    },
    {
      .name = "Conv2D_46_off_bias_out_76",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 40,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_28_28,
      .per_channel = 0,
      .scale = buff_info_Conv2D_46_off_bias_out_76_quant_scale,
      .offset = buff_info_Conv2D_46_off_bias_out_76_quant_offset,
    },
    {
      .name = "Conv2D_46_off_bias_out_76_inserted_out630",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 41,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_28_28,
    },
    {
      .name = "BatchNormalization_49_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 42,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_28_28,
    },
    {
      .name = "PReLU_50_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 43,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_28_28,
    },
    {
      .name = "PReLU_50_out_0_inserted_out631",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 44,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_28_28,
      .per_channel = 0,
      .scale = buff_info_PReLU_50_out_0_inserted_out631_quant_scale,
      .offset = buff_info_PReLU_50_out_0_inserted_out631_quant_offset,
    },
    {
      .name = "PReLU_50_out_0_inserted_out631_cp_in_135",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 301056,
      .offset_limit = 301120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 45,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_128_28_28,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_28_28,
      .per_channel = 0,
      .scale = buff_info_PReLU_50_out_0_inserted_out631_cp_in_135_quant_scale,
      .offset = buff_info_PReLU_50_out_0_inserted_out631_cp_in_135_quant_offset,
    },
    {
      .name = "Conv2D_51_out_0_cp_in_105_cp_in_106_cp_in_107",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 301056,
      .offset_end = 326144,
      .offset_limit = 326208,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_256_28_28,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 18,
      .Qn = -3,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "Conv2D_51_off_bias_out_85",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_Conv2D_51_off_bias_out_85_quant_scale,
      .offset = buff_info_Conv2D_51_off_bias_out_85_quant_offset,
    },
    {
      .name = "Conv2D_51_off_bias_out_85_inserted_out635",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 47,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "BatchNormalization_54_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 802816,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 48,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "PReLU_55_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 49,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "PReLU_55_out_0_inserted_out636",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 50,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_PReLU_55_out_0_inserted_out636_quant_scale,
      .offset = buff_info_PReLU_55_out_0_inserted_out636_quant_offset,
    },
    {
      .name = "Conv2D_57_off_bias_out_94",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_Conv2D_57_off_bias_out_94_quant_scale,
      .offset = buff_info_Conv2D_57_off_bias_out_94_quant_offset,
    },
    {
      .name = "Conv2D_57_off_bias_out_94_inserted_out637",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "BatchNormalization_60_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 802816,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "PReLU_61_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 54,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "PReLU_61_out_0_inserted_out638",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 55,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_PReLU_61_out_0_inserted_out638_quant_scale,
      .offset = buff_info_PReLU_61_out_0_inserted_out638_quant_offset,
    },
    {
      .name = "PReLU_61_out_0_inserted_out638_cp_in_136",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 56,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_256_28_28,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_PReLU_61_out_0_inserted_out638_cp_in_136_quant_scale,
      .offset = buff_info_PReLU_61_out_0_inserted_out638_cp_in_136_quant_offset,
    },
    {
      .name = "Conv2D_62_out_0_cp_in_108_cp_in_109_cp_in_110",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 426496,
      .offset_limit = 426560,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 57,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_256_28_28,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "Conv2D_62_off_bias_out_103",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 57,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_Conv2D_62_off_bias_out_103_quant_scale,
      .offset = buff_info_Conv2D_62_off_bias_out_103_quant_offset,
    },
    {
      .name = "Conv2D_62_off_bias_out_103_inserted_out642",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 58,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "BatchNormalization_65_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 802816,
      .offset_end = 1605632,
      .offset_limit = 1605696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 59,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "PReLU_66_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 802816,
      .offset_limit = 802880,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 60,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
    },
    {
      .name = "PReLU_66_out_0_inserted_out643",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 61,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_28_28,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_28_28,
      .per_channel = 0,
      .scale = buff_info_PReLU_66_out_0_inserted_out643_quant_scale,
      .offset = buff_info_PReLU_66_out_0_inserted_out643_quant_offset,
    },
    {
      .name = "Conv2D_68_off_bias_out_112",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 62,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_68_off_bias_out_112_quant_scale,
      .offset = buff_info_Conv2D_68_off_bias_out_112_quant_offset,
    },
    {
      .name = "Conv2D_68_off_bias_out_112_inserted_out644",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 63,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_14_14,
    },
    {
      .name = "BatchNormalization_71_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 64,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_14_14,
    },
    {
      .name = "PReLU_72_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 65,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_14_14,
    },
    {
      .name = "PReLU_72_out_0_inserted_out645",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 66,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_72_out_0_inserted_out645_quant_scale,
      .offset = buff_info_PReLU_72_out_0_inserted_out645_quant_offset,
    },
    {
      .name = "PReLU_72_out_0_inserted_out645_cp_in_137",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 250880,
      .offset_end = 301056,
      .offset_limit = 301120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 67,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_256_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_256_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_72_out_0_inserted_out645_cp_in_137_quant_scale,
      .offset = buff_info_PReLU_72_out_0_inserted_out645_cp_in_137_quant_offset,
    },
    {
      .name = "Conv2D_73_out_0_cp_in_111_cp_in_112_cp_in_113",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 407680,
      .offset_limit = 407744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 68,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "Conv2D_73_off_bias_out_121",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 68,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_73_off_bias_out_121_quant_scale,
      .offset = buff_info_Conv2D_73_off_bias_out_121_quant_offset,
    },
    {
      .name = "Conv2D_73_off_bias_out_121_inserted_out649",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_76_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_77_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 71,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_77_out_0_inserted_out650",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_77_out_0_inserted_out650_quant_scale,
      .offset = buff_info_PReLU_77_out_0_inserted_out650_quant_offset,
    },
    {
      .name = "Conv2D_79_off_bias_out_130",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 100352,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 73,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_79_off_bias_out_130_quant_scale,
      .offset = buff_info_Conv2D_79_off_bias_out_130_quant_offset,
    },
    {
      .name = "Conv2D_79_off_bias_out_130_inserted_out651",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 74,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_82_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 75,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_83_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 76,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_83_out_0_inserted_out652",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 77,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_83_out_0_inserted_out652_quant_scale,
      .offset = buff_info_PReLU_83_out_0_inserted_out652_quant_offset,
    },
    {
      .name = "PReLU_83_out_0_inserted_out652_cp_in_138",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 78,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_83_out_0_inserted_out652_cp_in_138_quant_scale,
      .offset = buff_info_PReLU_83_out_0_inserted_out652_cp_in_138_quant_offset,
    },
    {
      .name = "Conv2D_84_out_0_cp_in_114_cp_in_115_cp_in_116",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 407680,
      .offset_limit = 407744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 79,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "Conv2D_84_off_bias_out_139",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 79,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_84_off_bias_out_139_quant_scale,
      .offset = buff_info_Conv2D_84_off_bias_out_139_quant_offset,
    },
    {
      .name = "Conv2D_84_off_bias_out_139_inserted_out656",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 80,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_87_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 81,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_88_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 82,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_88_out_0_inserted_out657",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 83,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_88_out_0_inserted_out657_quant_scale,
      .offset = buff_info_PReLU_88_out_0_inserted_out657_quant_offset,
    },
    {
      .name = "Conv2D_90_off_bias_out_148",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 100352,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 84,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_90_off_bias_out_148_quant_scale,
      .offset = buff_info_Conv2D_90_off_bias_out_148_quant_offset,
    },
    {
      .name = "Conv2D_90_off_bias_out_148_inserted_out658",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 85,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_93_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 86,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_94_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 87,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_94_out_0_inserted_out659",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 88,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_94_out_0_inserted_out659_quant_scale,
      .offset = buff_info_PReLU_94_out_0_inserted_out659_quant_offset,
    },
    {
      .name = "PReLU_94_out_0_inserted_out659_cp_in_139",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 89,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_94_out_0_inserted_out659_cp_in_139_quant_scale,
      .offset = buff_info_PReLU_94_out_0_inserted_out659_cp_in_139_quant_offset,
    },
    {
      .name = "Conv2D_95_out_0_cp_in_117_cp_in_118_cp_in_119",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 407680,
      .offset_limit = 407744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 90,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "Conv2D_95_off_bias_out_157",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 90,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_95_off_bias_out_157_quant_scale,
      .offset = buff_info_Conv2D_95_off_bias_out_157_quant_offset,
    },
    {
      .name = "Conv2D_95_off_bias_out_157_inserted_out663",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 91,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_98_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 92,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_99_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 93,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_99_out_0_inserted_out664",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 94,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_99_out_0_inserted_out664_quant_scale,
      .offset = buff_info_PReLU_99_out_0_inserted_out664_quant_offset,
    },
    {
      .name = "Conv2D_101_off_bias_out_166",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 100352,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 95,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_101_off_bias_out_166_quant_scale,
      .offset = buff_info_Conv2D_101_off_bias_out_166_quant_offset,
    },
    {
      .name = "Conv2D_101_off_bias_out_166_inserted_out665",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 96,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_104_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 97,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_105_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 98,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_105_out_0_inserted_out666",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 99,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_105_out_0_inserted_out666_quant_scale,
      .offset = buff_info_PReLU_105_out_0_inserted_out666_quant_offset,
    },
    {
      .name = "PReLU_105_out_0_inserted_out666_cp_in_140",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 100,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_105_out_0_inserted_out666_cp_in_140_quant_scale,
      .offset = buff_info_PReLU_105_out_0_inserted_out666_cp_in_140_quant_offset,
    },
    {
      .name = "Conv2D_106_out_0_cp_in_120_cp_in_121_cp_in_122",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 407680,
      .offset_limit = 407744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 101,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "Conv2D_106_off_bias_out_175",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 101,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_106_off_bias_out_175_quant_scale,
      .offset = buff_info_Conv2D_106_off_bias_out_175_quant_offset,
    },
    {
      .name = "Conv2D_106_off_bias_out_175_inserted_out670",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 102,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_109_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 103,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_110_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 104,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_110_out_0_inserted_out671",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 105,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_110_out_0_inserted_out671_quant_scale,
      .offset = buff_info_PReLU_110_out_0_inserted_out671_quant_offset,
    },
    {
      .name = "Conv2D_112_off_bias_out_184",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 100352,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 106,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_112_off_bias_out_184_quant_scale,
      .offset = buff_info_Conv2D_112_off_bias_out_184_quant_offset,
    },
    {
      .name = "Conv2D_112_off_bias_out_184_inserted_out672",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 107,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_115_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 108,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_116_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 109,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_116_out_0_inserted_out673",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 110,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_116_out_0_inserted_out673_quant_scale,
      .offset = buff_info_PReLU_116_out_0_inserted_out673_quant_offset,
    },
    {
      .name = "PReLU_116_out_0_inserted_out673_cp_in_141",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 111,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_116_out_0_inserted_out673_cp_in_141_quant_scale,
      .offset = buff_info_PReLU_116_out_0_inserted_out673_cp_in_141_quant_offset,
    },
    {
      .name = "Conv2D_117_out_0_cp_in_123_cp_in_124_cp_in_125",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 407680,
      .offset_limit = 407744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 112,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "Conv2D_117_off_bias_out_193",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 112,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_117_off_bias_out_193_quant_scale,
      .offset = buff_info_Conv2D_117_off_bias_out_193_quant_offset,
    },
    {
      .name = "Conv2D_117_off_bias_out_193_inserted_out677",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 113,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_120_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 114,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_121_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 115,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_121_out_0_inserted_out678",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 116,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_121_out_0_inserted_out678_quant_scale,
      .offset = buff_info_PReLU_121_out_0_inserted_out678_quant_offset,
    },
    {
      .name = "Conv2D_123_off_bias_out_202",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 100352,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 117,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_123_off_bias_out_202_quant_scale,
      .offset = buff_info_Conv2D_123_off_bias_out_202_quant_offset,
    },
    {
      .name = "Conv2D_123_off_bias_out_202_inserted_out679",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 118,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_126_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 119,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_127_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 120,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_127_out_0_inserted_out680",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 121,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_127_out_0_inserted_out680_quant_scale,
      .offset = buff_info_PReLU_127_out_0_inserted_out680_quant_offset,
    },
    {
      .name = "PReLU_127_out_0_inserted_out680_cp_in_142",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 122,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_127_out_0_inserted_out680_cp_in_142_quant_scale,
      .offset = buff_info_PReLU_127_out_0_inserted_out680_cp_in_142_quant_offset,
    },
    {
      .name = "Conv2D_128_out_0_cp_in_126_cp_in_127_cp_in_128",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 401408,
      .offset_end = 407680,
      .offset_limit = 407744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 123,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_14_14,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "Conv2D_128_off_bias_out_211",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 123,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_Conv2D_128_off_bias_out_211_quant_scale,
      .offset = buff_info_Conv2D_128_off_bias_out_211_quant_offset,
    },
    {
      .name = "Conv2D_128_off_bias_out_211_inserted_out684",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 124,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "BatchNormalization_131_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 125,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_132_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 126,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
    },
    {
      .name = "PReLU_132_out_0_inserted_out685",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 127,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_14_14,
      .per_channel = 0,
      .scale = buff_info_PReLU_132_out_0_inserted_out685_quant_scale,
      .offset = buff_info_PReLU_132_out_0_inserted_out685_quant_offset,
    },
    {
      .name = "Conv2D_134_off_bias_out_220",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 100352,
      .offset_end = 125440,
      .offset_limit = 125504,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 128,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_7_7,
      .per_channel = 0,
      .scale = buff_info_Conv2D_134_off_bias_out_220_quant_scale,
      .offset = buff_info_Conv2D_134_off_bias_out_220_quant_offset,
    },
    {
      .name = "Conv2D_134_off_bias_out_220_inserted_out686",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_7_7,
    },
    {
      .name = "BatchNormalization_137_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 100352,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_7_7,
    },
    {
      .name = "PReLU_138_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 100352,
      .offset_limit = 100416,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 131,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_7_7,
    },
    {
      .name = "PReLU_138_out_0_inserted_out687",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 100352,
      .offset_end = 125440,
      .offset_limit = 125504,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 132,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_7_7,
      .per_channel = 0,
      .scale = buff_info_PReLU_138_out_0_inserted_out687_quant_scale,
      .offset = buff_info_PReLU_138_out_0_inserted_out687_quant_offset,
    },
    {
      .name = "PReLU_138_out_0_inserted_out687_cp_in_143",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 250880,
      .offset_end = 275968,
      .offset_limit = 276032,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 133,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_512_7_7,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_512_7_7,
      .per_channel = 0,
      .scale = buff_info_PReLU_138_out_0_inserted_out687_cp_in_143_quant_scale,
      .offset = buff_info_PReLU_138_out_0_inserted_out687_cp_in_143_quant_offset,
    },
    {
      .name = "Conv2D_139_out_0_cp_in_129_cp_in_130_cp_in_131",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 275968,
      .offset_end = 277536,
      .offset_limit = 277600,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 134,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_1024_7_7,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 19,
      .Qn = -4,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "Conv2D_139_off_bias_out_229",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 134,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_Conv2D_139_off_bias_out_229_quant_scale,
      .offset = buff_info_Conv2D_139_off_bias_out_229_quant_offset,
    },
    {
      .name = "Conv2D_139_off_bias_out_229_inserted_out691",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 135,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "BatchNormalization_142_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 136,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "PReLU_143_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 137,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "PReLU_143_out_0_inserted_out692",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 138,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_PReLU_143_out_0_inserted_out692_quant_scale,
      .offset = buff_info_PReLU_143_out_0_inserted_out692_quant_offset,
    },
    {
      .name = "Conv2D_145_off_bias_out_238",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 250880,
      .offset_end = 301056,
      .offset_limit = 301120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 139,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_Conv2D_145_off_bias_out_238_quant_scale,
      .offset = buff_info_Conv2D_145_off_bias_out_238_quant_offset,
    },
    {
      .name = "Conv2D_145_off_bias_out_238_inserted_out693",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 140,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "BatchNormalization_148_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 141,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "PReLU_149_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 142,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "PReLU_149_out_0_inserted_out694",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 143,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_PReLU_149_out_0_inserted_out694_quant_scale,
      .offset = buff_info_PReLU_149_out_0_inserted_out694_quant_offset,
    },
    {
      .name = "PReLU_149_out_0_inserted_out694_cp_in_144",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 50176,
      .offset_limit = 50240,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 144,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_1024_7_7,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_PReLU_149_out_0_inserted_out694_cp_in_144_quant_scale,
      .offset = buff_info_PReLU_149_out_0_inserted_out694_cp_in_144_quant_offset,
    },
    {
      .name = "Conv2D_150_out_0_cp_in_132_cp_in_133_cp_in_134",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 250880,
      .offset_end = 252448,
      .offset_limit = 252512,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 145,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_M_1_1024_7_7,
      .mem_ndims = 5,
      .chpos = CHPos_Mixed,
      .Qm = 20,
      .Qn = -5,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 16,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "Conv2D_150_off_bias_out_247",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 145,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_Conv2D_150_off_bias_out_247_quant_scale,
      .offset = buff_info_Conv2D_150_off_bias_out_247_quant_offset,
    },
    {
      .name = "Conv2D_150_off_bias_out_247_inserted_out698",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 146,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "BatchNormalization_153_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 147,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "PReLU_154_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 148,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "BatchNormalization_155bn_mul276_out",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 401408,
      .offset_limit = 401472,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 149,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "BatchNormalization_155bn_add278_out",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 200704,
      .offset_limit = 200768,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 150,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
    },
    {
      .name = "Quantize_156_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 200704,
      .offset_end = 250880,
      .offset_limit = 250944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 151,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_7_7,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_7_7,
      .per_channel = 0,
      .scale = buff_info_Quantize_156_out_0_quant_scale,
      .offset = buff_info_Quantize_156_out_0_quant_offset,
    },
    {
      .name = "Gemm_159_reshape_x_2",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 250880,
      .offset_end = 301056,
      .offset_limit = 301120,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 152,
      .batch = 50176,
      .mem_shape = buff_info__mem_shape_F_1_50176_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_50176_1_1,
      .per_channel = 0,
      .scale = buff_info_Gemm_159_reshape_x_2_quant_scale,
      .offset = buff_info_Gemm_159_reshape_x_2_quant_offset,
    },
    {
      .name = "Gemm_159_conv_4",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 201744,
      .offset_end = 201872,
      .offset_limit = 201936,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 153,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_F_1_128_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128_1_1,
      .per_channel = 0,
      .scale = buff_info_Gemm_159_conv_4_quant_scale,
      .offset = buff_info_Gemm_159_conv_4_quant_offset,
    },
    {
      .name = "SCRATCH_Gemm_159_conv_4_PORT_OUT",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 201731,
      .offset_limit = 201800,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 153,
      .batch = 201731,
      .mem_shape = buff_info__mem_shape_F_1_201731_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_201731_1_1,
    },
    {
      .name = "Gemm_159_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 201744,
      .offset_end = 201872,
      .offset_limit = 201936,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 154,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_128,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 7,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT8,
      .nbits = 8,
      .ndims = 4,
      .shape = buff_info__shape_1_128,
      .per_channel = 0,
      .scale = buff_info_Gemm_159_out_0_quant_scale,
      .offset = buff_info_Gemm_159_out_0_quant_offset,
    },
    {
      .name = "Gemm_159_out_0_inserted_out699",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 512,
      .offset_end = 1024,
      .offset_limit = 1088,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 155,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_128,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

